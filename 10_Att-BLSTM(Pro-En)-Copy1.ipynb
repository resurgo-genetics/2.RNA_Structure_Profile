{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README    \n",
    "1. generate a Keras Layer :AttLayer  \n",
    "2. Use csv module to import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1)使用Kmer进行输入 没有效果 \n",
    "  predict 一下 有预测输出，说明训练有效果\n",
    "  但是 无论是正负样本，预测成负样本的value远远大于正样本的（0.8》》0.2）\n",
    "  可能是训练时正负样本悬殊引起的\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "from collections  import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        '''   \n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/pairs.csv'\n",
    "import csv\n",
    "\n",
    "with open(datafile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for  row in spamreader:\n",
    "        csvkeys = row\n",
    "        break\n",
    "print csvkeys\n",
    "\n",
    "csvdata = dict()\n",
    "for item in csvkeys:\n",
    "    csvdata[item] =[]\n",
    "with open(datafile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        # Add values of every keys to the csvdata for every row\n",
    "        for item in csvkeys:\n",
    "            csvdata[item].append(row[item])\n",
    "            if abs(int(row['enhancer_start'] )-int( row['enhancer_end'])) <6:\n",
    "                print row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "bin :  [22271, 83570.8]\n",
      "enhancer_chrom :  chr22\n",
      "enhancer_distance_to_promoter :  32514\n",
      "enhancer_end :  43515000\n",
      "enhancer_name :  GM12878|chr22:43514400-43515000\n",
      "enhancer_start :  43514400\n",
      "label :  0\n",
      "promoter_chrom :  chr22\n",
      "promoter_end :  43548957\n",
      "promoter_name :  GM12878|chr22:43547516-43548957\n",
      "promoter_start :  43547516\n",
      "window_end :  43547515\n",
      "window_start :  43515001\n",
      "window_chrom :  chr22\n",
      "window_name :  GM12878|chr22:43515001-43547515\n",
      "interactions_in_window :  0\n",
      "active_promoters_in_window :  1\n"
     ]
    }
   ],
   "source": [
    "#print csvdata.keys()\n",
    "for i in range(1):\n",
    "    print '-'*40\n",
    "    for item in csvkeys:\n",
    "        print item ,': ',csvdata[item][5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Promoter/Enhancer Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfasta import Fasta\n",
    "genome = Fasta('/home/yinqijin/WorkSpace/DataHub/genome.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-8-66f86077914a>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-66f86077914a>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    #print genome[item][100:102]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#证明序列不会有错\n",
    "import numpy as np\n",
    "chrom = np.unique(csvdata['enhancer_chrom'])\n",
    "print chrom\n",
    "for item in chrom:\n",
    "    #print genome[item][100:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得ｋｍｅｒ的函数\n",
    "d2 = {'a':0, 'A':0, 'g':1, 'G':1, 'c':2, 'C':2, 't':3, 'T':3, 'N':0, 'n':0}\n",
    "def seq_to_kspec(seq):\n",
    "    mat = np.zeros((4096,1))\n",
    "    k = 0\n",
    "    if len(seq)<6:\n",
    "        return mat\n",
    "    for i in range(6):\n",
    "        k = k*4 + d2[seq[i]]\n",
    "    mat[k]+=1  \n",
    "    for i in range(6,len(seq)):\n",
    "        k = k - 4**5*d2[seq[i-6]]\n",
    "        k = k*4 + d2[seq[i]]\n",
    "        mat[k] += 1  \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PESeq=dict()\n",
    "PESeq['Pro-index']=[]\n",
    "PESeq['Pro-Seq']=[]\n",
    "PESeq['Pro-Kmer']=[]\n",
    "PESeq['Ehr-index']=[]\n",
    "PESeq['Ehr-Seq']=[]\n",
    "PESeq['Ehr-Kmer']=[]\n",
    "PESeq['label'] =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1452 \tehr_index ['chr17', '41080595', '41080600']\n",
      "1760 \tehr_index ['chr3', '144015595', '144015600']\n",
      "1887 \tehr_index ['chr6', '27857195', '27857200']\n",
      "2937 \tehr_index ['chr1', '149764795', '149764800']\n",
      "2943 \tehr_index ['chr20', '44642400', '44642405']\n",
      "3065 \tehr_index ['chr6', '31793795', '31793800']\n",
      "4066 \tehr_index ['chr11', '62541195', '62541200']\n",
      "4186 \tehr_index ['chr19', '17576795', '17576800']\n",
      "4510 \tehr_index ['chr10', '70758000', '70758005']\n",
      "4598 \tehr_index ['chr19', '17829600', '17829605']\n",
      "4866 \tehr_index ['chr9', '131801000', '131801005']\n",
      "5335 \tehr_index ['chr3', '61169000', '61169005']\n",
      "7503 \tehr_index ['chr14', '23285600', '23285605']\n",
      "8441 \tehr_index ['chrX', '153725595', '153725600']\n",
      "9221 \tehr_index ['chr4', '40580595', '40580600']\n",
      "9249 \tehr_index ['chr11', '62541195', '62541200']\n",
      "9430 \tehr_index ['chr4', '36313000', '36313005']\n",
      "9445 \tehr_index ['chr14', '23285600', '23285605']\n",
      "10193 \tehr_index ['chr12', '111135595', '111135600']\n",
      "10649 \tehr_index ['chr4', '159783200', '159783205']\n",
      "11393 \tehr_index ['chr17', '53703200', '53703205']\n",
      "11449 \tehr_index ['chr14', '64703800', '64703805']\n",
      "11580 \tehr_index ['chr6', '43338800', '43338805']\n",
      "14230 \tehr_index ['chr6', '31793795', '31793800']\n",
      "14240 \tehr_index ['chr1', '160769395', '160769400']\n",
      "14372 \tehr_index ['chr11', '62541195', '62541200']\n",
      "15298 \tehr_index ['chr1', '229506395', '229506400']\n",
      "16639 \tehr_index ['chr3', '4876600', '4876605']\n",
      "16660 \tehr_index ['chr14', '75892195', '75892200']\n",
      "17407 \tehr_index ['chr11', '62541195', '62541200']\n",
      "18969 \tehr_index ['chr6', '31793795', '31793800']\n",
      "19059 \tehr_index ['chr4', '110913200', '110913205']\n",
      "19584 \tehr_index ['chr18', '32620795', '32620800']\n",
      "20858 \tehr_index ['chr5', '1784400', '1784405']\n",
      "21765 \tehr_index ['chr17', '72439800', '72439805']\n",
      "21860 \tehr_index ['chr15', '57395400', '57395405']\n",
      "22039 \tehr_index ['chr19', '6040400', '6040405']\n",
      "22267 \tehr_index ['chr11', '48064795', '48064800']\n",
      "22368 \tehr_index ['chr5', '68484395', '68484400']\n",
      "22471 \tehr_index ['chr19', '17829600', '17829605']\n",
      "23346 \tehr_index ['chr19', '979800', '979805']\n",
      "23555 \tehr_index ['chr6', '36694795', '36694800']\n",
      "24097 \tehr_index ['chr2', '128586395', '128586400']\n",
      "25034 \tehr_index ['chr17', '41080595', '41080600']\n",
      "25093 \tehr_index ['chr1', '160769395', '160769400']\n",
      "25354 \tehr_index ['chr4', '110913200', '110913205']\n",
      "25773 \tehr_index ['chr10', '121128195', '121128200']\n",
      "25800 \tehr_index ['chr2', '55261600', '55261605']\n",
      "26023 \tehr_index ['chr17', '45809795', '45809800']\n",
      "26043 \tehr_index ['chr1', '160769395', '160769400']\n",
      "26478 \tehr_index ['chr11', '34071995', '34072000']\n",
      "26626 \tehr_index ['chr14', '23285600', '23285605']\n",
      "26943 \tehr_index ['chr16', '66958800', '66958805']\n",
      "27218 \tehr_index ['chr1', '45858795', '45858800']\n",
      "29338 \tehr_index ['chr20', '43209000', '43209005']\n",
      "32928 \tehr_index ['chr5', '125951395', '125951400']\n",
      "33776 \tehr_index ['chr12', '8798400', '8798405']\n",
      "34380 \tehr_index ['chr5', '59799195', '59799200']\n",
      "34656 \tehr_index ['chr1', '45858795', '45858800']\n",
      "34740 \tehr_index ['chr6', '2856795', '2856800']\n",
      "34794 \tehr_index ['chr10', '104874000', '104874005']\n",
      "34901 \tehr_index ['chr1', '24807395', '24807400']\n",
      "35588 \tehr_index ['chr7', '25008400', '25008405']\n",
      "36345 \tehr_index ['chr6', '2856795', '2856800']\n",
      "36556 \tehr_index ['chr14', '23285600', '23285605']\n",
      "36775 \tehr_index ['chr10', '124907400', '124907405']\n",
      "37393 \tehr_index ['chr19', '45677600', '45677605']\n",
      "37655 \tehr_index ['chr14', '64703800', '64703805']\n",
      "37701 \tehr_index ['chr19', '979800', '979805']\n",
      "39430 \tehr_index ['chr3', '32156600', '32156605']\n",
      "40336 \tehr_index ['chr17', '49308200', '49308205']\n",
      "42448 \tehr_index ['chr6', '27857195', '27857200']\n",
      "43177 \tehr_index ['chr19', '45677600', '45677605']\n",
      "43722 \tehr_index ['chr6', '27857195', '27857200']\n",
      "43878 \tehr_index ['chr1', '45858795', '45858800']\n",
      "43969 \tehr_index ['chr5', '138923600', '138923605']\n",
      "44084 \tehr_index ['chr16', '66958800', '66958805']\n"
     ]
    }
   ],
   "source": [
    "for index in range( len( csvdata[csvkeys[0]])):\n",
    "    pro_index = [csvdata['promoter_chrom'][index],csvdata['promoter_start'][index],csvdata['promoter_end'][index]]   \n",
    "    ehr_index = [csvdata['enhancer_chrom'][index],csvdata['enhancer_start'][index] ,csvdata['enhancer_end'][index]]\n",
    "    pro_seq =  genome[csvdata['promoter_chrom'][index]][int(csvdata['promoter_start'][index]) :int(csvdata['promoter_end'][index])].upper()\n",
    "    ehr_seq =  genome[csvdata['enhancer_chrom'][index]] [int(csvdata['enhancer_start'][index]) : int(csvdata['enhancer_end'][index])].upper()    \n",
    "\n",
    "    if len(pro_seq)<6:\n",
    "        print index,'\\tpro_index',pro_index\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print index,'\\tehr_index',ehr_index\n",
    "        continue\n",
    "    PESeq['Pro-index'].append(pro_index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    PESeq['Pro-Kmer'].append( seq_to_kspec(pro_seq))\n",
    "    PESeq['Ehr-index'].append(ehr_index)\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    PESeq['Ehr-Kmer'].append(seq_to_kspec(ehr_seq))\n",
    "    PESeq['label'].append(csvdata['label'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Ehr-Kmer (44236, 4096, 1)\n",
      "Ehr-Seq (44236,)\n",
      "Pro-index (44236, 3)\n",
      "Pro-Kmer (44236, 4096, 1)\n",
      "Pro-Seq (44236,)\n",
      "Ehr-index (44236, 3)\n",
      "label (44236,)\n",
      "pos data 2110\n",
      "neg data 42126\n"
     ]
    }
   ],
   "source": [
    "print '-'*50\n",
    "for item in PESeq.keys():\n",
    "    print item, np.shape(PESeq[item])\n",
    "print 'pos data' , sum(int( item ) for item in PESeq['label'])\n",
    "print 'neg data',sum(int( 1) if item =='0' else int(0) for item in PESeq['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "(array([ 2110,  2111,  2112, ..., 44233, 44234, 44235]),)\n",
      "(array([   0,    1,    2, ..., 2107, 2108, 2109]),)\n",
      "(array([   0,    1,    2, ..., 2107, 2108, 2109]), array([ 2110,  2111,  2112, ..., 44233, 44234, 44235]))\n",
      "(array([   0,    1,    2, ..., 2107, 2108, 2109]), array([ 2110,  2111,  2112, ..., 44233, 44234, 44235]))\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "pos_neg_index = [ int(item) for item in PESeq['label']]\n",
    "print pos_neg_index[:50]\n",
    "pos_neg_index = np.array(pos_neg_index )  #很关键\n",
    "\n",
    "neg_index  = np.where(pos_neg_index==0)\n",
    "pos_index  = np.where(pos_neg_index ==1)\n",
    "print neg_index\n",
    "print pos_index\n",
    "bal_pos_neg_index = pos_index + neg_index[:len(pos_index)]\n",
    "print bal_pos_neg_index[:50]\n",
    "print bal_pos_neg_index[-50:]\n",
    "print len(bal_pos_neg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengthest Enhancer 9466\n",
      "lengthest Promoter 18594\n"
     ]
    }
   ],
   "source": [
    "print 'lengthest Enhancer',len(max(PESeq['Ehr-Seq'],key= lambda x:len(x)))\n",
    "print  'lengthest Promoter',len(max(PESeq['Pro-Seq'],key= lambda x:len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Get X,Y with no  limited\n",
    "X=[]\n",
    "Y=[]\n",
    "for index in range( len( PESeq['label'])):\n",
    "    Y.append(PESeq['label'][index])\n",
    "    X.append(np.append(PESeq['Pro-Kmer'][index],PESeq['Ehr-Kmer'][index],axis=1))\n",
    "print np.shape(X)\n",
    "print np.shape(Y)\n",
    "X = np.array(X)\n",
    "Y  = np.array(Y)\n",
    "\n",
    "index = range(len(PESeq['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4220, 4096, 2)\n",
      "(4220,)\n"
     ]
    }
   ],
   "source": [
    "# Get X,Y with   limite  making positive and negative samples 's numbers is the same\n",
    "X=[]\n",
    "Y=[]\n",
    "for index in range( 4220):\n",
    "    Y.append(PESeq['label'][index])\n",
    "    X.append(np.append(PESeq['Pro-Kmer'][index],PESeq['Ehr-Kmer'][index],axis=1))\n",
    "print np.shape(X)\n",
    "print np.shape(Y)\n",
    "X = np.array(X)\n",
    "Y  = np.array(Y)\n",
    "\n",
    "\n",
    "index = range(4220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "VALIDATION_SPLIT =0.2\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "np.random.shuffle(index)\n",
    "nb_validation_samples = int(VALIDATION_SPLIT*len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = X[index[: - nb_validation_samples]]\n",
    "y_train = Y[index[:-nb_validation_samples]]\n",
    "x_val = X[index[-nb_validation_samples:]]\n",
    "y_val =Y[index[-nb_validation_samples:]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) #one-hot 编码labels\n",
    "y_val = to_categorical(np.asarray(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_shape: (3376, 4096, 2)\n",
      "y_train_shape: (3376, 2)\n"
     ]
    }
   ],
   "source": [
    "print 'x_train_shape:',np.shape(x_train)\n",
    "print 'y_train_shape:',np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#使验证集正负样本数量相等\n",
    "print np.shape(y_val)\n",
    "\n",
    "print 'valid负正样本比例' sum(y_val)\n",
    "\n",
    "\n",
    "count = defaultdict()\n",
    "count['0']=[]\n",
    "count['1']=[]\n",
    "for i in range(8847):\n",
    "    if y_val[i][0]==1:\n",
    "        count['0'].append(i)\n",
    "    else:\n",
    "        count['1'].append(i)\n",
    "print len(count['0']),len(count['1'])\n",
    "\n",
    "\n",
    " index_my = count['0'][:401]+count['1']\n",
    "\n",
    "x_val_my = x_val[index_my,:,:]\n",
    "y_val_my  =y_val[index_my,:]\n",
    "\n",
    "print 'x_val_shape:',np.shape(x_val_my)\n",
    "print 'y_val_shape:',np.shape(y_val_my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_val_my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5f3c86bb5f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0my_val_my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mx_val_my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_val_my' is not defined"
     ]
    }
   ],
   "source": [
    "print y_val_my\n",
    "print x_val_my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(4096,2), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_lstm =Bidirectional(LSTM(2,return_sequences=False))(kmer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "#l_att = AttLayer()(l_lstm_drop)\n",
    "#l_att_drop = Dropout(0.5)(l_att)\n",
    "preds = Dense(len( y_train[0]), activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_lstm_drop)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_5 (InputLayer)             (None, 4096, 2)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional)  (None, 4)             80          input_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 4)             0           bidirectional_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 2)             10          dropout_11[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 90\n",
      "Trainable params: 90\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3376 samples, validate on 844 samples\n",
      "Epoch 1/20\n",
      "3376/3376 [==============================] - 545s - loss: 0.5085 - acc: 0.5409 - val_loss: 0.4942 - val_acc: 0.5427\n",
      "Epoch 2/20\n",
      "3376/3376 [==============================] - 545s - loss: 0.5048 - acc: 0.5391 - val_loss: 0.4920 - val_acc: 0.5592\n",
      "Epoch 3/20\n",
      "3376/3376 [==============================] - 548s - loss: 0.5031 - acc: 0.5435 - val_loss: 0.4904 - val_acc: 0.5628\n",
      "Epoch 4/20\n",
      "3376/3376 [==============================] - 551s - loss: 0.4997 - acc: 0.5616 - val_loss: 0.4895 - val_acc: 0.5735\n",
      "Epoch 5/20\n",
      "3376/3376 [==============================] - 546s - loss: 0.4984 - acc: 0.5501 - val_loss: 0.4889 - val_acc: 0.5723\n",
      "Epoch 6/20\n",
      "3376/3376 [==============================] - 544s - loss: 0.4986 - acc: 0.5533 - val_loss: 0.4883 - val_acc: 0.5770\n",
      "Epoch 7/20\n",
      "3376/3376 [==============================] - 538s - loss: 0.4976 - acc: 0.5554 - val_loss: 0.4879 - val_acc: 0.5794\n",
      "Epoch 8/20\n",
      "3376/3376 [==============================] - 550s - loss: 0.4968 - acc: 0.5512 - val_loss: 0.4877 - val_acc: 0.5912\n",
      "Epoch 9/20\n",
      "3376/3376 [==============================] - 546s - loss: 0.4959 - acc: 0.5723 - val_loss: 0.4876 - val_acc: 0.5912\n",
      "Epoch 10/20\n",
      "3376/3376 [==============================] - 551s - loss: 0.4966 - acc: 0.5578 - val_loss: 0.4875 - val_acc: 0.5853\n",
      "Epoch 11/20\n",
      "3376/3376 [==============================] - 557s - loss: 0.4962 - acc: 0.5684 - val_loss: 0.4874 - val_acc: 0.5912\n",
      "Epoch 12/20\n",
      "3376/3376 [==============================] - 546s - loss: 0.4952 - acc: 0.5628 - val_loss: 0.4872 - val_acc: 0.5936\n",
      "Epoch 13/20\n",
      "3376/3376 [==============================] - 544s - loss: 0.4954 - acc: 0.5572 - val_loss: 0.4871 - val_acc: 0.5972\n",
      "Epoch 14/20\n",
      "3376/3376 [==============================] - 298s - loss: 0.4961 - acc: 0.5578 - val_loss: 0.4870 - val_acc: 0.6066\n",
      "Epoch 15/20\n",
      "3376/3376 [==============================] - 300s - loss: 0.4945 - acc: 0.5687 - val_loss: 0.4868 - val_acc: 0.6043\n",
      "Epoch 16/20\n",
      "3376/3376 [==============================] - 513s - loss: 0.4947 - acc: 0.5735 - val_loss: 0.4867 - val_acc: 0.6043\n",
      "Epoch 17/20\n",
      "3376/3376 [==============================] - 544s - loss: 0.4953 - acc: 0.5607 - val_loss: 0.4866 - val_acc: 0.6137\n",
      "Epoch 18/20\n",
      "3376/3376 [==============================] - 546s - loss: 0.4951 - acc: 0.5675 - val_loss: 0.4865 - val_acc: 0.6126\n",
      "Epoch 19/20\n",
      "3376/3376 [==============================] - 549s - loss: 0.4942 - acc: 0.5782 - val_loss: 0.4864 - val_acc: 0.6149\n",
      "Epoch 20/20\n",
      "3376/3376 [==============================] - 545s - loss: 0.4934 - acc: 0.5791 - val_loss: 0.4861 - val_acc: 0.6161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7375e9410>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3376 samples, validate on 844 samples\n",
      "Epoch 1/20\n",
      "3376/3376 [==============================] - 548s - loss: 0.4932 - acc: 0.5749 - val_loss: 0.4860 - val_acc: 0.6209\n",
      "Epoch 2/20\n",
      "3376/3376 [==============================] - 555s - loss: 0.4935 - acc: 0.5773 - val_loss: 0.4858 - val_acc: 0.6232\n",
      "Epoch 3/20\n",
      "3376/3376 [==============================] - 549s - loss: 0.4928 - acc: 0.5809 - val_loss: 0.4856 - val_acc: 0.6220\n",
      "Epoch 4/20\n",
      "3376/3376 [==============================] - 557s - loss: 0.4934 - acc: 0.5749 - val_loss: 0.4857 - val_acc: 0.6256\n",
      "Epoch 5/20\n",
      "3376/3376 [==============================] - 555s - loss: 0.4934 - acc: 0.5749 - val_loss: 0.4858 - val_acc: 0.6232\n",
      "Epoch 6/20\n",
      "3376/3376 [==============================] - 552s - loss: 0.4932 - acc: 0.5868 - val_loss: 0.4857 - val_acc: 0.6232\n",
      "Epoch 7/20\n",
      "3376/3376 [==============================] - 548s - loss: 0.4935 - acc: 0.5776 - val_loss: 0.4857 - val_acc: 0.6244\n",
      "Epoch 8/20\n",
      "3376/3376 [==============================] - 544s - loss: 0.4939 - acc: 0.5773 - val_loss: 0.4857 - val_acc: 0.6244\n",
      "Epoch 9/20\n",
      "3376/3376 [==============================] - 533s - loss: 0.4924 - acc: 0.5871 - val_loss: 0.4856 - val_acc: 0.6256\n",
      "Epoch 10/20\n",
      "3376/3376 [==============================] - 549s - loss: 0.4926 - acc: 0.5844 - val_loss: 0.4855 - val_acc: 0.6244\n",
      "Epoch 11/20\n",
      "3376/3376 [==============================] - 550s - loss: 0.4942 - acc: 0.5711 - val_loss: 0.4857 - val_acc: 0.6209\n",
      "Epoch 12/20\n",
      "3376/3376 [==============================] - 547s - loss: 0.4931 - acc: 0.5690 - val_loss: 0.4857 - val_acc: 0.6256\n",
      "Epoch 13/20\n",
      "3376/3376 [==============================] - 556s - loss: 0.4927 - acc: 0.5758 - val_loss: 0.4857 - val_acc: 0.6232\n",
      "Epoch 14/20\n",
      "3376/3376 [==============================] - 560s - loss: 0.4929 - acc: 0.5815 - val_loss: 0.4856 - val_acc: 0.6220\n",
      "Epoch 15/20\n",
      "3376/3376 [==============================] - 535s - loss: 0.4936 - acc: 0.5723 - val_loss: 0.4857 - val_acc: 0.6256\n",
      "Epoch 16/20\n",
      "3376/3376 [==============================] - 555s - loss: 0.4934 - acc: 0.5741 - val_loss: 0.4856 - val_acc: 0.6244\n",
      "Epoch 17/20\n",
      "3376/3376 [==============================] - 560s - loss: 0.4928 - acc: 0.5835 - val_loss: 0.4857 - val_acc: 0.6173\n",
      "Epoch 18/20\n",
      "3376/3376 [==============================] - 563s - loss: 0.4924 - acc: 0.5933 - val_loss: 0.4856 - val_acc: 0.6244\n",
      "Epoch 19/20\n",
      "3376/3376 [==============================] - 564s - loss: 0.4932 - acc: 0.5785 - val_loss: 0.4855 - val_acc: 0.6197\n",
      "Epoch 20/20\n",
      "3376/3376 [==============================] - 551s - loss: 0.4931 - acc: 0.5832 - val_loss: 0.4855 - val_acc: 0.6232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7375e9f90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3376 samples, validate on 844 samples\n",
      "Epoch 1/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4931 - acc: 0.5770 - val_loss: 0.4855 - val_acc: 0.6197\n",
      "Epoch 2/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4922 - acc: 0.5764 - val_loss: 0.4854 - val_acc: 0.6232\n",
      "Epoch 3/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4929 - acc: 0.5800 - val_loss: 0.4855 - val_acc: 0.6173\n",
      "Epoch 4/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4931 - acc: 0.5741 - val_loss: 0.4855 - val_acc: 0.6197\n",
      "Epoch 5/50\n",
      "3376/3376 [==============================] - 551s - loss: 0.4923 - acc: 0.5859 - val_loss: 0.4854 - val_acc: 0.6173\n",
      "Epoch 6/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4927 - acc: 0.5829 - val_loss: 0.4853 - val_acc: 0.6197\n",
      "Epoch 7/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4936 - acc: 0.5714 - val_loss: 0.4855 - val_acc: 0.6161\n",
      "Epoch 8/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4931 - acc: 0.5794 - val_loss: 0.4854 - val_acc: 0.6220\n",
      "Epoch 9/50\n",
      "3376/3376 [==============================] - 558s - loss: 0.4926 - acc: 0.5820 - val_loss: 0.4855 - val_acc: 0.6137\n",
      "Epoch 10/50\n",
      "3376/3376 [==============================] - 556s - loss: 0.4931 - acc: 0.5779 - val_loss: 0.4854 - val_acc: 0.6197\n",
      "Epoch 11/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4927 - acc: 0.5806 - val_loss: 0.4854 - val_acc: 0.6161\n",
      "Epoch 12/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4925 - acc: 0.5755 - val_loss: 0.4854 - val_acc: 0.6209\n",
      "Epoch 13/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4927 - acc: 0.5791 - val_loss: 0.4854 - val_acc: 0.6185\n",
      "Epoch 14/50\n",
      "3376/3376 [==============================] - 555s - loss: 0.4924 - acc: 0.5859 - val_loss: 0.4854 - val_acc: 0.6185\n",
      "Epoch 15/50\n",
      "3376/3376 [==============================] - 560s - loss: 0.4925 - acc: 0.5800 - val_loss: 0.4855 - val_acc: 0.6149\n",
      "Epoch 16/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4926 - acc: 0.5883 - val_loss: 0.4855 - val_acc: 0.6161\n",
      "Epoch 17/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4925 - acc: 0.5838 - val_loss: 0.4854 - val_acc: 0.6149\n",
      "Epoch 18/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4924 - acc: 0.5820 - val_loss: 0.4855 - val_acc: 0.6137\n",
      "Epoch 19/50\n",
      "3376/3376 [==============================] - 553s - loss: 0.4927 - acc: 0.5835 - val_loss: 0.4854 - val_acc: 0.6161\n",
      "Epoch 20/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4921 - acc: 0.5779 - val_loss: 0.4854 - val_acc: 0.6197\n",
      "Epoch 21/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4921 - acc: 0.5933 - val_loss: 0.4852 - val_acc: 0.6220\n",
      "Epoch 22/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4929 - acc: 0.5791 - val_loss: 0.4854 - val_acc: 0.6173\n",
      "Epoch 23/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4932 - acc: 0.5681 - val_loss: 0.4855 - val_acc: 0.6114\n",
      "Epoch 24/50\n",
      "3376/3376 [==============================] - 552s - loss: 0.4927 - acc: 0.5782 - val_loss: 0.4855 - val_acc: 0.6173\n",
      "Epoch 25/50\n",
      "3376/3376 [==============================] - 561s - loss: 0.4932 - acc: 0.5812 - val_loss: 0.4854 - val_acc: 0.6173\n",
      "Epoch 26/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4931 - acc: 0.5782 - val_loss: 0.4855 - val_acc: 0.6114\n",
      "Epoch 27/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4934 - acc: 0.5752 - val_loss: 0.4856 - val_acc: 0.6090\n",
      "Epoch 28/50\n",
      "3376/3376 [==============================] - 558s - loss: 0.4931 - acc: 0.5895 - val_loss: 0.4855 - val_acc: 0.6137\n",
      "Epoch 29/50\n",
      "3376/3376 [==============================] - 558s - loss: 0.4923 - acc: 0.5865 - val_loss: 0.4853 - val_acc: 0.6126\n",
      "Epoch 30/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4922 - acc: 0.5906 - val_loss: 0.4853 - val_acc: 0.6149\n",
      "Epoch 31/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4929 - acc: 0.5903 - val_loss: 0.4854 - val_acc: 0.6114\n",
      "Epoch 32/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4925 - acc: 0.5892 - val_loss: 0.4854 - val_acc: 0.6126\n",
      "Epoch 33/50\n",
      "3376/3376 [==============================] - 550s - loss: 0.4913 - acc: 0.5995 - val_loss: 0.4852 - val_acc: 0.6149\n",
      "Epoch 34/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4920 - acc: 0.5877 - val_loss: 0.4852 - val_acc: 0.6137\n",
      "Epoch 35/50\n",
      "3376/3376 [==============================] - 561s - loss: 0.4924 - acc: 0.5838 - val_loss: 0.4851 - val_acc: 0.6137\n",
      "Epoch 36/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4920 - acc: 0.6004 - val_loss: 0.4851 - val_acc: 0.6185\n",
      "Epoch 37/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4924 - acc: 0.5886 - val_loss: 0.4852 - val_acc: 0.6173\n",
      "Epoch 38/50\n",
      "3376/3376 [==============================] - 551s - loss: 0.4923 - acc: 0.5918 - val_loss: 0.4852 - val_acc: 0.6078\n",
      "Epoch 39/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4929 - acc: 0.5921 - val_loss: 0.4852 - val_acc: 0.6173\n",
      "Epoch 40/50\n",
      "3376/3376 [==============================] - 564s - loss: 0.4922 - acc: 0.5883 - val_loss: 0.4853 - val_acc: 0.6090\n",
      "Epoch 41/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4920 - acc: 0.5900 - val_loss: 0.4852 - val_acc: 0.6137\n",
      "Epoch 42/50\n",
      "3376/3376 [==============================] - 556s - loss: 0.4922 - acc: 0.5924 - val_loss: 0.4851 - val_acc: 0.6161\n",
      "Epoch 43/50\n",
      "3376/3376 [==============================] - 558s - loss: 0.4918 - acc: 0.5942 - val_loss: 0.4851 - val_acc: 0.6090\n",
      "Epoch 44/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4923 - acc: 0.5850 - val_loss: 0.4853 - val_acc: 0.6102\n",
      "Epoch 45/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4921 - acc: 0.6013 - val_loss: 0.4851 - val_acc: 0.6090\n",
      "Epoch 46/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4920 - acc: 0.5921 - val_loss: 0.4850 - val_acc: 0.6126\n",
      "Epoch 47/50\n",
      "3376/3376 [==============================] - 551s - loss: 0.4925 - acc: 0.5906 - val_loss: 0.4852 - val_acc: 0.6090\n",
      "Epoch 48/50\n",
      "3376/3376 [==============================] - 563s - loss: 0.4921 - acc: 0.5898 - val_loss: 0.4851 - val_acc: 0.6102\n",
      "Epoch 49/50\n",
      "3376/3376 [==============================] - 562s - loss: 0.4926 - acc: 0.5877 - val_loss: 0.4851 - val_acc: 0.6078\n",
      "Epoch 50/50\n",
      "3376/3376 [==============================] - 550s - loss: 0.4914 - acc: 0.5909 - val_loss: 0.4850 - val_acc: 0.6161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb7375a23d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=50, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro = model.predict_on_batch(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save&Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__doc__',\n",
       " '__format__',\n",
       " '__getattribute__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_collected_trainable_weights',\n",
       " '_fit_loop',\n",
       " '_function_kwargs',\n",
       " '_get_node_attribute_at_index',\n",
       " '_make_predict_function',\n",
       " '_make_test_function',\n",
       " '_make_train_function',\n",
       " '_output_mask_cache',\n",
       " '_output_shape_cache',\n",
       " '_output_tensor_cache',\n",
       " '_predict_loop',\n",
       " '_standardize_user_data',\n",
       " '_test_loop',\n",
       " '_updated_config',\n",
       " 'add_inbound_node',\n",
       " 'add_loss',\n",
       " 'add_update',\n",
       " 'add_weight',\n",
       " 'assert_input_compatibility',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compile',\n",
       " 'compute_mask',\n",
       " 'constraints',\n",
       " 'container_nodes',\n",
       " 'count_params',\n",
       " 'create_input_layer',\n",
       " 'evaluate',\n",
       " 'evaluate_generator',\n",
       " 'fit',\n",
       " 'fit_generator',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_layer',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_output_shape_for',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'history',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_layers',\n",
       " 'input_layers_node_indices',\n",
       " 'input_layers_tensor_indices',\n",
       " 'input_mask',\n",
       " 'input_names',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'inputs',\n",
       " 'internal_input_shapes',\n",
       " 'internal_output_shapes',\n",
       " 'layers',\n",
       " 'layers_by_depth',\n",
       " 'load_weights',\n",
       " 'load_weights_from_hdf5_group',\n",
       " 'load_weights_from_hdf5_group_by_name',\n",
       " 'loss',\n",
       " 'loss_functions',\n",
       " 'loss_weights',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'metrics_names',\n",
       " 'metrics_tensors',\n",
       " 'name',\n",
       " 'nodes_by_depth',\n",
       " 'non_trainable_weights',\n",
       " 'optimizer',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_layers',\n",
       " 'output_layers_node_indices',\n",
       " 'output_layers_tensor_indices',\n",
       " 'output_mask',\n",
       " 'output_names',\n",
       " 'output_shape',\n",
       " 'outputs',\n",
       " 'predict',\n",
       " 'predict_function',\n",
       " 'predict_generator',\n",
       " 'predict_on_batch',\n",
       " 'regularizers',\n",
       " 'reset_states',\n",
       " 'run_internal_graph',\n",
       " 'sample_weight_mode',\n",
       " 'sample_weight_modes',\n",
       " 'sample_weights',\n",
       " 'save',\n",
       " 'save_weights',\n",
       " 'save_weights_to_hdf5_group',\n",
       " 'set_weights',\n",
       " 'state_updates',\n",
       " 'stateful',\n",
       " 'stop_training',\n",
       " 'summary',\n",
       " 'supports_masking',\n",
       " 'targets',\n",
       " 'test_function',\n",
       " 'test_on_batch',\n",
       " 'to_json',\n",
       " 'to_yaml',\n",
       " 'total_loss',\n",
       " 'train_function',\n",
       " 'train_on_batch',\n",
       " 'trainable',\n",
       " 'trainable_weights',\n",
       " 'updates',\n",
       " 'uses_learning_phase',\n",
       " 'validation_data',\n",
       " 'weights']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_collected_trainable_weights': [<CudaNdarrayType(float32, vector)>,\n",
       "  backward_lstm_1_U_c,\n",
       "  backward_lstm_1_U_f,\n",
       "  backward_lstm_1_U_i,\n",
       "  backward_lstm_1_U_o,\n",
       "  backward_lstm_1_W_c,\n",
       "  backward_lstm_1_W_f,\n",
       "  backward_lstm_1_W_i,\n",
       "  backward_lstm_1_W_o,\n",
       "  backward_lstm_1_b_c,\n",
       "  backward_lstm_1_b_f,\n",
       "  backward_lstm_1_b_i,\n",
       "  backward_lstm_1_b_o,\n",
       "  dense_1_W,\n",
       "  dense_1_b,\n",
       "  forward_lstm_1_U_c,\n",
       "  forward_lstm_1_U_f,\n",
       "  forward_lstm_1_U_i,\n",
       "  forward_lstm_1_U_o,\n",
       "  forward_lstm_1_W_c,\n",
       "  forward_lstm_1_W_f,\n",
       "  forward_lstm_1_W_i,\n",
       "  forward_lstm_1_W_o,\n",
       "  forward_lstm_1_b_c,\n",
       "  forward_lstm_1_b_f,\n",
       "  forward_lstm_1_b_i,\n",
       "  forward_lstm_1_b_o],\n",
       " '_function_kwargs': {},\n",
       " '_output_mask_cache': {'140689816233104_140699161811792': None},\n",
       " '_output_shape_cache': {},\n",
       " '_output_tensor_cache': {},\n",
       " 'built': True,\n",
       " 'container_nodes': {'attlayer_1_ib-0',\n",
       "  'bidirectional_1_ib-0',\n",
       "  'dense_1_ib-0',\n",
       "  'dropout_1_ib-0',\n",
       "  'input_1_ib-0'},\n",
       " 'history': <keras.callbacks.History at 0x7ff3d07675d0>,\n",
       " 'inbound_nodes': [<keras.engine.topology.Node at 0x7ff6511a7910>],\n",
       " 'input_layers': [<keras.engine.topology.InputLayer at 0x7ff4e6852b10>],\n",
       " 'input_layers_node_indices': [0],\n",
       " 'input_layers_tensor_indices': [0],\n",
       " 'input_names': ['input_1'],\n",
       " 'inputs': [input_1],\n",
       " 'internal_input_shapes': [(None, 4096, 2)],\n",
       " 'internal_output_shapes': [(None, 2)],\n",
       " 'layers': [<keras.engine.topology.InputLayer at 0x7ff4e6852b10>,\n",
       "  <keras.layers.wrappers.Bidirectional at 0x7ff4d233a390>,\n",
       "  <keras.layers.core.Dropout at 0x7ff4ad974550>,\n",
       "  <__main__.AttLayer at 0x7ff4ad974590>,\n",
       "  <keras.layers.core.Dense at 0x7ff4e308c450>],\n",
       " 'layers_by_depth': {0: [<keras.layers.core.Dense at 0x7ff4e308c450>],\n",
       "  1: [<__main__.AttLayer at 0x7ff4ad974590>],\n",
       "  2: [<keras.layers.core.Dropout at 0x7ff4ad974550>],\n",
       "  3: [<keras.layers.wrappers.Bidirectional at 0x7ff4d233a390>],\n",
       "  4: [<keras.engine.topology.InputLayer at 0x7ff4e6852b10>]},\n",
       " 'loss': 'mse',\n",
       " 'loss_functions': [<function keras.objectives.mean_squared_error>],\n",
       " 'loss_weights': None,\n",
       " 'metrics': ['acc'],\n",
       " 'metrics_names': ['loss', 'acc'],\n",
       " 'metrics_tensors': [mean],\n",
       " 'name': 'model_1',\n",
       " 'nodes_by_depth': {0: [<keras.engine.topology.Node at 0x7ff4e68581d0>],\n",
       "  1: [<keras.engine.topology.Node at 0x7ff4acf26e90>],\n",
       "  2: [<keras.engine.topology.Node at 0x7ff4ca58e090>],\n",
       "  3: [<keras.engine.topology.Node at 0x7ff4ad974c90>],\n",
       "  4: [<keras.engine.topology.Node at 0x7ff4d233ac90>]},\n",
       " 'optimizer': <keras.optimizers.RMSprop at 0x7ff4cf534550>,\n",
       " 'outbound_nodes': [],\n",
       " 'output_layers': [<keras.layers.core.Dense at 0x7ff4e308c450>],\n",
       " 'output_layers_node_indices': [0],\n",
       " 'output_layers_tensor_indices': [0],\n",
       " 'output_names': ['dense_1'],\n",
       " 'outputs': [Softmax.0],\n",
       " 'predict_function': None,\n",
       " 'sample_weight_mode': None,\n",
       " 'sample_weight_modes': [None],\n",
       " 'sample_weights': [dense_1_sample_weights],\n",
       " 'stop_training': False,\n",
       " 'supports_masking': False,\n",
       " 'targets': [dense_1_target],\n",
       " 'test_function': <keras.backend.theano_backend.Function at 0x7ff4cf5349d0>,\n",
       " 'total_loss': Elemwise{add,no_inplace}.0,\n",
       " 'train_function': <keras.backend.theano_backend.Function at 0x7ff3d02b8690>,\n",
       " 'trainable': True,\n",
       " 'validation_data': [array([[[  0.,   0.],\n",
       "          [  0.,   0.],\n",
       "          [  0.,   0.],\n",
       "          ..., \n",
       "          [  0.,   0.],\n",
       "          [  0.,   0.],\n",
       "          [  0.,   0.]],\n",
       "  \n",
       "         [[  0.,   5.],\n",
       "          [  2.,   1.],\n",
       "          [  1.,   0.],\n",
       "          ..., \n",
       "          [  0.,   0.],\n",
       "          [  1.,   0.],\n",
       "          [  1.,   0.]],\n",
       "  \n",
       "         [[ 47.,   0.],\n",
       "          [ 12.,   0.],\n",
       "          [  8.,   0.],\n",
       "          ..., \n",
       "          [  4.,   0.],\n",
       "          [  0.,   0.],\n",
       "          [ 19.,   0.]],\n",
       "  \n",
       "         ..., \n",
       "         [[  3.,   0.],\n",
       "          [  4.,   0.],\n",
       "          [  1.,   0.],\n",
       "          ..., \n",
       "          [ 17.,   0.],\n",
       "          [  4.,   0.],\n",
       "          [ 57.,   0.]],\n",
       "  \n",
       "         [[ 11.,   1.],\n",
       "          [  0.,   0.],\n",
       "          [  2.,   1.],\n",
       "          ..., \n",
       "          [  0.,   0.],\n",
       "          [  0.,   0.],\n",
       "          [  0.,   0.]],\n",
       "  \n",
       "         [[  1.,   0.],\n",
       "          [  1.,   0.],\n",
       "          [  4.,   1.],\n",
       "          ..., \n",
       "          [  2.,   0.],\n",
       "          [  4.,   0.],\n",
       "          [ 14.,   0.]]]), array([[ 1.,  0.],\n",
       "         [ 1.,  0.],\n",
       "         [ 1.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  1.],\n",
       "         [ 0.,  1.],\n",
       "         [ 0.,  1.]]), array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.], dtype=float32), 0.0]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./Gen_data/10_Att-BLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./Gen_data/10_Att-BLSTM.h5',by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
