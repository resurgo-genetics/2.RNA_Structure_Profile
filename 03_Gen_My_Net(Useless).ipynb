{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "1. genearate data set as CNN network input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#获得４＊Ｌ维０１矩阵的函数\n",
    "d = {'a':0, 'A':0, 'g':1, 'G':1, 'c':2, 'C':2, 't':3, 'T':3, 'N':4, 'n':4}\n",
    "def seq_to_mat(seq):\n",
    "    mat = np.zeros((5,len(seq)))\n",
    "    for i in range(len(seq)):\n",
    "        mat[d[seq[i]],i] =1\n",
    "        #mat[d[seq[i]],i] =1\n",
    "        #mat[d[seq[i]],i] = 1\n",
    "        pass\n",
    "    mat = mat[:4,:]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "seq_freq = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rootdir =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile'\n",
    "cutdown_len  = 13\n",
    "\n",
    "data = []\n",
    "with open(rootdir+'/Orig_data/sharpr.txt','r')  as f:\n",
    "    with open(rootdir+'/Orig_data/score.txt','r') as f1:     \n",
    "        while 1:\n",
    "            name = f.readline()\n",
    "            if not name :\n",
    "                break\n",
    "            sequence = f.readline()\n",
    "            scores = f1.readline().split()[1:]  #得到每个碱基的分数\n",
    "            bp_name  = name.split('_')  [:-1]\n",
    "            bp_point = name.split('_')[-1]\n",
    "            for bp_iter  in range(0,len(sequence)-cutdown_len-1):\n",
    "                bp_seq_temp = {}\n",
    "                bp_seq_temp['seq'] =sequence[bp_iter:(bp_iter+cutdown_len)].strip()\n",
    "                #seq_temp= bp_seq_temp['seq']\n",
    "                bp_seq_temp['mat'] = seq_to_mat(bp_seq_temp['seq'] )\n",
    "                bp_seq_temp['name'] = bp_name[0]+'_'+bp_name[1]+'_'+bp_name[2]+'_'+bp_name[3]+'_'+ str(int(bp_point)+bp_iter*(cutdown_len-1))\n",
    "                scores_str = scores[bp_iter:(bp_iter+cutdown_len)]\n",
    "                bp_seq_temp['score'] = [float(item) for item in scores_str]\n",
    "                data.append(bp_seq_temp)\n",
    "                `\n",
    "                \n",
    "        \n",
    "        \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "d = defaultdict(list)\n",
    "d ['aa'] .append(1)\n",
    "d['aa'].append(2)\n",
    "print d['aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print  data[1],data[282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print seq_to_mat(data[283]['seq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2 = {}\n",
    "for key in data[0].keys():\n",
    "    data2['seq'] = [item['seq'] for item in data]\n",
    "    data2['name'] = [item['name'] for item in data]\n",
    "    data2['score'] = [item['score'] for item in data]\n",
    "    data2['mat'] = [item['mat'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print data2['seq'][:2]\n",
    "print data2['name'][:2]\n",
    "print data2['score'][:2]\n",
    "print data2['mat'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#统计seq出现的次数\n",
    "\n",
    "from collections import defaultdict\n",
    "#rom collections import Counter\n",
    "\n",
    "\n",
    "seqdict_order = defaultdict(list)\n",
    "#seqdict = Counter(data2['seq'])\n",
    "for index in range(0,len(data2['seq'])):\n",
    "\n",
    "    seqdict_order[ data2['seq'][index]].append(int(index))\n",
    "\n",
    "remove_point = []\n",
    "for key in seqdict_order.keys():\n",
    "    key_dict = seqdict_order[key]\n",
    "    if 1 !=  len(key_dict):\n",
    "        for key_order in key_dict:\n",
    "            remove_point.append(key_order)\n",
    "remove_point  = sorted(remove_point)\n",
    "\n",
    "print remove_point[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(remove_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slicea = slice (1,4)\n",
    "print data2['seq'][slicea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_2_prun = {}\n",
    "slicea  =slice([1,2,3])\n",
    "for key in data2.keys:\n",
    "    data_2_prun[key] = data2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low =seqdict.most_common(500)\n",
    "print low[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hickle  as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hkl.dump(data2,'./Gen_data/CROSS_Net_input_0.hkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tips  删除u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate NetWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
    "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
    "    print 'Using Lasagne.layers.dnn (faster)'\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer\n",
    "    from lasagne.layers import MaxPool2DLayer\n",
    "    print 'Using Lasagne.layers (slower)'\n",
    "from lasagne.nonlinearities import softmax, rectify, leaky_rectify\n",
    "from lasagne.updates import adam\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from nolearn.lasagne import objective\n",
    "from nolearn.lasagne import BatchIterator\n",
    "\n",
    "from lasagne.nonlinearities import tanh \n",
    "floatX = theano.config.floatX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutdown_len = 13\n",
    "layer1 = InputLayer(shape=(None,1,cutdown_len,4))\n",
    "layer2 = DenseLayer(layer1, num_units=20 ,nonlinearity=tanh)\n",
    "network = DenseLayer(layer2,num_units=1,nonlinearity=tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "lr = theano.shared(np.float32(1e-4))\n",
    "\n",
    "net = NeuralNet(\n",
    "            network,\n",
    "            max_epochs=7,\n",
    "            update=adam,\n",
    "            update_learning_rate=lr,\n",
    "            train_split=TrainSplit(eval_size=0.1),\n",
    "                regression = True,\n",
    "            objective_loss_function = squared_error,\n",
    "            #on_epoch_finished=[\n",
    "            #    AdjustVariable(lr, target=1e-8, half_life=20)],\n",
    "            verbose=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data2['mat']\n",
    "Y = data2['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_ = [ item[(len(item)-1)/2] for item in Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(X),len(Y_)\n",
    "print Y_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ = np.zeros((len(X),4,cutdown_len))\n",
    "print X_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print  np.shape( X[284]),np.shape(X_[i,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_[:,:,:] = X[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(len(Y)):\n",
    "    if i%10000==0 :\n",
    "        print i\n",
    "    X_[i,:,:] = X[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_ = np.array(Y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import ShuffleSplit\n",
    "rs = ShuffleSplit(len(Y_), n_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train_idx, test_idx in rs:\n",
    "    X_train = X_[train_idx,:]\n",
    "    y_train = Y_[train_idx]\n",
    "    X_test = X_[test_idx,:]\n",
    "    y_test = Y_[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train[:2]\n",
    "print y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X_train[:2]\n",
    "print y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cutdown_len = 13\n",
    "layer1 = InputLayer(shape=(None,4,cutdown_len))\n",
    "layer2 = DenseLayer(layer1, num_units=20 ,nonlinearity=tanh)\n",
    "network = DenseLayer(layer2,num_units=1,nonlinearity=tanh)\n",
    "\n",
    "lr = theano.shared(np.float32(1e-4))\n",
    "\n",
    "net = NeuralNet(\n",
    "            network,\n",
    "            max_epochs=100,\n",
    "            update=adam,\n",
    "            update_learning_rate=lr,\n",
    "            train_split=TrainSplit(eval_size=0.1),\n",
    "                regression = True,\n",
    "            objective_loss_function = squared_error,\n",
    "            #on_epoch_finished=[\n",
    "            #    AdjustVariable(lr, target=1e-8, half_life=20)],\n",
    "            verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save network values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nolearn.lasagne import NeuralNet\n",
    "net.initialize_layers()\n",
    "net_params=[]\n",
    "net.save_params_to('./Gen_data/CROSS_net_values.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Network Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import DropoutLayer\n",
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
    "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
    "    print 'Using Lasagne.layers.dnn (faster)'\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer\n",
    "    from lasagne.layers import MaxPool2DLayer\n",
    "    print 'Using Lasagne.layers (slower)'\n",
    "from lasagne.nonlinearities import softmax, rectify, leaky_rectify\n",
    "from lasagne.updates import adam\n",
    "from lasagne.objectives import squared_error\n",
    "\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from nolearn.lasagne import objective\n",
    "from nolearn.lasagne import BatchIterator\n",
    "\n",
    "from lasagne.nonlinearities import tanh \n",
    "floatX = theano.config.floatX\n",
    "\n",
    "cutdown_len = 13\n",
    "layer1 = InputLayer(shape=(None,cutdown_len,4))\n",
    "layer2 = DenseLayer(layer1, num_units=20 ,nonlinearity=tanh)\n",
    "network = DenseLayer(layer2,num_units=1,nonlinearity=tanh)\n",
    "\n",
    "lr = theano.shared(np.float32(1e-4))\n",
    "\n",
    "net = NeuralNet(\n",
    "            network,\n",
    "            max_epochs=7,\n",
    "            update=adam,\n",
    "            update_learning_rate=lr,\n",
    "            train_split=TrainSplit(eval_size=0.1),\n",
    "                regression = True,\n",
    "            objective_loss_function = squared_error,\n",
    "            #on_epoch_finished=[\n",
    "            #    AdjustVariable(lr, target=1e-8, half_life=20)],\n",
    "            verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load  NetWork Values\n",
    "from nolearn.lasagne import NeuralNet\n",
    "net.initialize_layers()\n",
    "net.load_params_from('./Gen_data/CROSS_net_values.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_1 = net.predict(X_test[:2])\n",
    "print y_1,y_test[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slope, intercept, r_value, p_value, std_err = stats.linregress(y_test,y_pred[:,0])\n",
    "print r_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
