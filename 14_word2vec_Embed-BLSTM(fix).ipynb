{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README    \n",
    "1.数据输入-->添加序列,kmer序列-->切分测试/验证集-->测试集产生promoter/enhancer embeding 矩阵-->网络输入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile  =   '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/targetfinder/GM12878/output-epw/pairs.csv'\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bin', 'enhancer_chrom', 'enhancer_distance_to_promoter', 'enhancer_end', 'enhancer_name', 'enhancer_start', 'label', 'promoter_chrom', 'promoter_end', 'promoter_name', 'promoter_start', 'window_end', 'window_start', 'window_chrom', 'window_name', 'interactions_in_window', 'active_promoters_in_window']\n"
     ]
    }
   ],
   "source": [
    "#读入CSV表头\n",
    "with open(datafile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for  row in spamreader:\n",
    "        csvkeys = row\n",
    "        break\n",
    "print csvkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#分别读入正负样本[完全未处理]\n",
    "poscsv = {}\n",
    "negcsv = {}\n",
    "with open(datafile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for line in reader:\n",
    "        if line['bin']  not in poscsv.keys():\n",
    "            poscsv[line['bin']] =[]\n",
    "        if line ['bin']  not in negcsv.keys():\n",
    "            negcsv[line['bin']] =[]\n",
    "\n",
    "        record=[ line['enhancer_chrom']+' '+\n",
    "                line[ 'enhancer_start']+' '+\n",
    "                line['enhancer_end']+' '+\n",
    "                line['promoter_chrom']+' '+\n",
    "                line['promoter_start']+' '+\n",
    "                line['promoter_end']+' '+\n",
    "                line['enhancer_name']+'-'+line['promoter_name']\n",
    "                  ]\n",
    "        if line['label']==str(1):\n",
    "            poscsv[line['bin']].append(record)\n",
    "        else:\n",
    "            negcsv[line['bin']].append(record)\n",
    "\n",
    "    for key in poscsv.keys():\n",
    "        length = len(poscsv[key])\n",
    "        index = range(len(negcsv[key]))\n",
    "        np.random.shuffle(index)\n",
    "        negcsv[key] = np.array(negcsv[key])\n",
    "        negcsv[key] = negcsv[key][index[:length]]\n",
    "        #print len(negcsv[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2113 2113\n"
     ]
    }
   ],
   "source": [
    "#分别读入正负样本[处理:样本均衡]\n",
    "posdata =[]\n",
    "negdata = []\n",
    "for key in poscsv.keys():\n",
    "    posdata = np.append(posdata,poscsv[key])\n",
    "    negdata = np.append(negdata,negcsv[key])\n",
    "assert  len(posdata) == len(negdata)\n",
    "print  len(posdata),len(negdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据序号-->数据序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfasta import Fasta\n",
    "genome = Fasta('/home/yinqijin/WorkSpace/DataHub/genome.fa')\n",
    "\n",
    "#获得ｋｍｅｒ的函数\n",
    "embed_dict =[]\n",
    "def seq_to_embed_seq(seq):\n",
    "    kmer=6\n",
    "    embed_seq = []\n",
    "    if len (seq)< kmer:\n",
    "        return embed_seq\n",
    "    for i in range(kmer,len(seq)):\n",
    "        word  = seq[(i-kmer):i] \n",
    "        #if  word not in embed_dict.keys():\n",
    "        #    embed_dict[ word  ] =str( len(embed_dict.keys()) ) #给标号\n",
    "        #embed_seq.append(  str( embed_dict[ word  ]  )  )\n",
    "        embed_seq.append(  word )\n",
    "        if word not in embed_dict:\n",
    "            embed_dict.append(word)\n",
    "    return embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr17', '41080595', '41080600', 'chr17', '40728821', '40732200', 'GM12878|chr17:41080595-41080600-GM12878|chr17:40728821-40732200']\n",
      "['chr3', '144015595', '144015600', 'chr3', '143689613', '143692895', 'GM12878|chr3:144015595-144015600-GM12878|chr3:143689613-143692895']\n",
      "['chr6', '27857195', '27857200', 'chr6', '27803860', '27807953', 'GM12878|chr6:27857195-27857200-GM12878|chr6:27803860-27807953']\n",
      "pos over\n",
      "2110\n",
      "['chr19', '45677600', '45677605', 'chr19', '44455037', '44455747', 'GM12878|chr19:45677600-45677605-GM12878|chr19:44455037-44455747']\n",
      "['chr14', '23285600', '23285605', 'chr14', '24898401', '24901080', 'GM12878|chr14:23285600-23285605-GM12878|chr14:24898401-24901080']\n",
      "['chr6', '2856795', '2856800', 'chr6', '3156975', '3158233', 'GM12878|chr6:2856795-2856800-GM12878|chr6:3156975-3158233']\n",
      "['chr17', '41080595', '41080600', 'chr17', '41276342', '41278462', 'GM12878|chr17:41080595-41080600-GM12878|chr17:41276342-41278462']\n",
      "['chr19', '17576795', '17576800', 'chr19', '17621165', '17623906', 'GM12878|chr19:17576795-17576800-GM12878|chr19:17621165-17623906']\n",
      "['chr10', '70758000', '70758005', 'chr10', '70715161', '70717744', 'GM12878|chr10:70758000-70758005-GM12878|chr10:70715161-70717744']\n",
      "neg over\n",
      "4217\n"
     ]
    }
   ],
   "source": [
    "sentence_len = 806\n",
    "half_len = sentence_len/2\n",
    "\n",
    "PESeq=dict()\n",
    "PESeq['index']=[]\n",
    "PESeq['Pro-Seq']=[]\n",
    "PESeq['Pro-ESeq']=[]\n",
    "PESeq['Ehr-Seq']=[]\n",
    "PESeq['Ehr-ESeq']=[]\n",
    "PESeq['label'] =[]\n",
    "\n",
    "#正样本\n",
    "for index in range(len(posdata)):\n",
    "    line = posdata[index]\n",
    "    #print line\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    #print len(pro_seq)-len(seq_to_embed_seq(pro_seq))\n",
    "    #print seq_to_embed_seq(pro_seq)\n",
    "    \n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    #print seq_to_embed_seq(ehr_seq)\n",
    "    #break\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(1)\n",
    "print 'pos over'\n",
    "print len(PESeq['label'])  \n",
    "\n",
    "#负样本\n",
    "  \n",
    "for  index in range(len(negdata)):\n",
    "    #break\n",
    "    line = negdata[index]\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    #负样本比较多 可以严格限制 不可以 设置为1000 只剩168\n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "     \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(0)\n",
    "print 'neg over'\n",
    "\n",
    "print len(PESeq['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "index (4217,)\n",
      "Ehr-Seq (4217,)\n",
      "label (4217,)\n",
      "Ehr-ESeq (4217, 800)\n",
      "Pro-Seq (4217,)\n",
      "Pro-ESeq (4217, 800)\n",
      "pos data 2110\n",
      "neg data 2107\n"
     ]
    }
   ],
   "source": [
    "print '-'*50\n",
    "for item in PESeq.keys():\n",
    "    print item, np.shape(PESeq[item])\n",
    "print 'pos data' , sum(int( 1 ) if item ==1 else int(0) for item in PESeq['label'])\n",
    "print 'neg data',sum(int( 1) if item ==0 else int(0) for item in PESeq['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 拆分训练集和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# embeding 的训练只用训练集的数据\n",
    "from numpy.random import shuffle\n",
    "dataset_length = np.shape( PESeq['Pro-ESeq'])[0]\n",
    "SPLIT_point = int(0.85*dataset_length)\n",
    "seq_index = range(dataset_length)\n",
    "shuffle(seq_index)\n",
    "train_index = seq_index[:SPLIT_point]\n",
    "valid_index = seq_index[SPLIT_point:]\n",
    "\n",
    "Pro_Sentense = np.array(PESeq['Pro-ESeq'])[train_index]\n",
    "Ehr_Sentense = np.array(PESeq['Ehr-ESeq'])[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('./Gen_data/14_Pro_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Pro_Sentense)[0]):\n",
    "        Sentense = Pro_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')\n",
    "        \n",
    "with open('./Gen_data/14_Ehr_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Ehr_Sentense)[0]):\n",
    "        Sentense = Ehr_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Pro_sentences = word2vec.LineSentence('./Gen_data/14_Pro_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "pro_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Ehr_sentences = word2vec.LineSentence('./Gen_data/14_Ehr_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "ehr_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "TTTTTT\n",
      "AAAAAA\n",
      "ATTTTT\n",
      "AAAAAT\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2513"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print  len(ehr_word_vectors.index2word)\n",
    "for item in ehr_word_vectors.index2word[:4]:\n",
    "    print item\n",
    "print  np.shape(ehr_word_vectors['AAAAAA'])\n",
    "ehr_word_vectors.word_vec\n",
    "\n",
    "ehr_word_vectors.index2word[965]\n",
    "\n",
    "ehr_word_vectors.index2word.index('TAACTA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in\n"
     ]
    }
   ],
   "source": [
    "if 'TAACTA'  in ehr_word_vectors.index2word:\n",
    "    print 'in'\n",
    "else:\n",
    "    print 'no'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embed_dict_len = len(embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_embedding_matrix = np.random.random((embed_dict_len,100))  #4096个字,每个字100维\n",
    "for item in range(embed_dict_len):\n",
    "    word  = embed_dict[item] \n",
    "    if word in pro_word_vectors.index2word :   \n",
    "        pro_embedding_matrix[item]= pro_word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ehr_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    ehr_embedding_matrix[item]= ehr_word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  2.77023703e-01,   1.30715024e+00,  -1.05627215e+00,\n",
       "        -7.14119256e-01,   7.42347598e-01,   1.29613113e+00,\n",
       "        -8.30136120e-01,   9.37287867e-01,  -3.01328039e+00,\n",
       "        -4.43652540e-01,  -3.22756678e-01,  -9.62789178e-01,\n",
       "        -1.60254383e+00,   1.57136619e+00,  -2.18090439e+00,\n",
       "         2.48331904e+00,   3.82074654e-01,   1.97296083e+00,\n",
       "        -4.27986085e-01,   1.88951135e-01,  -6.32472411e-02,\n",
       "         3.49259049e-01,  -2.63271546e+00,   1.04935847e-01,\n",
       "        -1.61331385e-01,   6.14249483e-02,   1.39444220e+00,\n",
       "        -4.56674218e-01,   1.24256778e+00,   3.83091122e-01,\n",
       "         4.77632791e-01,  -1.88991761e+00,  -1.49155331e+00,\n",
       "        -6.65175676e-01,   3.26359129e+00,   3.69734555e-01,\n",
       "         8.13600302e-01,   1.28095910e-01,  -1.55147660e+00,\n",
       "        -1.39809108e+00,   1.75127995e+00,   1.07478642e+00,\n",
       "         7.49775290e-01,   7.24820197e-01,   4.28902835e-01,\n",
       "        -1.76280308e+00,   1.35799837e+00,  -1.10192430e+00,\n",
       "         7.14373469e-01,   5.43995976e-01,   3.59150648e-01,\n",
       "         1.72138822e+00,  -2.35577273e+00,  -1.61319494e+00,\n",
       "         1.08977091e+00,  -1.16290176e+00,  -1.00822926e+00,\n",
       "        -1.38653710e-03,  -3.41841131e-01,   1.05723035e+00,\n",
       "         4.93990600e-01,  -5.52319825e-01,   5.76323688e-01,\n",
       "         6.59892797e-01,   4.89659905e-01,  -2.22253919e+00,\n",
       "        -8.83568764e-01,   1.35183847e+00,   1.90364408e+00,\n",
       "         3.16290766e-01,  -2.67366827e-01,   2.54181790e+00,\n",
       "         2.27725601e+00,  -5.04930198e-01,   2.92168200e-01,\n",
       "         4.01097685e-02,  -5.00674486e-01,  -1.23636615e+00,\n",
       "        -9.94441092e-01,  -1.55525911e+00,  -2.64808834e-01,\n",
       "         6.65261924e-01,   3.07892263e-01,  -1.81727946e+00,\n",
       "         2.40389514e+00,   3.51978272e-01,  -7.13451743e-01,\n",
       "         3.25049609e-01,   9.51321602e-01,  -4.44755107e-01,\n",
       "         1.27447116e+00,   1.22698283e+00,  -7.56115615e-02,\n",
       "         1.24201620e+00,   4.41301525e-01,   2.79692590e-01,\n",
       "         6.35037661e-01,  -9.06045735e-02,   3.48021775e-01,\n",
       "        -1.82930779e+00])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.shape(pro_embedding_matrix)\n",
    "pro_embedding_matrix[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "output = open('./Gen_data/14_'+'GM12878'+'_embedding_matrix.pkl','w')\n",
    "pkl.dump(pro_embedding_matrix,output)\n",
    "pkl.dump(ehr_embedding_matrix,output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = open('./Gen_data/14_'+'GM12878'+'_embedding_matrix.pkl','r')\n",
    "pro_embedding_matrix = pkl.load(input_file)\n",
    "ehr_embedding_matrix = pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro_fin!\n",
      "ehr_fin!\n"
     ]
    }
   ],
   "source": [
    "pro_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_origin_sentense = PESeq['Pro-ESeq'][item]\n",
    "    pro_index_sentense = []\n",
    "    for word in  pro_origin_sentense:\n",
    "        \n",
    "        pro_index_sentense.append( pro_word_vectors.index2word.index( word))\n",
    "    pro_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'pro_fin!'\n",
    "\n",
    "ehr_input=[]\n",
    "func = lambda word: ehr_word_vectors.index2word.index(word)\n",
    "for item in range(len(PESeq['label'])):\n",
    "    ehr_origin_sentense = PESeq['Ehr-ESeq'][item]\n",
    "    ehr_index_sentense = []\n",
    "    for word in  ehr_origin_sentense:\n",
    "        ehr_index_sentense.append( ehr_word_vectors.index2word.index( word))\n",
    "    ehr_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'ehr_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)\n",
    "ehr_input = np.array(ehr_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_org = PESeq['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle as pkl\n",
    "output = open('./Gen_data/14_GM12878_Network_input.pkl','w')\n",
    "pkl.dump(pro_input,output)\n",
    "pkl.dump(ehr_input,output)\n",
    "pkl.dump(PESeq['label'],output)\n",
    "pkl.dump(train_index,output)\n",
    "pkl.dump(valid_index,output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "input_file = open('./Gen_data/14_GM12878_Network_input.pkl','r')\n",
    "pro_input = pkl.load(input_file)\n",
    "ehr_input = pkl.load(input_file)\n",
    "Y_org=pkl.load(input_file)\n",
    "train_index = pkl.load(input_file)\n",
    "valid_index = pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = to_categorical(Y_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train ,y_train =pro_input[train_index], Y[train_index]\n",
    "x_valid ,y_valid = pro_input[valid_index], Y[valid_index]\n",
    "\n",
    "x_train_ehr ,x_valid_ehr =ehr_input[train_index], ehr_input[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) [ 1.  0.]\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train[0]),y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,InputLayer\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten,Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "MAX_SEQUENCE_LENGTH = 800  \n",
    "nb_words =4096   #字典的len(keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Net1: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(2,return_sequences=True)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "model.compile(loss='mse', optimizer='rmsprop',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=20, validation_data=([x_valid, x_valid_ehr], y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net2 : Att-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "# Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 800, 200)      240800      merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 200)           200         bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             402         attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,060,602\n",
      "Trainable params: 1,060,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "rmsprop = opt.rmsprop(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=rmsprop,metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.5706 - acc: 0.4897 - val_loss: 0.5673 - val_acc: 0.4803\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5699 - acc: 0.5120 - val_loss: 0.5672 - val_acc: 0.4866\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5693 - acc: 0.5396 - val_loss: 0.5673 - val_acc: 0.4755\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.5687 - acc: 0.5608 - val_loss: 0.5667 - val_acc: 0.5103\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5682 - acc: 0.5823 - val_loss: 0.5665 - val_acc: 0.5308\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5677 - acc: 0.5918 - val_loss: 0.5668 - val_acc: 0.5071\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5669 - acc: 0.6113 - val_loss: 0.5665 - val_acc: 0.5245\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.5665 - acc: 0.6124 - val_loss: 0.5670 - val_acc: 0.4755\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5657 - acc: 0.6205 - val_loss: 0.5659 - val_acc: 0.5403\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.5654 - acc: 0.6323 - val_loss: 0.5661 - val_acc: 0.5387\n",
      "0.538705,0.544915,0.516556\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.5646 - acc: 0.6465 - val_loss: 0.5659 - val_acc: 0.5482\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5640 - acc: 0.6454 - val_loss: 0.5657 - val_acc: 0.5561\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.5631 - acc: 0.6571 - val_loss: 0.5685 - val_acc: 0.4739\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5625 - acc: 0.6596 - val_loss: 0.5658 - val_acc: 0.5498\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.5617 - acc: 0.6713 - val_loss: 0.5669 - val_acc: 0.4913\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5608 - acc: 0.6744 - val_loss: 0.5660 - val_acc: 0.5371\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5600 - acc: 0.6808 - val_loss: 0.5658 - val_acc: 0.5482\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.5588 - acc: 0.6939 - val_loss: 0.5662 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.5576 - acc: 0.7048 - val_loss: 0.5661 - val_acc: 0.5545\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5563 - acc: 0.7054 - val_loss: 0.5663 - val_acc: 0.5387\n",
      "0.538705,0.556762,0.557576\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5548 - acc: 0.7171 - val_loss: 0.5681 - val_acc: 0.5024\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5532 - acc: 0.7215 - val_loss: 0.5674 - val_acc: 0.5071\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5511 - acc: 0.7363 - val_loss: 0.5664 - val_acc: 0.5545\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5488 - acc: 0.7402 - val_loss: 0.5671 - val_acc: 0.5387\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.5464 - acc: 0.7592 - val_loss: 0.5685 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.5433 - acc: 0.7584 - val_loss: 0.5695 - val_acc: 0.5371\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5395 - acc: 0.7748 - val_loss: 0.5698 - val_acc: 0.5371\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5356 - acc: 0.7879 - val_loss: 0.5717 - val_acc: 0.5308\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.5319 - acc: 0.7902 - val_loss: 0.5745 - val_acc: 0.5245\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.5285 - acc: 0.8008 - val_loss: 0.5754 - val_acc: 0.5387\n",
      "0.538705,0.544545,0.532051\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.5232 - acc: 0.8189 - val_loss: 0.5776 - val_acc: 0.5387\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.5196 - acc: 0.8348 - val_loss: 0.5827 - val_acc: 0.5308\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.5149 - acc: 0.8504 - val_loss: 0.5844 - val_acc: 0.5419\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.5118 - acc: 0.8549 - val_loss: 0.5887 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.5071 - acc: 0.8747 - val_loss: 0.5959 - val_acc: 0.5166\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.5037 - acc: 0.8859 - val_loss: 0.5953 - val_acc: 0.5276\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.5027 - acc: 0.8917 - val_loss: 0.6002 - val_acc: 0.5166\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4994 - acc: 0.9001 - val_loss: 0.6140 - val_acc: 0.5055\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4986 - acc: 0.9046 - val_loss: 0.6025 - val_acc: 0.5150\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4948 - acc: 0.9196 - val_loss: 0.5964 - val_acc: 0.5308\n",
      "0.530806,0.536236,0.523274\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4945 - acc: 0.9224 - val_loss: 0.5963 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4915 - acc: 0.9355 - val_loss: 0.6025 - val_acc: 0.5134\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4900 - acc: 0.9400 - val_loss: 0.6051 - val_acc: 0.5071\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4877 - acc: 0.9484 - val_loss: 0.6235 - val_acc: 0.4976\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4895 - acc: 0.9417 - val_loss: 0.6072 - val_acc: 0.5103\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4857 - acc: 0.9548 - val_loss: 0.6219 - val_acc: 0.5024\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4858 - acc: 0.9542 - val_loss: 0.6153 - val_acc: 0.5103\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4836 - acc: 0.9629 - val_loss: 0.6368 - val_acc: 0.4850\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4842 - acc: 0.9609 - val_loss: 0.6076 - val_acc: 0.5150\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4820 - acc: 0.9668 - val_loss: 0.6260 - val_acc: 0.4961\n",
      "0.496051,0.534925,0.587322\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4819 - acc: 0.9701 - val_loss: 0.6049 - val_acc: 0.5245\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4812 - acc: 0.9693 - val_loss: 0.6094 - val_acc: 0.5087\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4790 - acc: 0.9780 - val_loss: 0.6046 - val_acc: 0.5245\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4791 - acc: 0.9799 - val_loss: 0.6047 - val_acc: 0.5245\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4790 - acc: 0.9771 - val_loss: 0.6198 - val_acc: 0.5087\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4778 - acc: 0.9841 - val_loss: 0.6108 - val_acc: 0.5150\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4762 - acc: 0.9860 - val_loss: 0.6300 - val_acc: 0.5039\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4762 - acc: 0.9872 - val_loss: 0.6064 - val_acc: 0.5340\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4764 - acc: 0.9883 - val_loss: 0.6130 - val_acc: 0.5498\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4760 - acc: 0.9883 - val_loss: 0.6087 - val_acc: 0.5561\n",
      "0.556082,0.533263,0.434608\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4745 - acc: 0.9908 - val_loss: 0.6377 - val_acc: 0.4866\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4757 - acc: 0.9888 - val_loss: 0.6221 - val_acc: 0.5055\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4749 - acc: 0.9886 - val_loss: 0.6150 - val_acc: 0.5150\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4742 - acc: 0.9905 - val_loss: 0.6192 - val_acc: 0.5024\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4744 - acc: 0.9914 - val_loss: 0.6135 - val_acc: 0.5150\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4728 - acc: 0.9894 - val_loss: 0.6274 - val_acc: 0.5087\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4730 - acc: 0.9911 - val_loss: 0.6284 - val_acc: 0.5103\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4729 - acc: 0.9916 - val_loss: 0.6168 - val_acc: 0.5039\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4731 - acc: 0.9914 - val_loss: 0.6096 - val_acc: 0.5166\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4722 - acc: 0.9902 - val_loss: 0.6170 - val_acc: 0.5024\n",
      "0.502370,0.534414,0.569083\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4730 - acc: 0.9905 - val_loss: 0.6193 - val_acc: 0.4992\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4698 - acc: 0.9911 - val_loss: 0.6138 - val_acc: 0.5150\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4728 - acc: 0.9902 - val_loss: 0.6078 - val_acc: 0.5166\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4716 - acc: 0.9922 - val_loss: 0.6183 - val_acc: 0.5087\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4705 - acc: 0.9933 - val_loss: 0.6174 - val_acc: 0.5087\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4712 - acc: 0.9894 - val_loss: 0.6214 - val_acc: 0.5087\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4709 - acc: 0.9919 - val_loss: 0.6113 - val_acc: 0.5134\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4718 - acc: 0.9897 - val_loss: 0.6165 - val_acc: 0.5087\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4705 - acc: 0.9919 - val_loss: 0.6094 - val_acc: 0.5182\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4701 - acc: 0.9941 - val_loss: 0.6154 - val_acc: 0.5118\n",
      "0.511848,0.536116,0.570236\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4710 - acc: 0.9911 - val_loss: 0.6141 - val_acc: 0.5087\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4686 - acc: 0.9914 - val_loss: 0.6253 - val_acc: 0.5087\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4706 - acc: 0.9916 - val_loss: 0.6054 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4701 - acc: 0.9914 - val_loss: 0.6039 - val_acc: 0.5229\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4702 - acc: 0.9900 - val_loss: 0.6036 - val_acc: 0.5229\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4701 - acc: 0.9902 - val_loss: 0.6049 - val_acc: 0.5482\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4684 - acc: 0.9925 - val_loss: 0.6241 - val_acc: 0.5087\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4701 - acc: 0.9911 - val_loss: 0.6213 - val_acc: 0.5039\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4689 - acc: 0.9922 - val_loss: 0.6129 - val_acc: 0.5103\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4692 - acc: 0.9916 - val_loss: 0.6083 - val_acc: 0.5150\n",
      "0.515008,0.535586,0.542474\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4690 - acc: 0.9930 - val_loss: 0.6124 - val_acc: 0.5039\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4690 - acc: 0.9939 - val_loss: 0.6119 - val_acc: 0.5103\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4682 - acc: 0.9939 - val_loss: 0.6093 - val_acc: 0.5150\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4692 - acc: 0.9914 - val_loss: 0.6120 - val_acc: 0.5134\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4695 - acc: 0.9914 - val_loss: 0.6205 - val_acc: 0.5055\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4682 - acc: 0.9925 - val_loss: 0.6070 - val_acc: 0.5182\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4689 - acc: 0.9916 - val_loss: 0.6079 - val_acc: 0.5150\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4677 - acc: 0.9922 - val_loss: 0.6168 - val_acc: 0.5071\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4691 - acc: 0.9916 - val_loss: 0.6080 - val_acc: 0.5087\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4684 - acc: 0.9908 - val_loss: 0.6145 - val_acc: 0.5103\n",
      "0.510269,0.534825,0.570637\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4685 - acc: 0.9933 - val_loss: 0.6123 - val_acc: 0.5134\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4682 - acc: 0.9905 - val_loss: 0.6101 - val_acc: 0.5166\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4684 - acc: 0.9919 - val_loss: 0.6029 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4678 - acc: 0.9911 - val_loss: 0.6204 - val_acc: 0.5118\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4678 - acc: 0.9927 - val_loss: 0.6215 - val_acc: 0.5103\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4680 - acc: 0.9911 - val_loss: 0.6114 - val_acc: 0.5118\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4685 - acc: 0.9914 - val_loss: 0.6086 - val_acc: 0.5071\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4679 - acc: 0.9919 - val_loss: 0.6156 - val_acc: 0.5024\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4675 - acc: 0.9908 - val_loss: 0.6118 - val_acc: 0.5103\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4679 - acc: 0.9919 - val_loss: 0.6028 - val_acc: 0.5213\n",
      "0.521327,0.535185,0.492462\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4676 - acc: 0.9914 - val_loss: 0.6047 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4676 - acc: 0.9936 - val_loss: 0.6029 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4678 - acc: 0.9927 - val_loss: 0.6024 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4673 - acc: 0.9927 - val_loss: 0.6026 - val_acc: 0.5340\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4676 - acc: 0.9927 - val_loss: 0.6046 - val_acc: 0.5498\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4675 - acc: 0.9911 - val_loss: 0.6040 - val_acc: 0.5498\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4673 - acc: 0.9936 - val_loss: 0.6022 - val_acc: 0.5213\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4673 - acc: 0.9933 - val_loss: 0.6025 - val_acc: 0.5166\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4675 - acc: 0.9919 - val_loss: 0.6023 - val_acc: 0.5292\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4667 - acc: 0.9936 - val_loss: 0.6055 - val_acc: 0.5498\n",
      "0.549763,0.535666,0.433400\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4677 - acc: 0.9905 - val_loss: 0.6012 - val_acc: 0.5261\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4673 - acc: 0.9933 - val_loss: 0.6014 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4665 - acc: 0.9925 - val_loss: 0.6160 - val_acc: 0.5482\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4675 - acc: 0.9914 - val_loss: 0.6027 - val_acc: 0.5387\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4671 - acc: 0.9930 - val_loss: 0.6054 - val_acc: 0.5545\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4672 - acc: 0.9908 - val_loss: 0.6025 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4671 - acc: 0.9911 - val_loss: 0.6047 - val_acc: 0.5529\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4667 - acc: 0.9933 - val_loss: 0.6035 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4674 - acc: 0.9916 - val_loss: 0.6013 - val_acc: 0.5276\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4669 - acc: 0.9908 - val_loss: 0.6032 - val_acc: 0.5513\n",
      "0.551343,0.536547,0.449612\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4666 - acc: 0.9922 - val_loss: 0.6045 - val_acc: 0.5529\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4668 - acc: 0.9922 - val_loss: 0.6047 - val_acc: 0.5529\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4664 - acc: 0.9916 - val_loss: 0.6036 - val_acc: 0.5466\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4674 - acc: 0.9905 - val_loss: 0.6006 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4661 - acc: 0.9914 - val_loss: 0.6118 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4666 - acc: 0.9919 - val_loss: 0.6036 - val_acc: 0.5498\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4670 - acc: 0.9925 - val_loss: 0.6039 - val_acc: 0.5498\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4661 - acc: 0.9927 - val_loss: 0.6031 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4671 - acc: 0.9908 - val_loss: 0.6020 - val_acc: 0.5419\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4668 - acc: 0.9902 - val_loss: 0.6015 - val_acc: 0.5166\n",
      "0.516588,0.537367,0.486577\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4665 - acc: 0.9916 - val_loss: 0.6014 - val_acc: 0.5197\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4667 - acc: 0.9916 - val_loss: 0.6017 - val_acc: 0.5245\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4668 - acc: 0.9919 - val_loss: 0.6012 - val_acc: 0.5324\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4661 - acc: 0.9914 - val_loss: 0.6175 - val_acc: 0.5024\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4659 - acc: 0.9905 - val_loss: 0.6058 - val_acc: 0.5229\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4671 - acc: 0.9925 - val_loss: 0.6145 - val_acc: 0.5024\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4658 - acc: 0.9919 - val_loss: 0.6016 - val_acc: 0.5213\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4665 - acc: 0.9925 - val_loss: 0.6072 - val_acc: 0.5134\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4660 - acc: 0.9911 - val_loss: 0.6025 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4663 - acc: 0.9933 - val_loss: 0.6056 - val_acc: 0.5197\n",
      "0.519747,0.538709,0.551622\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4662 - acc: 0.9936 - val_loss: 0.6055 - val_acc: 0.5213\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4668 - acc: 0.9914 - val_loss: 0.6195 - val_acc: 0.5039\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4663 - acc: 0.9902 - val_loss: 0.6117 - val_acc: 0.5055\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4664 - acc: 0.9919 - val_loss: 0.6074 - val_acc: 0.5134\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4661 - acc: 0.9925 - val_loss: 0.6074 - val_acc: 0.5134\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4664 - acc: 0.9908 - val_loss: 0.6080 - val_acc: 0.5134\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4660 - acc: 0.9936 - val_loss: 0.6087 - val_acc: 0.5134\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4655 - acc: 0.9911 - val_loss: 0.6222 - val_acc: 0.5024\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4660 - acc: 0.9930 - val_loss: 0.6179 - val_acc: 0.5024\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4666 - acc: 0.9933 - val_loss: 0.6085 - val_acc: 0.5118\n",
      "0.511848,0.540140,0.562942\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4657 - acc: 0.9925 - val_loss: 0.6176 - val_acc: 0.5024\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4662 - acc: 0.9902 - val_loss: 0.6096 - val_acc: 0.5087\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4659 - acc: 0.9922 - val_loss: 0.6131 - val_acc: 0.5071\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4663 - acc: 0.9927 - val_loss: 0.6046 - val_acc: 0.5276\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4655 - acc: 0.9939 - val_loss: 0.6178 - val_acc: 0.5039\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4660 - acc: 0.9927 - val_loss: 0.6097 - val_acc: 0.5150\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4661 - acc: 0.9925 - val_loss: 0.6198 - val_acc: 0.5024\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4657 - acc: 0.9919 - val_loss: 0.6094 - val_acc: 0.5071\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4658 - acc: 0.9930 - val_loss: 0.6097 - val_acc: 0.5118\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4656 - acc: 0.9933 - val_loss: 0.6075 - val_acc: 0.5150\n",
      "0.515008,0.539960,0.563300\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4660 - acc: 0.9925 - val_loss: 0.6163 - val_acc: 0.5024\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4659 - acc: 0.9922 - val_loss: 0.6078 - val_acc: 0.5166\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4657 - acc: 0.9908 - val_loss: 0.6116 - val_acc: 0.5419\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4653 - acc: 0.9922 - val_loss: 0.5999 - val_acc: 0.5245\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4663 - acc: 0.9919 - val_loss: 0.5992 - val_acc: 0.5292\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4657 - acc: 0.9925 - val_loss: 0.6009 - val_acc: 0.5340\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4658 - acc: 0.9916 - val_loss: 0.5998 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4654 - acc: 0.9925 - val_loss: 0.5997 - val_acc: 0.5324\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4662 - acc: 0.9908 - val_loss: 0.5992 - val_acc: 0.5276\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4654 - acc: 0.9919 - val_loss: 0.5989 - val_acc: 0.5340\n",
      "0.533965,0.541612,0.470377\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4659 - acc: 0.9927 - val_loss: 0.5989 - val_acc: 0.5355\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4652 - acc: 0.9916 - val_loss: 0.6003 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4656 - acc: 0.9911 - val_loss: 0.6009 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4659 - acc: 0.9905 - val_loss: 0.5984 - val_acc: 0.5340\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4654 - acc: 0.9908 - val_loss: 0.6005 - val_acc: 0.5340\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4658 - acc: 0.9922 - val_loss: 0.5993 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4652 - acc: 0.9933 - val_loss: 0.5992 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4655 - acc: 0.9914 - val_loss: 0.6030 - val_acc: 0.5340\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4656 - acc: 0.9919 - val_loss: 0.6068 - val_acc: 0.5134\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4653 - acc: 0.9925 - val_loss: 0.6132 - val_acc: 0.5039\n",
      "0.503949,0.541632,0.579088\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4660 - acc: 0.9919 - val_loss: 0.6067 - val_acc: 0.5182\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4653 - acc: 0.9925 - val_loss: 0.6126 - val_acc: 0.5039\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4655 - acc: 0.9927 - val_loss: 0.6034 - val_acc: 0.5292\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4651 - acc: 0.9925 - val_loss: 0.6055 - val_acc: 0.5197\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4656 - acc: 0.9930 - val_loss: 0.6060 - val_acc: 0.5229\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4648 - acc: 0.9922 - val_loss: 0.6116 - val_acc: 0.5118\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4659 - acc: 0.9914 - val_loss: 0.6082 - val_acc: 0.5166\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4654 - acc: 0.9922 - val_loss: 0.6089 - val_acc: 0.5150\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4654 - acc: 0.9922 - val_loss: 0.6073 - val_acc: 0.5150\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4653 - acc: 0.9897 - val_loss: 0.6011 - val_acc: 0.5308\n",
      "0.530806,0.542272,0.550681\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4654 - acc: 0.9916 - val_loss: 0.6008 - val_acc: 0.5245\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4654 - acc: 0.9911 - val_loss: 0.5991 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4653 - acc: 0.9902 - val_loss: 0.5984 - val_acc: 0.5324\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4653 - acc: 0.9925 - val_loss: 0.6006 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4649 - acc: 0.9905 - val_loss: 0.6066 - val_acc: 0.5419\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4658 - acc: 0.9916 - val_loss: 0.5983 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4651 - acc: 0.9925 - val_loss: 0.6014 - val_acc: 0.5498\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4651 - acc: 0.9941 - val_loss: 0.5983 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4651 - acc: 0.9925 - val_loss: 0.5991 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4647 - acc: 0.9905 - val_loss: 0.6092 - val_acc: 0.5529\n",
      "0.552923,0.544525,0.386117\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4656 - acc: 0.9916 - val_loss: 0.6017 - val_acc: 0.5529\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4653 - acc: 0.9900 - val_loss: 0.5983 - val_acc: 0.5292\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4651 - acc: 0.9930 - val_loss: 0.6042 - val_acc: 0.5308\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4652 - acc: 0.9908 - val_loss: 0.6040 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4649 - acc: 0.9922 - val_loss: 0.6086 - val_acc: 0.5166\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4653 - acc: 0.9927 - val_loss: 0.6025 - val_acc: 0.5229\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4653 - acc: 0.9919 - val_loss: 0.6035 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4647 - acc: 0.9916 - val_loss: 0.6041 - val_acc: 0.5276\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4649 - acc: 0.9936 - val_loss: 0.6037 - val_acc: 0.5213\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4646 - acc: 0.9947 - val_loss: 0.6083 - val_acc: 0.5182\n",
      "0.518167,0.542633,0.574616\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4648 - acc: 0.9922 - val_loss: 0.5999 - val_acc: 0.5292\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4653 - acc: 0.9905 - val_loss: 0.6057 - val_acc: 0.5166\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4653 - acc: 0.9908 - val_loss: 0.6056 - val_acc: 0.5166\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4652 - acc: 0.9900 - val_loss: 0.6068 - val_acc: 0.5166\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4647 - acc: 0.9930 - val_loss: 0.6026 - val_acc: 0.5261\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4650 - acc: 0.9922 - val_loss: 0.6061 - val_acc: 0.5182\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4650 - acc: 0.9939 - val_loss: 0.6055 - val_acc: 0.5118\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4651 - acc: 0.9897 - val_loss: 0.6039 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4645 - acc: 0.9919 - val_loss: 0.6006 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4651 - acc: 0.9922 - val_loss: 0.6016 - val_acc: 0.5292\n",
      "0.529226,0.543073,0.548485\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4648 - acc: 0.9922 - val_loss: 0.5994 - val_acc: 0.5276\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4648 - acc: 0.9930 - val_loss: 0.5981 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4650 - acc: 0.9930 - val_loss: 0.5985 - val_acc: 0.5387\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9927 - val_loss: 0.5982 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4648 - acc: 0.9939 - val_loss: 0.5998 - val_acc: 0.5434\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4649 - acc: 0.9925 - val_loss: 0.5980 - val_acc: 0.5403\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4647 - acc: 0.9894 - val_loss: 0.5986 - val_acc: 0.5371\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4650 - acc: 0.9930 - val_loss: 0.5999 - val_acc: 0.5482\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4647 - acc: 0.9927 - val_loss: 0.6009 - val_acc: 0.5513\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4644 - acc: 0.9927 - val_loss: 0.5984 - val_acc: 0.5387\n",
      "0.538705,0.545465,0.471014\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4653 - acc: 0.9922 - val_loss: 0.5979 - val_acc: 0.5261\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4645 - acc: 0.9925 - val_loss: 0.5976 - val_acc: 0.5292\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4647 - acc: 0.9908 - val_loss: 0.5998 - val_acc: 0.5450\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4651 - acc: 0.9916 - val_loss: 0.5995 - val_acc: 0.5482\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4642 - acc: 0.9919 - val_loss: 0.6000 - val_acc: 0.5482\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4649 - acc: 0.9925 - val_loss: 0.6023 - val_acc: 0.5434\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4648 - acc: 0.9933 - val_loss: 0.5966 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9922 - val_loss: 0.5979 - val_acc: 0.5371\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4647 - acc: 0.9927 - val_loss: 0.5977 - val_acc: 0.5276\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4647 - acc: 0.9902 - val_loss: 0.5986 - val_acc: 0.5371\n",
      "0.537125,0.545305,0.472072\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4647 - acc: 0.9914 - val_loss: 0.5982 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4643 - acc: 0.9894 - val_loss: 0.6093 - val_acc: 0.5403\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9919 - val_loss: 0.6105 - val_acc: 0.5071\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4651 - acc: 0.9916 - val_loss: 0.6068 - val_acc: 0.5182\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9922 - val_loss: 0.6043 - val_acc: 0.5197\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4646 - acc: 0.9925 - val_loss: 0.6013 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9919 - val_loss: 0.6080 - val_acc: 0.5434\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4644 - acc: 0.9911 - val_loss: 0.5979 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4651 - acc: 0.9914 - val_loss: 0.6038 - val_acc: 0.5182\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4642 - acc: 0.9897 - val_loss: 0.6004 - val_acc: 0.5324\n",
      "0.532385,0.545295,0.556886\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4648 - acc: 0.9927 - val_loss: 0.6039 - val_acc: 0.5166\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4641 - acc: 0.9914 - val_loss: 0.6040 - val_acc: 0.5229\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4649 - acc: 0.9908 - val_loss: 0.6053 - val_acc: 0.5182\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4640 - acc: 0.9919 - val_loss: 0.6083 - val_acc: 0.5166\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9911 - val_loss: 0.6058 - val_acc: 0.5197\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4646 - acc: 0.9919 - val_loss: 0.6081 - val_acc: 0.5134\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4644 - acc: 0.9911 - val_loss: 0.5979 - val_acc: 0.5261\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4647 - acc: 0.9919 - val_loss: 0.5979 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4641 - acc: 0.9919 - val_loss: 0.5982 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9914 - val_loss: 0.6058 - val_acc: 0.5197\n",
      "0.519747,0.544555,0.573034\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4642 - acc: 0.9927 - val_loss: 0.5988 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4645 - acc: 0.9905 - val_loss: 0.6011 - val_acc: 0.5261\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4646 - acc: 0.9919 - val_loss: 0.6026 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9925 - val_loss: 0.6144 - val_acc: 0.5039\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9914 - val_loss: 0.6042 - val_acc: 0.5229\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4647 - acc: 0.9916 - val_loss: 0.6004 - val_acc: 0.5292\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4644 - acc: 0.9914 - val_loss: 0.6026 - val_acc: 0.5213\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4647 - acc: 0.9905 - val_loss: 0.6025 - val_acc: 0.5182\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4643 - acc: 0.9925 - val_loss: 0.6030 - val_acc: 0.5182\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4643 - acc: 0.9911 - val_loss: 0.5988 - val_acc: 0.5324\n",
      "0.532385,0.545666,0.541796\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4642 - acc: 0.9925 - val_loss: 0.6005 - val_acc: 0.5482\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4647 - acc: 0.9911 - val_loss: 0.5982 - val_acc: 0.5292\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4643 - acc: 0.9908 - val_loss: 0.5986 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9908 - val_loss: 0.6133 - val_acc: 0.5039\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4643 - acc: 0.9916 - val_loss: 0.6054 - val_acc: 0.5245\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4644 - acc: 0.9919 - val_loss: 0.6033 - val_acc: 0.5197\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9939 - val_loss: 0.6019 - val_acc: 0.5276\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4644 - acc: 0.9925 - val_loss: 0.6012 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4640 - acc: 0.9894 - val_loss: 0.6072 - val_acc: 0.5134\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9925 - val_loss: 0.5991 - val_acc: 0.5292\n",
      "0.529226,0.547087,0.545732\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4645 - acc: 0.9911 - val_loss: 0.6016 - val_acc: 0.5276\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4642 - acc: 0.9925 - val_loss: 0.6087 - val_acc: 0.5071\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4639 - acc: 0.9916 - val_loss: 0.5982 - val_acc: 0.5403\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4643 - acc: 0.9925 - val_loss: 0.5996 - val_acc: 0.5482\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4644 - acc: 0.9914 - val_loss: 0.5981 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4643 - acc: 0.9919 - val_loss: 0.5973 - val_acc: 0.5292\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4639 - acc: 0.9925 - val_loss: 0.5995 - val_acc: 0.5466\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9925 - val_loss: 0.5982 - val_acc: 0.5434\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9933 - val_loss: 0.5985 - val_acc: 0.5434\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4639 - acc: 0.9911 - val_loss: 0.5973 - val_acc: 0.5371\n",
      "0.537125,0.546006,0.499145\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4644 - acc: 0.9927 - val_loss: 0.5990 - val_acc: 0.5387\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9925 - val_loss: 0.5976 - val_acc: 0.5403\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9911 - val_loss: 0.5995 - val_acc: 0.5308\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9902 - val_loss: 0.6085 - val_acc: 0.5103\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4637 - acc: 0.9919 - val_loss: 0.5968 - val_acc: 0.5324\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4641 - acc: 0.9911 - val_loss: 0.5977 - val_acc: 0.5340\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9905 - val_loss: 0.6000 - val_acc: 0.5561\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4642 - acc: 0.9916 - val_loss: 0.5977 - val_acc: 0.5371\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4642 - acc: 0.9919 - val_loss: 0.6004 - val_acc: 0.5292\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4637 - acc: 0.9925 - val_loss: 0.6045 - val_acc: 0.5150\n",
      "0.515008,0.547217,0.569425\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4639 - acc: 0.9936 - val_loss: 0.6007 - val_acc: 0.5276\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4643 - acc: 0.9925 - val_loss: 0.6002 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4639 - acc: 0.9925 - val_loss: 0.6121 - val_acc: 0.5055\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9916 - val_loss: 0.6072 - val_acc: 0.5118\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9914 - val_loss: 0.5975 - val_acc: 0.5371\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9916 - val_loss: 0.5992 - val_acc: 0.5419\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4644 - acc: 0.9905 - val_loss: 0.5978 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4640 - acc: 0.9936 - val_loss: 0.5976 - val_acc: 0.5419\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9908 - val_loss: 0.5989 - val_acc: 0.5292\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4636 - acc: 0.9927 - val_loss: 0.6061 - val_acc: 0.5245\n",
      "0.524487,0.546617,0.581363\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4639 - acc: 0.9902 - val_loss: 0.5972 - val_acc: 0.5355\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4643 - acc: 0.9902 - val_loss: 0.5992 - val_acc: 0.5498\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4637 - acc: 0.9908 - val_loss: 0.6043 - val_acc: 0.5213\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4644 - acc: 0.9911 - val_loss: 0.5976 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4640 - acc: 0.9919 - val_loss: 0.5984 - val_acc: 0.5466\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4638 - acc: 0.9914 - val_loss: 0.5967 - val_acc: 0.5387\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9916 - val_loss: 0.5968 - val_acc: 0.5403\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4640 - acc: 0.9919 - val_loss: 0.5966 - val_acc: 0.5371\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4640 - acc: 0.9911 - val_loss: 0.6082 - val_acc: 0.5087\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9930 - val_loss: 0.5989 - val_acc: 0.5355\n",
      "0.535545,0.548378,0.554545\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4642 - acc: 0.9900 - val_loss: 0.5982 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4638 - acc: 0.9911 - val_loss: 0.6013 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4640 - acc: 0.9902 - val_loss: 0.6018 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4638 - acc: 0.9930 - val_loss: 0.5972 - val_acc: 0.5419\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4637 - acc: 0.9916 - val_loss: 0.5967 - val_acc: 0.5324\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4640 - acc: 0.9900 - val_loss: 0.5970 - val_acc: 0.5387\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4640 - acc: 0.9919 - val_loss: 0.5967 - val_acc: 0.5387\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4638 - acc: 0.9911 - val_loss: 0.5973 - val_acc: 0.5213\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4640 - acc: 0.9925 - val_loss: 0.5968 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9902 - val_loss: 0.5993 - val_acc: 0.5340\n",
      "0.533965,0.547748,0.555053\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4639 - acc: 0.9919 - val_loss: 0.6023 - val_acc: 0.5229\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4637 - acc: 0.9916 - val_loss: 0.6053 - val_acc: 0.5150\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4641 - acc: 0.9922 - val_loss: 0.6020 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9927 - val_loss: 0.6041 - val_acc: 0.5213\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9888 - val_loss: 0.5999 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9916 - val_loss: 0.5974 - val_acc: 0.5292\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9919 - val_loss: 0.5973 - val_acc: 0.5434\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4639 - acc: 0.9930 - val_loss: 0.5971 - val_acc: 0.5434\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4638 - acc: 0.9925 - val_loss: 0.5981 - val_acc: 0.5482\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9933 - val_loss: 0.5978 - val_acc: 0.5261\n",
      "0.526066,0.548018,0.520767\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4639 - acc: 0.9919 - val_loss: 0.5968 - val_acc: 0.5466\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4638 - acc: 0.9911 - val_loss: 0.5966 - val_acc: 0.5434\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4637 - acc: 0.9919 - val_loss: 0.5989 - val_acc: 0.5450\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4638 - acc: 0.9914 - val_loss: 0.5967 - val_acc: 0.5450\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9919 - val_loss: 0.5969 - val_acc: 0.5292\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9902 - val_loss: 0.6010 - val_acc: 0.5292\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4639 - acc: 0.9925 - val_loss: 0.6012 - val_acc: 0.5324\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9911 - val_loss: 0.5971 - val_acc: 0.5245\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4637 - acc: 0.9908 - val_loss: 0.5989 - val_acc: 0.5324\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4642 - acc: 0.9891 - val_loss: 0.5969 - val_acc: 0.5229\n",
      "0.522907,0.548539,0.516026\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4635 - acc: 0.9927 - val_loss: 0.5972 - val_acc: 0.5482\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4636 - acc: 0.9914 - val_loss: 0.5988 - val_acc: 0.5450\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4637 - acc: 0.9922 - val_loss: 0.5966 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4641 - acc: 0.9900 - val_loss: 0.5977 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9922 - val_loss: 0.5983 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4637 - acc: 0.9900 - val_loss: 0.5985 - val_acc: 0.5419\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9911 - val_loss: 0.5968 - val_acc: 0.5371\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4642 - acc: 0.9908 - val_loss: 0.5976 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4638 - acc: 0.9902 - val_loss: 0.5995 - val_acc: 0.5498\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4634 - acc: 0.9911 - val_loss: 0.6065 - val_acc: 0.5071\n",
      "0.507109,0.548388,0.569061\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4638 - acc: 0.9925 - val_loss: 0.6038 - val_acc: 0.5197\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4637 - acc: 0.9902 - val_loss: 0.5979 - val_acc: 0.5340\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4637 - acc: 0.9925 - val_loss: 0.6003 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4637 - acc: 0.9911 - val_loss: 0.5979 - val_acc: 0.5261\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9922 - val_loss: 0.5976 - val_acc: 0.5340\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4635 - acc: 0.9916 - val_loss: 0.6001 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4639 - acc: 0.9914 - val_loss: 0.5979 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4636 - acc: 0.9911 - val_loss: 0.5969 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4636 - acc: 0.9908 - val_loss: 0.5971 - val_acc: 0.5466\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4636 - acc: 0.9925 - val_loss: 0.5980 - val_acc: 0.5498\n",
      "0.549763,0.548719,0.478976\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9905 - val_loss: 0.5982 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 62s - loss: 0.4638 - acc: 0.9936 - val_loss: 0.5965 - val_acc: 0.5324\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 64s - loss: 0.4636 - acc: 0.9916 - val_loss: 0.5969 - val_acc: 0.5308\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 63s - loss: 0.4638 - acc: 0.9922 - val_loss: 0.5968 - val_acc: 0.5387\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4635 - acc: 0.9911 - val_loss: 0.5981 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4635 - acc: 0.9914 - val_loss: 0.5964 - val_acc: 0.5276\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4636 - acc: 0.9919 - val_loss: 0.5968 - val_acc: 0.5340\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9914 - val_loss: 0.6002 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4637 - acc: 0.9919 - val_loss: 0.6024 - val_acc: 0.5213\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 64s - loss: 0.4637 - acc: 0.9902 - val_loss: 0.6052 - val_acc: 0.5150\n",
      "0.515008,0.549850,0.574202\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4632 - acc: 0.9900 - val_loss: 0.5984 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4637 - acc: 0.9902 - val_loss: 0.5972 - val_acc: 0.5245\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4635 - acc: 0.9925 - val_loss: 0.5967 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4637 - acc: 0.9922 - val_loss: 0.5971 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 63s - loss: 0.4635 - acc: 0.9911 - val_loss: 0.5998 - val_acc: 0.5292\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 61s - loss: 0.4635 - acc: 0.9925 - val_loss: 0.6030 - val_acc: 0.5229\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 62s - loss: 0.4633 - acc: 0.9911 - val_loss: 0.6050 - val_acc: 0.5166\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4638 - acc: 0.9916 - val_loss: 0.5998 - val_acc: 0.5324\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.6055 - val_acc: 0.5197\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9914 - val_loss: 0.5970 - val_acc: 0.5434\n",
      "0.543444,0.548629,0.507666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cell_type = 'GM12878'\n",
    "with open('./Gen_data/14_'+cell_type+'_newwork_result.txt','w') as f:\n",
    "    f.write( ','.join(  ['acc','auc','f1_score']))\n",
    "    for iter_index in range(40):\n",
    "        model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=10, validation_data=([x_valid, x_valid_ehr], y_valid))\n",
    "        pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "        y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "        acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "        auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "        f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "        f.write('{:2f},{:2f},{:2f}'.format(acc,auc,f1_score))\n",
    "        print '{:2f},{:2f},{:2f}'.format(acc,auc,f1_score)\n",
    "        if  iter_index%19==0 :\n",
    "            model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:03d}.h5'.format (iter_index*10))\n",
    "    model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:s}.h5'.format ('fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9911 - val_loss: 0.5974 - val_acc: 0.5498\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9900 - val_loss: 0.5966 - val_acc: 0.5403\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4635 - acc: 0.9914 - val_loss: 0.6009 - val_acc: 0.5197\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9911 - val_loss: 0.6155 - val_acc: 0.4976\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4640 - acc: 0.9927 - val_loss: 0.5975 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4634 - acc: 0.9911 - val_loss: 0.5973 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9900 - val_loss: 0.6087 - val_acc: 0.5055\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9925 - val_loss: 0.5980 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4637 - acc: 0.9891 - val_loss: 0.5962 - val_acc: 0.5387\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4634 - acc: 0.9908 - val_loss: 0.6049 - val_acc: 0.5118\n",
      "0.511848,0.549800,0.569038\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5969 - val_acc: 0.5513\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4638 - acc: 0.9897 - val_loss: 0.5966 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9919 - val_loss: 0.5983 - val_acc: 0.5434\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9905 - val_loss: 0.5970 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9880 - val_loss: 0.5968 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4637 - acc: 0.9902 - val_loss: 0.5968 - val_acc: 0.5387\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4634 - acc: 0.9930 - val_loss: 0.5985 - val_acc: 0.5308\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9908 - val_loss: 0.5970 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5976 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9922 - val_loss: 0.5991 - val_acc: 0.5276\n",
      "0.527646,0.548769,0.547655\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9933 - val_loss: 0.5967 - val_acc: 0.5371\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9919 - val_loss: 0.5972 - val_acc: 0.5513\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9919 - val_loss: 0.5977 - val_acc: 0.5498\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4634 - acc: 0.9902 - val_loss: 0.6000 - val_acc: 0.5340\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9927 - val_loss: 0.5973 - val_acc: 0.5308\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9902 - val_loss: 0.6005 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9927 - val_loss: 0.6020 - val_acc: 0.5245\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4634 - acc: 0.9894 - val_loss: 0.6031 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4636 - acc: 0.9894 - val_loss: 0.5988 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9902 - val_loss: 0.5997 - val_acc: 0.5355\n",
      "0.535545,0.550210,0.563798\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9902 - val_loss: 0.6020 - val_acc: 0.5229\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9911 - val_loss: 0.6012 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9902 - val_loss: 0.5970 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4633 - acc: 0.9908 - val_loss: 0.5973 - val_acc: 0.5513\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4635 - acc: 0.9902 - val_loss: 0.5975 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4634 - acc: 0.9914 - val_loss: 0.5972 - val_acc: 0.5450\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4634 - acc: 0.9916 - val_loss: 0.5965 - val_acc: 0.5450\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5978 - val_acc: 0.5276\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4633 - acc: 0.9905 - val_loss: 0.5999 - val_acc: 0.5482\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9933 - val_loss: 0.5969 - val_acc: 0.5466\n",
      "0.546603,0.549700,0.507719\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4633 - acc: 0.9922 - val_loss: 0.5973 - val_acc: 0.5340\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4634 - acc: 0.9911 - val_loss: 0.5967 - val_acc: 0.5387\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4633 - acc: 0.9916 - val_loss: 0.5975 - val_acc: 0.5466\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9902 - val_loss: 0.5977 - val_acc: 0.5498\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4633 - acc: 0.9916 - val_loss: 0.5962 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9911 - val_loss: 0.6017 - val_acc: 0.5213\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4634 - acc: 0.9922 - val_loss: 0.6002 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4632 - acc: 0.9897 - val_loss: 0.5999 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.5977 - val_acc: 0.5513\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4633 - acc: 0.9916 - val_loss: 0.5982 - val_acc: 0.5450\n",
      "0.545024,0.550440,0.458647\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9905 - val_loss: 0.5968 - val_acc: 0.5371\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9936 - val_loss: 0.5979 - val_acc: 0.5466\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.6026 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4634 - acc: 0.9927 - val_loss: 0.5973 - val_acc: 0.5513\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4633 - acc: 0.9914 - val_loss: 0.5973 - val_acc: 0.5561\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9905 - val_loss: 0.6011 - val_acc: 0.5261\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9916 - val_loss: 0.5975 - val_acc: 0.5529\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4635 - acc: 0.9902 - val_loss: 0.6000 - val_acc: 0.5292\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9911 - val_loss: 0.5974 - val_acc: 0.5355\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4633 - acc: 0.9908 - val_loss: 0.5981 - val_acc: 0.5355\n",
      "0.535545,0.548388,0.543478\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4634 - acc: 0.9886 - val_loss: 0.6003 - val_acc: 0.5245\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4633 - acc: 0.9916 - val_loss: 0.6007 - val_acc: 0.5261\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9922 - val_loss: 0.6017 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9914 - val_loss: 0.6049 - val_acc: 0.5229\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4632 - acc: 0.9914 - val_loss: 0.5972 - val_acc: 0.5498\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.5972 - val_acc: 0.5466\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9897 - val_loss: 0.5990 - val_acc: 0.5466\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9925 - val_loss: 0.6043 - val_acc: 0.5213\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9900 - val_loss: 0.5999 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9905 - val_loss: 0.5980 - val_acc: 0.5276\n",
      "0.527646,0.549339,0.550376\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.6014 - val_acc: 0.5182\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4634 - acc: 0.9902 - val_loss: 0.5976 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9911 - val_loss: 0.6033 - val_acc: 0.5245\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9914 - val_loss: 0.6031 - val_acc: 0.5261\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9930 - val_loss: 0.5971 - val_acc: 0.5340\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4631 - acc: 0.9908 - val_loss: 0.5976 - val_acc: 0.5545\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9930 - val_loss: 0.5971 - val_acc: 0.5529\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.5966 - val_acc: 0.5482\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4632 - acc: 0.9919 - val_loss: 0.5972 - val_acc: 0.5340\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4632 - acc: 0.9925 - val_loss: 0.5996 - val_acc: 0.5292\n",
      "0.529226,0.549660,0.549849\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9911 - val_loss: 0.5969 - val_acc: 0.5482\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4631 - acc: 0.9905 - val_loss: 0.5971 - val_acc: 0.5545\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9922 - val_loss: 0.6022 - val_acc: 0.5213\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.5968 - val_acc: 0.5498\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4633 - acc: 0.9905 - val_loss: 0.5999 - val_acc: 0.5245\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9914 - val_loss: 0.5967 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9941 - val_loss: 0.5970 - val_acc: 0.5340\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4633 - acc: 0.9927 - val_loss: 0.6006 - val_acc: 0.5292\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4628 - acc: 0.9919 - val_loss: 0.5967 - val_acc: 0.5419\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4633 - acc: 0.9919 - val_loss: 0.5969 - val_acc: 0.5482\n",
      "0.548183,0.550130,0.508591\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9914 - val_loss: 0.5965 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4631 - acc: 0.9922 - val_loss: 0.5975 - val_acc: 0.5340\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4632 - acc: 0.9930 - val_loss: 0.5969 - val_acc: 0.5403\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4631 - acc: 0.9922 - val_loss: 0.6010 - val_acc: 0.5261\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.6061 - val_acc: 0.5197\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4633 - acc: 0.9877 - val_loss: 0.5988 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9908 - val_loss: 0.5998 - val_acc: 0.5245\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4632 - acc: 0.9914 - val_loss: 0.6000 - val_acc: 0.5276\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9905 - val_loss: 0.5990 - val_acc: 0.5276\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4632 - acc: 0.9916 - val_loss: 0.5985 - val_acc: 0.5308\n",
      "0.530806,0.550691,0.546565\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9908 - val_loss: 0.5968 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4632 - acc: 0.9911 - val_loss: 0.5995 - val_acc: 0.5213\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.5989 - val_acc: 0.5276\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4630 - acc: 0.9916 - val_loss: 0.5989 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9905 - val_loss: 0.5985 - val_acc: 0.5371\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9922 - val_loss: 0.5986 - val_acc: 0.5434\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4629 - acc: 0.9939 - val_loss: 0.5968 - val_acc: 0.5419\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4632 - acc: 0.9905 - val_loss: 0.5977 - val_acc: 0.5577\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.6002 - val_acc: 0.5245\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5983 - val_acc: 0.5340\n",
      "0.533965,0.550230,0.548239\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4633 - acc: 0.9911 - val_loss: 0.5977 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.5978 - val_acc: 0.5340\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9902 - val_loss: 0.5971 - val_acc: 0.5324\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9911 - val_loss: 0.5979 - val_acc: 0.5577\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4632 - acc: 0.9927 - val_loss: 0.5972 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4631 - acc: 0.9914 - val_loss: 0.6018 - val_acc: 0.5276\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4630 - acc: 0.9916 - val_loss: 0.5999 - val_acc: 0.5213\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4630 - acc: 0.9925 - val_loss: 0.5969 - val_acc: 0.5434\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 61s - loss: 0.4630 - acc: 0.9900 - val_loss: 0.5973 - val_acc: 0.5324\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4632 - acc: 0.9908 - val_loss: 0.5971 - val_acc: 0.5466\n",
      "0.546603,0.550410,0.514382\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4630 - acc: 0.9925 - val_loss: 0.5969 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9914 - val_loss: 0.5986 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4632 - acc: 0.9908 - val_loss: 0.5996 - val_acc: 0.5355\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9925 - val_loss: 0.5974 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9902 - val_loss: 0.5974 - val_acc: 0.5387\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9927 - val_loss: 0.5977 - val_acc: 0.5513\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9922 - val_loss: 0.5971 - val_acc: 0.5450\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9908 - val_loss: 0.6028 - val_acc: 0.5245\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9900 - val_loss: 0.5974 - val_acc: 0.5340\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4631 - acc: 0.9914 - val_loss: 0.5969 - val_acc: 0.5403\n",
      "0.540284,0.549850,0.510924\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9925 - val_loss: 0.5968 - val_acc: 0.5482\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4631 - acc: 0.9900 - val_loss: 0.6023 - val_acc: 0.5213\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9916 - val_loss: 0.5981 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4629 - acc: 0.9930 - val_loss: 0.6011 - val_acc: 0.5245\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9911 - val_loss: 0.5998 - val_acc: 0.5292\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9908 - val_loss: 0.5971 - val_acc: 0.5419\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9902 - val_loss: 0.5978 - val_acc: 0.5419\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9925 - val_loss: 0.5974 - val_acc: 0.5403\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9902 - val_loss: 0.6042 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4628 - acc: 0.9908 - val_loss: 0.5976 - val_acc: 0.5434\n",
      "0.543444,0.550410,0.497391\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5969 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4629 - acc: 0.9897 - val_loss: 0.5976 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9894 - val_loss: 0.6016 - val_acc: 0.5292\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4629 - acc: 0.9922 - val_loss: 0.5980 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9922 - val_loss: 0.5972 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4627 - acc: 0.9922 - val_loss: 0.6021 - val_acc: 0.5276\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4631 - acc: 0.9916 - val_loss: 0.5972 - val_acc: 0.5419\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4628 - acc: 0.9922 - val_loss: 0.5977 - val_acc: 0.5434\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9905 - val_loss: 0.5970 - val_acc: 0.5371\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9911 - val_loss: 0.5980 - val_acc: 0.5324\n",
      "0.532385,0.551582,0.521036\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9900 - val_loss: 0.5968 - val_acc: 0.5403\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9902 - val_loss: 0.5974 - val_acc: 0.5324\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9922 - val_loss: 0.5970 - val_acc: 0.5387\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4630 - acc: 0.9894 - val_loss: 0.5975 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9914 - val_loss: 0.5982 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9908 - val_loss: 0.6003 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4629 - acc: 0.9908 - val_loss: 0.5987 - val_acc: 0.5482\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4629 - acc: 0.9902 - val_loss: 0.5972 - val_acc: 0.5434\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9908 - val_loss: 0.5995 - val_acc: 0.5276\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9919 - val_loss: 0.5976 - val_acc: 0.5434\n",
      "0.543444,0.552332,0.536116\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4630 - acc: 0.9883 - val_loss: 0.5977 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4630 - acc: 0.9908 - val_loss: 0.5985 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9919 - val_loss: 0.6013 - val_acc: 0.5324\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4629 - acc: 0.9911 - val_loss: 0.6018 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4631 - acc: 0.9905 - val_loss: 0.5982 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4628 - acc: 0.9919 - val_loss: 0.6017 - val_acc: 0.5213\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9908 - val_loss: 0.5973 - val_acc: 0.5387\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4630 - acc: 0.9911 - val_loss: 0.5978 - val_acc: 0.5355\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9905 - val_loss: 0.5999 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9900 - val_loss: 0.5981 - val_acc: 0.5340\n",
      "0.533965,0.550020,0.515599\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9922 - val_loss: 0.5977 - val_acc: 0.5419\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9905 - val_loss: 0.5990 - val_acc: 0.5466\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4630 - acc: 0.9902 - val_loss: 0.5990 - val_acc: 0.5261\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5987 - val_acc: 0.5450\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9933 - val_loss: 0.5972 - val_acc: 0.5387\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9900 - val_loss: 0.5978 - val_acc: 0.5450\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9902 - val_loss: 0.5984 - val_acc: 0.5466\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9905 - val_loss: 0.5973 - val_acc: 0.5387\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4631 - acc: 0.9905 - val_loss: 0.5993 - val_acc: 0.5229\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5986 - val_acc: 0.5276\n",
      "0.527646,0.552052,0.540707\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9914 - val_loss: 0.5989 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9916 - val_loss: 0.5976 - val_acc: 0.5355\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4629 - acc: 0.9911 - val_loss: 0.5977 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9891 - val_loss: 0.6079 - val_acc: 0.5103\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9914 - val_loss: 0.6001 - val_acc: 0.5276\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9900 - val_loss: 0.6028 - val_acc: 0.5245\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4629 - acc: 0.9897 - val_loss: 0.5999 - val_acc: 0.5498\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9911 - val_loss: 0.6005 - val_acc: 0.5213\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9930 - val_loss: 0.5974 - val_acc: 0.5387\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9902 - val_loss: 0.5980 - val_acc: 0.5466\n",
      "0.546603,0.552052,0.534846\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4630 - acc: 0.9900 - val_loss: 0.5998 - val_acc: 0.5229\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4627 - acc: 0.9894 - val_loss: 0.5994 - val_acc: 0.5498\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9897 - val_loss: 0.5986 - val_acc: 0.5292\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.6001 - val_acc: 0.5229\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9911 - val_loss: 0.5975 - val_acc: 0.5355\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9905 - val_loss: 0.5983 - val_acc: 0.5387\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9902 - val_loss: 0.5976 - val_acc: 0.5482\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9908 - val_loss: 0.5989 - val_acc: 0.5419\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.5980 - val_acc: 0.5403\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5991 - val_acc: 0.5292\n",
      "0.529226,0.551952,0.537267\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9916 - val_loss: 0.5974 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4627 - acc: 0.9902 - val_loss: 0.5990 - val_acc: 0.5513\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9911 - val_loss: 0.5976 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9902 - val_loss: 0.5973 - val_acc: 0.5434\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9905 - val_loss: 0.5976 - val_acc: 0.5419\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9908 - val_loss: 0.5976 - val_acc: 0.5403\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9914 - val_loss: 0.6017 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4629 - acc: 0.9914 - val_loss: 0.5976 - val_acc: 0.5387\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5976 - val_acc: 0.5403\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4628 - acc: 0.9914 - val_loss: 0.5979 - val_acc: 0.5403\n",
      "0.540284,0.552232,0.507614\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4629 - acc: 0.9894 - val_loss: 0.6002 - val_acc: 0.5261\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.5987 - val_acc: 0.5419\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9914 - val_loss: 0.6029 - val_acc: 0.5355\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5976 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9927 - val_loss: 0.6008 - val_acc: 0.5197\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6001 - val_acc: 0.5213\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.5980 - val_acc: 0.5450\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9902 - val_loss: 0.5975 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.5977 - val_acc: 0.5434\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4627 - acc: 0.9919 - val_loss: 0.5989 - val_acc: 0.5466\n",
      "0.546603,0.553794,0.492035\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9897 - val_loss: 0.5980 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9900 - val_loss: 0.5977 - val_acc: 0.5403\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.5984 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4628 - acc: 0.9925 - val_loss: 0.5985 - val_acc: 0.5387\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9914 - val_loss: 0.5982 - val_acc: 0.5419\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6010 - val_acc: 0.5197\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5994 - val_acc: 0.5340\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4629 - acc: 0.9902 - val_loss: 0.5977 - val_acc: 0.5419\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.5978 - val_acc: 0.5450\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4627 - acc: 0.9916 - val_loss: 0.5982 - val_acc: 0.5292\n",
      "0.529226,0.553463,0.529968\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4628 - acc: 0.9902 - val_loss: 0.5981 - val_acc: 0.5466\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4628 - acc: 0.9908 - val_loss: 0.5977 - val_acc: 0.5419\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4628 - acc: 0.9900 - val_loss: 0.5993 - val_acc: 0.5371\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9911 - val_loss: 0.5998 - val_acc: 0.5434\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4629 - acc: 0.9922 - val_loss: 0.6002 - val_acc: 0.5292\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5984 - val_acc: 0.5450\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.6030 - val_acc: 0.5261\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4627 - acc: 0.9914 - val_loss: 0.5981 - val_acc: 0.5403\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4628 - acc: 0.9908 - val_loss: 0.5985 - val_acc: 0.5387\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4626 - acc: 0.9914 - val_loss: 0.6022 - val_acc: 0.5261\n",
      "0.526066,0.552643,0.557522\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5994 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6012 - val_acc: 0.5197\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.6012 - val_acc: 0.5182\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9916 - val_loss: 0.6019 - val_acc: 0.5245\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5988 - val_acc: 0.5466\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9897 - val_loss: 0.5999 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9886 - val_loss: 0.5982 - val_acc: 0.5419\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4627 - acc: 0.9919 - val_loss: 0.5995 - val_acc: 0.5292\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9916 - val_loss: 0.6050 - val_acc: 0.5292\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9891 - val_loss: 0.6007 - val_acc: 0.5245\n",
      "0.524487,0.551021,0.546003\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6021 - val_acc: 0.5245\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9919 - val_loss: 0.6006 - val_acc: 0.5340\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4627 - acc: 0.9900 - val_loss: 0.5983 - val_acc: 0.5403\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4627 - acc: 0.9916 - val_loss: 0.5994 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9911 - val_loss: 0.6010 - val_acc: 0.5324\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9891 - val_loss: 0.5988 - val_acc: 0.5513\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.6020 - val_acc: 0.5229\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9880 - val_loss: 0.6004 - val_acc: 0.5308\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9888 - val_loss: 0.5995 - val_acc: 0.5450\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.5988 - val_acc: 0.5419\n",
      "0.541864,0.553594,0.526144\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9888 - val_loss: 0.5993 - val_acc: 0.5387\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6020 - val_acc: 0.5229\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9897 - val_loss: 0.5998 - val_acc: 0.5292\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9914 - val_loss: 0.6007 - val_acc: 0.5229\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.5984 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.5995 - val_acc: 0.5466\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.6023 - val_acc: 0.5245\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.6015 - val_acc: 0.5261\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9911 - val_loss: 0.6007 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9894 - val_loss: 0.6010 - val_acc: 0.5245\n",
      "0.524487,0.552933,0.548726\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9902 - val_loss: 0.6011 - val_acc: 0.5213\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9914 - val_loss: 0.5992 - val_acc: 0.5276\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4628 - acc: 0.9888 - val_loss: 0.5989 - val_acc: 0.5482\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4624 - acc: 0.9897 - val_loss: 0.6029 - val_acc: 0.5466\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5980 - val_acc: 0.5403\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4627 - acc: 0.9911 - val_loss: 0.5995 - val_acc: 0.5324\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9902 - val_loss: 0.6009 - val_acc: 0.5197\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9891 - val_loss: 0.5987 - val_acc: 0.5419\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9902 - val_loss: 0.5986 - val_acc: 0.5403\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9916 - val_loss: 0.5989 - val_acc: 0.5403\n",
      "0.540284,0.552833,0.507614\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.5985 - val_acc: 0.5403\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9919 - val_loss: 0.5997 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9905 - val_loss: 0.5996 - val_acc: 0.5529\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4628 - acc: 0.9905 - val_loss: 0.5990 - val_acc: 0.5387\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9894 - val_loss: 0.5994 - val_acc: 0.5324\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9905 - val_loss: 0.5995 - val_acc: 0.5403\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.5990 - val_acc: 0.5513\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.5981 - val_acc: 0.5340\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4627 - acc: 0.9902 - val_loss: 0.5984 - val_acc: 0.5466\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.5998 - val_acc: 0.5292\n",
      "0.529226,0.553654,0.542945\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9891 - val_loss: 0.5987 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5984 - val_acc: 0.5434\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5989 - val_acc: 0.5498\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9908 - val_loss: 0.5991 - val_acc: 0.5276\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9900 - val_loss: 0.5992 - val_acc: 0.5434\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9897 - val_loss: 0.6002 - val_acc: 0.5292\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9914 - val_loss: 0.6018 - val_acc: 0.5229\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4627 - acc: 0.9888 - val_loss: 0.6012 - val_acc: 0.5229\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.6011 - val_acc: 0.5213\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9922 - val_loss: 0.6032 - val_acc: 0.5197\n",
      "0.519747,0.552012,0.552941\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9894 - val_loss: 0.5993 - val_acc: 0.5324\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6024 - val_acc: 0.5182\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4627 - acc: 0.9897 - val_loss: 0.5994 - val_acc: 0.5434\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.5999 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.6002 - val_acc: 0.5371\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.6030 - val_acc: 0.5197\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9916 - val_loss: 0.6010 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4626 - acc: 0.9897 - val_loss: 0.5990 - val_acc: 0.5324\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 56s - loss: 0.4625 - acc: 0.9922 - val_loss: 0.5989 - val_acc: 0.5450\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 60s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6051 - val_acc: 0.5229\n",
      "0.522907,0.550831,0.562319\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4626 - acc: 0.9900 - val_loss: 0.6005 - val_acc: 0.5498\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5990 - val_acc: 0.5450\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9919 - val_loss: 0.5992 - val_acc: 0.5450\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9897 - val_loss: 0.5992 - val_acc: 0.5403\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9905 - val_loss: 0.5981 - val_acc: 0.5450\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9897 - val_loss: 0.6002 - val_acc: 0.5308\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9888 - val_loss: 0.6000 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9905 - val_loss: 0.6021 - val_acc: 0.5197\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9888 - val_loss: 0.6018 - val_acc: 0.5213\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.5989 - val_acc: 0.5450\n",
      "0.545024,0.554720,0.521595\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4625 - acc: 0.9911 - val_loss: 0.5990 - val_acc: 0.5419\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.6010 - val_acc: 0.5308\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9911 - val_loss: 0.5992 - val_acc: 0.5466\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.5997 - val_acc: 0.5308\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9891 - val_loss: 0.6017 - val_acc: 0.5229\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.5992 - val_acc: 0.5403\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9894 - val_loss: 0.5994 - val_acc: 0.5355\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9891 - val_loss: 0.5988 - val_acc: 0.5466\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9911 - val_loss: 0.6039 - val_acc: 0.5182\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6000 - val_acc: 0.5308\n",
      "0.530806,0.552452,0.530806\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9914 - val_loss: 0.6014 - val_acc: 0.5308\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9925 - val_loss: 0.5986 - val_acc: 0.5450\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.5998 - val_acc: 0.5387\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9902 - val_loss: 0.5998 - val_acc: 0.5371\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9914 - val_loss: 0.5991 - val_acc: 0.5498\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.5994 - val_acc: 0.5355\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9916 - val_loss: 0.5996 - val_acc: 0.5466\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9905 - val_loss: 0.6002 - val_acc: 0.5387\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9911 - val_loss: 0.6037 - val_acc: 0.5182\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9908 - val_loss: 0.6000 - val_acc: 0.5371\n",
      "0.537125,0.552072,0.517298\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9897 - val_loss: 0.6003 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.5997 - val_acc: 0.5419\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 57s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6018 - val_acc: 0.5229\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4626 - acc: 0.9894 - val_loss: 0.5986 - val_acc: 0.5466\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9902 - val_loss: 0.6010 - val_acc: 0.5340\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9908 - val_loss: 0.5992 - val_acc: 0.5434\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9908 - val_loss: 0.6007 - val_acc: 0.5340\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9905 - val_loss: 0.6013 - val_acc: 0.5308\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4626 - acc: 0.9922 - val_loss: 0.6003 - val_acc: 0.5340\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9902 - val_loss: 0.6008 - val_acc: 0.5482\n",
      "0.548183,0.554124,0.501742\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.6001 - val_acc: 0.5276\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4623 - acc: 0.9914 - val_loss: 0.6009 - val_acc: 0.5324\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9900 - val_loss: 0.6009 - val_acc: 0.5308\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9897 - val_loss: 0.6002 - val_acc: 0.5434\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9897 - val_loss: 0.6010 - val_acc: 0.5276\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9916 - val_loss: 0.6005 - val_acc: 0.5276\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6003 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9911 - val_loss: 0.6002 - val_acc: 0.5498\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4624 - acc: 0.9908 - val_loss: 0.6012 - val_acc: 0.5308\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9900 - val_loss: 0.6005 - val_acc: 0.5292\n",
      "0.529226,0.553043,0.534375\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9902 - val_loss: 0.6033 - val_acc: 0.5182\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9908 - val_loss: 0.5997 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 58s - loss: 0.4624 - acc: 0.9894 - val_loss: 0.5995 - val_acc: 0.5403\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9897 - val_loss: 0.6002 - val_acc: 0.5466\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9902 - val_loss: 0.6025 - val_acc: 0.5197\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9894 - val_loss: 0.5992 - val_acc: 0.5419\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9914 - val_loss: 0.6005 - val_acc: 0.5450\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9902 - val_loss: 0.6011 - val_acc: 0.5371\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9891 - val_loss: 0.6006 - val_acc: 0.5371\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9908 - val_loss: 0.6012 - val_acc: 0.5276\n",
      "0.527646,0.552482,0.530612\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4624 - acc: 0.9911 - val_loss: 0.6060 - val_acc: 0.5182\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9905 - val_loss: 0.6051 - val_acc: 0.5150\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9902 - val_loss: 0.6015 - val_acc: 0.5340\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4624 - acc: 0.9894 - val_loss: 0.6022 - val_acc: 0.5450\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.5995 - val_acc: 0.5387\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9902 - val_loss: 0.6002 - val_acc: 0.5403\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9902 - val_loss: 0.6021 - val_acc: 0.5292\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4624 - acc: 0.9914 - val_loss: 0.6004 - val_acc: 0.5403\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4625 - acc: 0.9916 - val_loss: 0.6026 - val_acc: 0.5261\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9905 - val_loss: 0.6015 - val_acc: 0.5308\n",
      "0.530806,0.552533,0.532283\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9908 - val_loss: 0.6000 - val_acc: 0.5450\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9897 - val_loss: 0.5999 - val_acc: 0.5419\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 51s - loss: 0.4624 - acc: 0.9927 - val_loss: 0.6017 - val_acc: 0.5292\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4626 - acc: 0.9900 - val_loss: 0.6011 - val_acc: 0.5324\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9883 - val_loss: 0.6001 - val_acc: 0.5371\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9908 - val_loss: 0.5998 - val_acc: 0.5529\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9894 - val_loss: 0.6006 - val_acc: 0.5403\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9900 - val_loss: 0.6063 - val_acc: 0.5182\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9905 - val_loss: 0.6022 - val_acc: 0.5229\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9914 - val_loss: 0.6009 - val_acc: 0.5276\n",
      "0.527646,0.552943,0.527646\n",
      "Train on 3584 samples, validate on 633 samples\n",
      "Epoch 1/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4625 - acc: 0.9902 - val_loss: 0.6008 - val_acc: 0.5434\n",
      "Epoch 2/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4624 - acc: 0.9911 - val_loss: 0.6003 - val_acc: 0.5371\n",
      "Epoch 3/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9883 - val_loss: 0.6012 - val_acc: 0.5324\n",
      "Epoch 4/10\n",
      "3584/3584 [==============================] - 59s - loss: 0.4625 - acc: 0.9886 - val_loss: 0.6016 - val_acc: 0.5292\n",
      "Epoch 5/10\n",
      "3584/3584 [==============================] - 55s - loss: 0.4625 - acc: 0.9900 - val_loss: 0.6032 - val_acc: 0.5182\n",
      "Epoch 6/10\n",
      "3584/3584 [==============================] - 54s - loss: 0.4624 - acc: 0.9908 - val_loss: 0.6031 - val_acc: 0.5229\n",
      "Epoch 7/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9897 - val_loss: 0.6007 - val_acc: 0.5276\n",
      "Epoch 8/10\n",
      "3584/3584 [==============================] - 53s - loss: 0.4624 - acc: 0.9908 - val_loss: 0.6060 - val_acc: 0.5150\n",
      "Epoch 9/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4625 - acc: 0.9894 - val_loss: 0.6008 - val_acc: 0.5434\n",
      "Epoch 10/10\n",
      "3584/3584 [==============================] - 52s - loss: 0.4624 - acc: 0.9900 - val_loss: 0.6033 - val_acc: 0.5150\n",
      "0.515008,0.550821,0.545185\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "cell_type = 'GM12878'\n",
    "with open('./Gen_data/14_'+cell_type+'_newwork_result.txt','a') as f:\n",
    "    f.write( ','.join(  ['acc','auc','f1_score']))\n",
    "    for iter_index in range(40):\n",
    "        model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=10, validation_data=([x_valid, x_valid_ehr], y_valid))\n",
    "        pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "        y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "        acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "        auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "        f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "        f.write('{:2f},{:2f},{:2f}'.format(acc,auc,f1_score))\n",
    "        print '{:2f},{:2f},{:2f}'.format(acc,auc,f1_score)\n",
    "        if  iter_index%19==0 :\n",
    "            model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:03d}.h5'.format (iter_index*10+40))\n",
    "    model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:s}.h5'.format ('fin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=100, validation_data=([x_valid, x_valid_ehr], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.shape(x_train_ehr),np.shape(x_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.515007898894\n",
      "0.550820820821\n",
      "0.545185185185\n"
     ]
    }
   ],
   "source": [
    "pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "\n",
    "y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "print acc\n",
    "\n",
    "auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "print auc\n",
    "\n",
    "f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "print f1_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./Gen_data/14_Att-BLSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./Gen_data/14_w2v_Att-BLSTM_model_1.h5',by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26f9e02c90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26f9e02c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f26f8b20350>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFkCAYAAACuFXjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEhU/x+j2ah/R6jagAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26f8b20fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for index in range(len(y_valid)):\n",
    "    if y_valid[index,1]==1:\n",
    "        marker = 'o'\n",
    "        color = 'r'\n",
    "        label = '1'\n",
    "    else:\n",
    "        marker = 'x'\n",
    "        color = 'g'\n",
    "        label = '0'\n",
    "    s=20\n",
    "    plt.scatter(pro[index,0],pro[index,1],marker=marker,color=color,s = s,label=label)\n",
    "    \n",
    "a = np.array(range(10,80),dtype=float)/100\n",
    "plt.plot(a,a,color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-62-6ecb6ccfc9d4>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-62-6ecb6ccfc9d4>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [0:0.01:1]\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[0:0.01:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_valid[:,1], pro[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.057346698850805754,\n",
       " 1.0573466988508056,\n",
       " -0.057789279045439408,\n",
       " 1.0608662021223625)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAFdCAYAAAB1gNVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26f8b20890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fpr,tpr)\n",
    "plt.scatter(fpr,tpr,color ='g')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
