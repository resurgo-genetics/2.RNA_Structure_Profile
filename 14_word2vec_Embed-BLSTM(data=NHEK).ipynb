{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README    \n",
    "1. generate a Keras Layer :AttLayer  \n",
    "2. Use csv module to import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1)使用Dense层\n",
    "acc在.73\n",
    "roc在0.79\n",
    "说明数据集没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获得ｋｍｅｒ的函数\n",
    "embed_dict ={}\n",
    "def seq_to_embed_seq(seq):\n",
    "    kmer=6\n",
    "    embed_seq = []\n",
    "    if len (seq)< kmer:\n",
    "        return embed_seq\n",
    "    for i in range(kmer,len(seq)):\n",
    "        word  = seq[(i-kmer):i] \n",
    "        #if  word not in embed_dict.keys():\n",
    "        #    embed_dict[ word  ] =str( len(embed_dict.keys()) ) #给标号\n",
    "        #embed_seq.append(  str( embed_dict[ word  ]  )  )\n",
    "        embed_seq.append(  word )\n",
    "    return embed_seq\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_type='NHEK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bin', 'enhancer_chrom', 'enhancer_distance_to_promoter', 'enhancer_end', 'enhancer_name', 'enhancer_start', 'label', 'promoter_chrom', 'promoter_end', 'promoter_name', 'promoter_start', 'window_end', 'window_start', 'window_chrom', 'window_name', 'interactions_in_window', 'active_promoters_in_window']\n"
     ]
    }
   ],
   "source": [
    "datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/targetfinder/NHEK//output-epw/pairs.csv'\n",
    "#datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/pairs.csv'\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open(datafile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for  row in spamreader:\n",
    "        csvkeys = row\n",
    "        break\n",
    "print csvkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 1291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poscsv = {}\n",
    "negcsv = {}\n",
    "with open(datafile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for line in reader:\n",
    "        if line['bin']  not in poscsv.keys():\n",
    "            poscsv[line['bin']] =[]\n",
    "        if line ['bin']  not in negcsv.keys():\n",
    "            negcsv[line['bin']] =[]\n",
    "\n",
    "        record=[ line['enhancer_chrom']+' '+\n",
    "                line[ 'enhancer_start']+' '+\n",
    "                line['enhancer_end']+' '+\n",
    "                line['promoter_chrom']+' '+\n",
    "                line['promoter_start']+' '+\n",
    "                line['promoter_end']+' '+\n",
    "                line['enhancer_name']+'-'+line['promoter_name']\n",
    "                  ]\n",
    "        if line['label']==str(1):\n",
    "            poscsv[line['bin']].append(record)\n",
    "        else:\n",
    "            negcsv[line['bin']].append(record)\n",
    "\n",
    "    for key in poscsv.keys():\n",
    "        length = len(poscsv[key])\n",
    "        index = range(len(negcsv[key]))\n",
    "        np.random.shuffle(index)\n",
    "        negcsv[key] = np.array(negcsv[key])\n",
    "        negcsv[key] = negcsv[key][index[:length]]\n",
    "        #print len(negcsv[key])\n",
    "\n",
    "posdata =[]\n",
    "negdata = []\n",
    "for key in poscsv.keys():\n",
    "    posdata = np.append(posdata,poscsv[key])\n",
    "    negdata = np.append(negdata,negcsv[key])\n",
    "assert  len(posdata) == len(negdata)\n",
    "print  len(posdata),len(negdata)\n",
    "\n",
    "from pyfasta import Fasta\n",
    "genome = Fasta('/home/yinqijin/WorkSpace/DataHub/genome.fa')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos over\n",
      "1291\n",
      "neg over\n",
      "2582\n",
      "--------------------------------------------------\n",
      "index (2582,)\n",
      "Ehr-Seq (2582,)\n",
      "label (2582,)\n",
      "Ehr-ESeq (2582, 800)\n",
      "Pro-Seq (2582,)\n",
      "Pro-ESeq (2582, 800)\n",
      "pos data 1291\n",
      "neg data 1291\n"
     ]
    }
   ],
   "source": [
    "sentence_len = 806\n",
    "half_len = sentence_len/2\n",
    "\n",
    "PESeq=dict()\n",
    "PESeq['index']=[]\n",
    "PESeq['Pro-Seq']=[]\n",
    "PESeq['Pro-ESeq']=[]\n",
    "PESeq['Ehr-Seq']=[]\n",
    "PESeq['Ehr-ESeq']=[]\n",
    "PESeq['label'] =[]\n",
    "\n",
    "#正样本\n",
    "for index in range(len(posdata)):\n",
    "    line = posdata[index]\n",
    "    #print line\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    #print len(pro_seq)-len(seq_to_embed_seq(pro_seq))\n",
    "    #print seq_to_embed_seq(pro_seq)\n",
    "    \n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    #print seq_to_embed_seq(ehr_seq)\n",
    "    #break\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(1)\n",
    "print 'pos over'\n",
    "print len(PESeq['label'])  \n",
    "\n",
    "#负样本\n",
    "  \n",
    "for  index in range(len(negdata)):\n",
    "    #break\n",
    "    line = negdata[index]\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    #负样本比较多 可以严格限制 不可以 设置为1000 只剩168\n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "     \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(0)\n",
    "print 'neg over'\n",
    "\n",
    "print len(PESeq['label'])\n",
    "\n",
    "print '-'*50\n",
    "for item in PESeq.keys():\n",
    "    print item, np.shape(PESeq[item])\n",
    "print 'pos data' , sum(int( 1 ) if item ==1 else int(0) for item in PESeq['label'])\n",
    "print 'neg data',sum(int( 1) if item ==0 else int(0) for item in PESeq['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pro_Sentense = PESeq['Pro-ESeq']\n",
    "with open('./Gen_data/14_'+cell_type+'_Pro_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Pro_Sentense)[0]):\n",
    "        Sentense = Pro_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')\n",
    "        \n",
    "\n",
    "Ehr_Sentense = PESeq['Ehr-ESeq']\n",
    "with open('./Gen_data/14_'+cell_type+'_Ehr_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Ehr_Sentense)[0]):\n",
    "        Sentense = Ehr_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Pro_sentences = word2vec.LineSentence('./Gen_data/14_'+cell_type+'_Pro_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "pro_word_vectors = model.wv\n",
    "\n",
    "Ehr_sentences = word2vec.LineSentence('./Gen_data/14_'+cell_type+'_Ehr_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "ehr_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = pro_word_vectors.index2word[item]\n",
    "    pro_embedding_matrix[item]= pro_word_vectors[index]\n",
    "\n",
    "ehr_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = ehr_word_vectors.index2word[item]\n",
    "    ehr_embedding_matrix[item]= ehr_word_vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output = open('./Gen_data/14_'+cell_type+'_embedding_matrix.pkl','w')\n",
    "pkl.dump(pro_embedding_matrix,output)\n",
    "pkl.dump(ehr_embedding_matrix,output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file = open('./Gen_data/14_'+cell_type+'_embedding_matrix.pkl','r')\n",
    "pro_embedding_matrix = pkl.load(input_file)\n",
    "ehr_embedding_matrix = pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro_fin!\n",
      "ehr_fin!\n"
     ]
    }
   ],
   "source": [
    "pro_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_origin_sentense = PESeq['Pro-ESeq'][item]\n",
    "    pro_index_sentense = []\n",
    "    for word in  pro_origin_sentense:\n",
    "        \n",
    "        pro_index_sentense.append( pro_word_vectors.index2word.index( word))\n",
    "    pro_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'pro_fin!'\n",
    "ehr_input=[]\n",
    "func = lambda word: ehr_word_vectors.index2word.index(word)\n",
    "for item in range(len(PESeq['label'])):\n",
    "    ehr_origin_sentense = PESeq['Ehr-ESeq'][item]\n",
    "    ehr_index_sentense = []\n",
    "    for word in  ehr_origin_sentense:\n",
    "        ehr_index_sentense.append( ehr_word_vectors.index2word.index( word))\n",
    "    ehr_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'ehr_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)\n",
    "ehr_input = np.array(ehr_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2582, 800)\n",
      "(800,)\n",
      "(2582, 800)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(pro_input)\n",
    "print np.shape(pro_input[0])\n",
    "print np.shape(ehr_input)\n",
    "#print np,shape(ehr_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_org = PESeq['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "output = open('./Gen_data/14_'+cell_type+'_Network_input.pkl','w')\n",
    "pkl.dump(pro_input,output)\n",
    "pkl.dump(ehr_input,output)\n",
    "pkl.dump(PESeq['label'],output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "input_file = open('./Gen_data/14_'+cell_type+'_Network_input.pkl','r')\n",
    "pro_input = pkl.load(input_file)\n",
    "ehr_input = pkl.load(input_file)\n",
    "Y_org=pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = to_categorical(Y_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_point = int(0.85*len(Y_org))\n",
    "seq_index = range(len(Y_org))\n",
    "shuffle(seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train ,y_train =pro_input[seq_index[:SPLIT_point]], Y[seq_index[:SPLIT_point]]\n",
    "x_valid ,y_valid = pro_input[seq_index[SPLIT_point:]], Y[seq_index[SPLIT_point:]]\n",
    "\n",
    "x_train_ehr ,x_valid_ehr =ehr_input[seq_index[:SPLIT_point]], ehr_input[seq_index[SPLIT_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train[0]),y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,InputLayer\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten,Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp\n",
    "import numpy as np\n",
    "pro_input=np.zeros((800,4096),dtype = int )\n",
    "ehr_input=np.zeros((800,4096),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "MAX_SEQUENCE_LENGTH = 800  \n",
    "nb_words =4096   #字典的len(keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Net1: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(2,return_sequences=True)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "model.compile(loss='mse', optimizer='rmsprop',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=20, validation_data=([x_valid, x_valid_ehr], y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net2 : Att-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "# Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_13 (InputLayer)            (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)         (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "input_14 (InputLayer)            (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_14 (Embedding)         (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional)  (None, 800, 200)      240800      merge_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_7 (AttLayer)            (None, 200)           200         bidirectional_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 2)             402         attlayer_7[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,060,602\n",
      "Trainable params: 1,060,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "rmsprop = opt.rmsprop(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=rmsprop,metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def roc_auc_score(y_true, y_pred):\n",
    "    return metrics.roc_auc_score(y_true,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2194 samples, validate on 388 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Error allocating 41011200 bytes of device memory (out of memory).\nApply node that caused the error: GpuAllocEmpty(TensorConstant{801}, Shape_i{0}.0, TensorConstant{100})\nToposort index: 81\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), ()]\nInputs strides: [(), (), ()]\nInputs values: [array(801), array(128), array(100)]\nOutputs clients: [[GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty.0, GpuFromHost.0, Constant{1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c47897aca926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_ehr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid_ehr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mpro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_valid_ehr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mitem0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitem1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpro\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    884\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[1;32m    887\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/Theano/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[0;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# extra long error message in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/Theano/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Error allocating 41011200 bytes of device memory (out of memory).\nApply node that caused the error: GpuAllocEmpty(TensorConstant{801}, Shape_i{0}.0, TensorConstant{100})\nToposort index: 81\nInputs types: [TensorType(int64, scalar), TensorType(int64, scalar), TensorType(int64, scalar)]\nInputs shapes: [(), (), ()]\nInputs strides: [(), (), ()]\nInputs values: [array(801), array(128), array(100)]\nOutputs clients: [[GpuIncSubtensor{InplaceSet;:int64:}(GpuAllocEmpty.0, GpuFromHost.0, Constant{1})]]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "with open('./Gen_data/14_'+cell_type+'_newwork_result.txt','w') as f:\n",
    "    f.write( ','.join(  ['acc','auc','f1_score']))\n",
    "    for iter_index in range(30):\n",
    "        model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=10, validation_data=([x_valid, x_valid_ehr], y_valid))\n",
    "        pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "        y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "        acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "        auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "        f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "        f.write('{:2f},{:2f},{:2f}'.format(acc,auc,f1_score))\n",
    "        print '{:2f},{:2f},{:2f}'.format(acc,auc,f1_score)\n",
    "        if  iter_index%9==0 :\n",
    "            model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:03d}.h5'.format (iter_index*10))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=300, validation_data=([x_valid, x_valid_ehr], y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (3585,) (3585, 2)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train_ehr),np.shape(x_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952606635071\n",
      "0.997827172827\n",
      "0.955621301775\n"
     ]
    }
   ],
   "source": [
    "pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "\n",
    "y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "print acc\n",
    "\n",
    "auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "print auc\n",
    "\n",
    "f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "print f1_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./Gen_data/14_Att-BLSTM_'+cell_type+'_model_fin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./Gen_data/14_w2v_Att-BLSTM_'+cell_type+'_model_fin.h5',by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for index in range(len(y_valid)):\n",
    "    if y_valid[index,1]==1:\n",
    "        marker = 'o'\n",
    "        color = 'r'\n",
    "        label = '1'\n",
    "    else:\n",
    "        marker = 'x'\n",
    "        color = 'g'\n",
    "        label = '0'\n",
    "    s=20\n",
    "    plt.scatter(pro[index,0],pro[index,1],marker=marker,color=color,s = s,label=label)\n",
    "    \n",
    "a = np.array(range(10,80),dtype=float)/100\n",
    "plt.plot(a,a,color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_valid[:,1], pro[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr,tpr)\n",
    "plt.scatter(fpr,tpr,color ='g')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
