{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README    \n",
    "1. generate a Keras Layer :AttLayer  \n",
    "2. Use csv module to import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1)使用Dense层\n",
    "acc在.73\n",
    "roc在0.79\n",
    "说明数据集没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获得ｋｍｅｒ的函数\n",
    "embed_dict ={}\n",
    "def seq_to_embed_seq(seq):\n",
    "    kmer=6\n",
    "    embed_seq = []\n",
    "    if len (seq)< kmer:\n",
    "        return embed_seq\n",
    "    for i in range(kmer,len(seq)):\n",
    "        word  = seq[(i-kmer):i] \n",
    "        #if  word not in embed_dict.keys():\n",
    "        #    embed_dict[ word  ] =str( len(embed_dict.keys()) ) #给标号\n",
    "        #embed_seq.append(  str( embed_dict[ word  ]  )  )\n",
    "        embed_seq.append(  word )\n",
    "    return embed_seq\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cell_type='NHEK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bin', 'enhancer_chrom', 'enhancer_distance_to_promoter', 'enhancer_end', 'enhancer_name', 'enhancer_start', 'label', 'promoter_chrom', 'promoter_end', 'promoter_name', 'promoter_start', 'window_end', 'window_start', 'window_chrom', 'window_name', 'interactions_in_window', 'active_promoters_in_window']\n"
     ]
    }
   ],
   "source": [
    "datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/targetfinder/NHEK//output-epw/pairs.csv'\n",
    "#datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/pairs.csv'\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "with open(datafile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for  row in spamreader:\n",
    "        csvkeys = row\n",
    "        break\n",
    "print csvkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1291 1291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poscsv = {}\n",
    "negcsv = {}\n",
    "with open(datafile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for line in reader:\n",
    "        if line['bin']  not in poscsv.keys():\n",
    "            poscsv[line['bin']] =[]\n",
    "        if line ['bin']  not in negcsv.keys():\n",
    "            negcsv[line['bin']] =[]\n",
    "\n",
    "        record=[ line['enhancer_chrom']+' '+\n",
    "                line[ 'enhancer_start']+' '+\n",
    "                line['enhancer_end']+' '+\n",
    "                line['promoter_chrom']+' '+\n",
    "                line['promoter_start']+' '+\n",
    "                line['promoter_end']+' '+\n",
    "                line['enhancer_name']+'-'+line['promoter_name']\n",
    "                  ]\n",
    "        if line['label']==str(1):\n",
    "            poscsv[line['bin']].append(record)\n",
    "        else:\n",
    "            negcsv[line['bin']].append(record)\n",
    "\n",
    "    for key in poscsv.keys():\n",
    "        length = len(poscsv[key])\n",
    "        index = range(len(negcsv[key]))\n",
    "        np.random.shuffle(index)\n",
    "        negcsv[key] = np.array(negcsv[key])\n",
    "        negcsv[key] = negcsv[key][index[:length]]\n",
    "        #print len(negcsv[key])\n",
    "\n",
    "posdata =[]\n",
    "negdata = []\n",
    "for key in poscsv.keys():\n",
    "    posdata = np.append(posdata,poscsv[key])\n",
    "    negdata = np.append(negdata,negcsv[key])\n",
    "assert  len(posdata) == len(negdata)\n",
    "print  len(posdata),len(negdata)\n",
    "\n",
    "from pyfasta import Fasta\n",
    "genome = Fasta('/home/yinqijin/WorkSpace/DataHub/genome.fa')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos over\n",
      "1291\n",
      "neg over\n",
      "2582\n",
      "--------------------------------------------------\n",
      "index (2582,)\n",
      "Ehr-Seq (2582,)\n",
      "label (2582,)\n",
      "Ehr-ESeq (2582, 800)\n",
      "Pro-Seq (2582,)\n",
      "Pro-ESeq (2582, 800)\n",
      "pos data 1291\n",
      "neg data 1291\n"
     ]
    }
   ],
   "source": [
    "sentence_len = 806\n",
    "half_len = sentence_len/2\n",
    "\n",
    "PESeq=dict()\n",
    "PESeq['index']=[]\n",
    "PESeq['Pro-Seq']=[]\n",
    "PESeq['Pro-ESeq']=[]\n",
    "PESeq['Ehr-Seq']=[]\n",
    "PESeq['Ehr-ESeq']=[]\n",
    "PESeq['label'] =[]\n",
    "\n",
    "#正样本\n",
    "for index in range(len(posdata)):\n",
    "    line = posdata[index]\n",
    "    #print line\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    #print len(pro_seq)-len(seq_to_embed_seq(pro_seq))\n",
    "    #print seq_to_embed_seq(pro_seq)\n",
    "    \n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    #print seq_to_embed_seq(ehr_seq)\n",
    "    #break\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(1)\n",
    "print 'pos over'\n",
    "print len(PESeq['label'])  \n",
    "\n",
    "#负样本\n",
    "  \n",
    "for  index in range(len(negdata)):\n",
    "    #break\n",
    "    line = negdata[index]\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    #负样本比较多 可以严格限制 不可以 设置为1000 只剩168\n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "     \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(0)\n",
    "print 'neg over'\n",
    "\n",
    "print len(PESeq['label'])\n",
    "\n",
    "print '-'*50\n",
    "for item in PESeq.keys():\n",
    "    print item, np.shape(PESeq[item])\n",
    "print 'pos data' , sum(int( 1 ) if item ==1 else int(0) for item in PESeq['label'])\n",
    "print 'neg data',sum(int( 1) if item ==0 else int(0) for item in PESeq['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pro_Sentense = PESeq['Pro-ESeq']\n",
    "with open('./Gen_data/14_'+cell_type+'_Pro_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Pro_Sentense)[0]):\n",
    "        Sentense = Pro_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')\n",
    "        \n",
    "\n",
    "Ehr_Sentense = PESeq['Ehr-ESeq']\n",
    "with open('./Gen_data/14_'+cell_type+'_Ehr_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Ehr_Sentense)[0]):\n",
    "        Sentense = Ehr_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n",
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Pro_sentences = word2vec.LineSentence('./Gen_data/14_'+cell_type+'_Pro_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "pro_word_vectors = model.wv\n",
    "\n",
    "Ehr_sentences = word2vec.LineSentence('./Gen_data/14_'+cell_type+'_Ehr_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "ehr_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = pro_word_vectors.index2word[item]\n",
    "    pro_embedding_matrix[item]= pro_word_vectors[index]\n",
    "\n",
    "ehr_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = ehr_word_vectors.index2word[item]\n",
    "    ehr_embedding_matrix[item]= ehr_word_vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "output = open('./Gen_data/14_'+cell_type+'_embedding_matrix.pkl','w')\n",
    "pkl.dump(pro_embedding_matrix,output)\n",
    "pkl.dump(ehr_embedding_matrix,output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell_type = 'NHEK'\n",
    "input_file = open('./Gen_data/14_'+cell_type+'_embedding_matrix.pkl','r')\n",
    "pro_embedding_matrix = pkl.load(input_file)\n",
    "ehr_embedding_matrix = pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro_fin!\n",
      "ehr_fin!\n"
     ]
    }
   ],
   "source": [
    "pro_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_origin_sentense = PESeq['Pro-ESeq'][item]\n",
    "    pro_index_sentense = []\n",
    "    for word in  pro_origin_sentense:\n",
    "        \n",
    "        pro_index_sentense.append( pro_word_vectors.index2word.index( word))\n",
    "    pro_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'pro_fin!'\n",
    "ehr_input=[]\n",
    "func = lambda word: ehr_word_vectors.index2word.index(word)\n",
    "for item in range(len(PESeq['label'])):\n",
    "    ehr_origin_sentense = PESeq['Ehr-ESeq'][item]\n",
    "    ehr_index_sentense = []\n",
    "    for word in  ehr_origin_sentense:\n",
    "        ehr_index_sentense.append( ehr_word_vectors.index2word.index( word))\n",
    "    ehr_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'ehr_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)\n",
    "ehr_input = np.array(ehr_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2582, 800)\n",
      "(800,)\n",
      "(2582, 800)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(pro_input)\n",
    "print np.shape(pro_input[0])\n",
    "print np.shape(ehr_input)\n",
    "#print np,shape(ehr_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_org = PESeq['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import pickle as pkl\n",
    "output = open('./Gen_data/14_'+cell_type+'_Network_input.pkl','w')\n",
    "pkl.dump(pro_input,output)\n",
    "pkl.dump(ehr_input,output)\n",
    "pkl.dump(PESeq['label'],output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "input_file = open('./Gen_data/14_'+cell_type+'_Network_input.pkl','r')\n",
    "pro_input = pkl.load(input_file)\n",
    "ehr_input = pkl.load(input_file)\n",
    "Y_org=pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = to_categorical(Y_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_point = int(0.85*len(Y_org))\n",
    "seq_index = range(len(Y_org))\n",
    "shuffle(seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train ,y_train =pro_input[seq_index[:SPLIT_point]], Y[seq_index[:SPLIT_point]]\n",
    "x_valid ,y_valid = pro_input[seq_index[SPLIT_point:]], Y[seq_index[SPLIT_point:]]\n",
    "\n",
    "x_train_ehr ,x_valid_ehr =ehr_input[seq_index[:SPLIT_point]], ehr_input[seq_index[SPLIT_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train[0]),y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,InputLayer\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten,Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp\n",
    "import numpy as np\n",
    "pro_input=np.zeros((800,4096),dtype = int )\n",
    "ehr_input=np.zeros((800,4096),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "MAX_SEQUENCE_LENGTH = 800  \n",
    "nb_words =4096   #字典的len(keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Net1: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(2,return_sequences=True)))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(16))\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "model.compile(loss='mse', optimizer='rmsprop',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=20, validation_data=([x_valid, x_valid_ehr], y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Net2 : Att-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "# Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 800, 200)      240800      merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 200)           200         bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             402         attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,060,602\n",
      "Trainable params: 1,060,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "rmsprop = opt.rmsprop(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=rmsprop,metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dee47ef86237>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Gen_data/14_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcell_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_newwork_result.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'auc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miter_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train_ehr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_valid_ehr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/.local/lib/python2.7/site-packages/sklearn/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoverage_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/.local/lib/python2.7/site-packages/sklearn/metrics/ranking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbincount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_equal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparsefuncs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/.local/lib/python2.7/site-packages/sklearn/utils/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sp_rankdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbincount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   4484\u001b[0m                             lambda_=\"pearson\")\n\u001b[1;32m   4485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4486\u001b[0;31m \u001b[0mKs_2sampResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ks_2sampResult'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'statistic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pvalue'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/collections.pyc\u001b[0m in \u001b[0;36mnamedtuple\u001b[0;34m(typename, field_names, verbose, rename)\u001b[0m\n\u001b[1;32m    384\u001b[0m                      OrderedDict=OrderedDict, _property=property, _tuple=tuple)\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mexec\u001b[0m \u001b[0mclass_definition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mSyntaxError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m':\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclass_definition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "with open('./Gen_data/14_'+cell_type+'_newwork_result.txt','w') as f:\n",
    "    f.write( ','.join(  ['acc','auc','f1_score']))\n",
    "    for iter_index in range(60):\n",
    "        model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=10, validation_data=([x_valid, x_valid_ehr], y_valid))\n",
    "        pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "        y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "        acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "        auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "        f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "        f.write('{:2f},{:2f},{:2f}'.format(acc,auc,f1_score))\n",
    "        print '{:2f},{:2f},{:2f}'.format(acc,auc,f1_score)\n",
    "        if  iter_index%9==0 :\n",
    "            model.save_weights('./Gen_data/14_Att-BLSTM_model_iter{:03d}.h5'.format (iter_index*10))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=300, validation_data=([x_valid, x_valid_ehr], y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (3585,) (3585, 2)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train_ehr),np.shape(x_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.952606635071\n",
      "0.997827172827\n",
      "0.955621301775\n"
     ]
    }
   ],
   "source": [
    "pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "\n",
    "y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "print acc\n",
    "\n",
    "auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "print auc\n",
    "\n",
    "f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "print f1_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('./Gen_data/14_Att-BLSTM_'+cell_type+'_model_fin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./Gen_data/14_w2v_Att-BLSTM_'+cell_type+'_model_fin.h5',by_name = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for index in range(len(y_valid)):\n",
    "    if y_valid[index,1]==1:\n",
    "        marker = 'o'\n",
    "        color = 'r'\n",
    "        label = '1'\n",
    "    else:\n",
    "        marker = 'x'\n",
    "        color = 'g'\n",
    "        label = '0'\n",
    "    s=20\n",
    "    plt.scatter(pro[index,0],pro[index,1],marker=marker,color=color,s = s,label=label)\n",
    "    \n",
    "a = np.array(range(10,80),dtype=float)/100\n",
    "plt.plot(a,a,color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_valid[:,1], pro[:,1], pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(fpr,tpr)\n",
    "plt.scatter(fpr,tpr,color ='g')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
