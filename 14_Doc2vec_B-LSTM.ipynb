{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_root='/home/yinqijin/tempdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 1291 60\n"
     ]
    }
   ],
   "source": [
    "random.seed(666)\n",
    "\n",
    "fin1  = open(base_root+'/training.csv','r')\n",
    "\n",
    "pbin   = [ [],[],[],[],[] ]\n",
    "nbin   = [ [],[],[],[],[] ]\n",
    "bins   = []\n",
    "neg    = []\n",
    "pos    = []\n",
    "for line in fin1:\n",
    "\tif line[0] == 'b':\n",
    "\t\tcontinue\n",
    "\telse:\n",
    "\t\tdata  = line.split('\"')\n",
    "\t\tif data[1] not in bins:\n",
    "\t\t\tbins.append(data[1])\n",
    "\t\tindex = bins.index(data[1])\n",
    "\t\tdata  = line.split(',')\n",
    "\t\tif data[7] == '1':\n",
    "\t\t\tpbin[index].append(line)\n",
    "\t\t\tpos.append(line)\n",
    "\t\telse:\n",
    "\t\t\tnbin[index].append(line)\t\n",
    "\n",
    "\n",
    "for i in range(len(bins)):\n",
    "\tnbinlen = len(nbin[i])\n",
    "\ttemp    = random.sample(nbin[i],nbinlen/20)\n",
    "\tneg.extend(temp)\n",
    "\n",
    "nnum   = len(neg)\n",
    "pnum   = len(pos)\n",
    "fnum   = len(neg[0].strip().split(',')) - 18\n",
    "\n",
    "print nnum,pnum,fnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin2   = open(base_root+'enhancers.bed','r')\n",
    "fin3   = open(base_root+'promoters.bed','r')\n",
    "enhancers = []\n",
    "promoters = []\n",
    "\n",
    "arrays1 = numpy.zeros((nnum+pnum,100,2))\n",
    "labels1 = numpy.zeros(nnum+pnum)\n",
    "\n",
    "for line in fin2:\n",
    "\tdata = line.strip().split()\n",
    "\tenhancers.append(data[3])\n",
    "\n",
    "for line in fin3:\n",
    "\tdata = line.strip().split()\n",
    "\tpromoters.append(data[3])\n",
    "\n",
    "enhancer_model = Doc2Vec.load(base_root+'enhancers_6_1_100.d2v')\n",
    "promoter_model = Doc2Vec.load(base_root+'promoters_6_1_100.d2v')\n",
    "\n",
    "\n",
    "for i in range(len(pos)):\n",
    "        data = pos[i].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))\n",
    "\tarrays1[i] = numpy.append(enhancer_vec,promoter_vec,axis=1)\n",
    "\tlabels1[i] = 1\n",
    "\n",
    "for j in range(len(neg)):\n",
    "        data = neg[j].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))  #.reshape((1,100))\n",
    "\tarrays1[i+j] = numpy.append(enhancer_vec,promoter_vec,axis=1)  #umn_stack\n",
    "\tlabels1[i+j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2571, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "print numpy.shape(arrays1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = range(len(labels1))\n",
    "random.shuffle(index)\n",
    "SPLIT_RATIO = 0.85\n",
    "split_point =int(SPLIT_RATIO*len(labels1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2185\n"
     ]
    }
   ],
   "source": [
    "print split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrays1 = numpy.array(arrays1)\n",
    "labels1 = numpy.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train , y_train=  arrays1[index[:split_point]] ,labels1[ index [:split_point]]\n",
    "x_valid ,y_valid= arrays1[index[split_point:]] , labels1[index[split_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedata =open('./Gen_data/14_data.pkl','w')\n",
    "pkl.dump(x_train,savedata)\n",
    "pkl.dump(y_train,savedata)\n",
    "pkl.dump(x_valid,savedata)\n",
    "pkl.dump(y_valid,savedata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "insecure string pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3445f5eea7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaddata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaddata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaddata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaddata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload_string\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"insecure string pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                 \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: insecure string pickle"
     ]
    }
   ],
   "source": [
    "loaddata = open('./Gen_data/14_data.pkl','r')\n",
    "x_train=pkl.load(loaddata)\n",
    "y_train=pkl.load(loaddata)\n",
    "x_valid=pkl.load(loaddata)\n",
    "y_valid=pkl.load(loaddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2185, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.shape(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "from collections  import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')\n",
    "\n",
    "l_lstm =Bidirectional(LSTM(20,return_sequences=True))(kmer_input)\n",
    "\n",
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "#l_flatten = Flatten()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_att)\n",
    "#l_dense = Dense(256)(l_att_drop)\n",
    "l_dense_1 = Dense(16)(l_att_drop)\n",
    "\n",
    "\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_dense_1)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 100, 2)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 100, 40)       3680        input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)              (None, 100, 40)       0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_2 (AttLayer)            (None, 40)            40          dropout_3[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 40)            0           attlayer_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 16)            656         dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             34          dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 4,410\n",
      "Trainable params: 4,410\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '135373' (I am process '7393')\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '11960' (I am process '7393')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/yinqijin/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.2.1511-Core-x86_64-2.7.12-64/lock_dir\n",
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '11960' (I am process '7393')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/yinqijin/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.2.1511-Core-x86_64-2.7.12-64/lock_dir\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2185 samples, validate on 386 samples\n",
      "Epoch 1/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3747 - acc: 0.4975 - val_loss: 0.3739 - val_acc: 0.4404\n",
      "Epoch 2/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3745 - acc: 0.5112 - val_loss: 0.3730 - val_acc: 0.4378\n",
      "Epoch 3/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3742 - acc: 0.5176 - val_loss: 0.3720 - val_acc: 0.4585\n",
      "Epoch 4/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3744 - acc: 0.5039 - val_loss: 0.3734 - val_acc: 0.4534\n",
      "Epoch 5/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3742 - acc: 0.5272 - val_loss: 0.3726 - val_acc: 0.4637\n",
      "Epoch 6/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3740 - acc: 0.5204 - val_loss: 0.3715 - val_acc: 0.5233\n",
      "Epoch 7/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3737 - acc: 0.5327 - val_loss: 0.3702 - val_acc: 0.5544\n",
      "Epoch 8/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3743 - acc: 0.5140 - val_loss: 0.3711 - val_acc: 0.5440\n",
      "Epoch 9/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3740 - acc: 0.5217 - val_loss: 0.3735 - val_acc: 0.4430\n",
      "Epoch 10/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3738 - acc: 0.5213 - val_loss: 0.3790 - val_acc: 0.4352\n",
      "Epoch 11/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3738 - acc: 0.5332 - val_loss: 0.3746 - val_acc: 0.4404\n",
      "Epoch 12/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3737 - acc: 0.5245 - val_loss: 0.3756 - val_acc: 0.4508\n",
      "Epoch 13/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3739 - acc: 0.5373 - val_loss: 0.3737 - val_acc: 0.4404\n",
      "Epoch 14/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5368 - val_loss: 0.3749 - val_acc: 0.4430\n",
      "Epoch 15/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5391 - val_loss: 0.3726 - val_acc: 0.4896\n",
      "Epoch 16/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3732 - acc: 0.5529 - val_loss: 0.3729 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3744 - acc: 0.5286 - val_loss: 0.3727 - val_acc: 0.5104\n",
      "Epoch 18/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5254 - val_loss: 0.3797 - val_acc: 0.4326\n",
      "Epoch 19/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5318 - val_loss: 0.3737 - val_acc: 0.5026\n",
      "Epoch 20/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3738 - acc: 0.5300 - val_loss: 0.3720 - val_acc: 0.5181\n",
      "Epoch 21/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3742 - acc: 0.5373 - val_loss: 0.3733 - val_acc: 0.4922\n",
      "Epoch 22/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3734 - acc: 0.5300 - val_loss: 0.3724 - val_acc: 0.4922\n",
      "Epoch 23/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3741 - acc: 0.5341 - val_loss: 0.3731 - val_acc: 0.5026\n",
      "Epoch 24/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3736 - acc: 0.5368 - val_loss: 0.3730 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5291 - val_loss: 0.3837 - val_acc: 0.4378\n",
      "Epoch 26/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3734 - acc: 0.5355 - val_loss: 0.3818 - val_acc: 0.4404\n",
      "Epoch 27/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5277 - val_loss: 0.3751 - val_acc: 0.5078\n",
      "Epoch 28/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3737 - acc: 0.5295 - val_loss: 0.3742 - val_acc: 0.4767\n",
      "Epoch 29/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3733 - acc: 0.5254 - val_loss: 0.3770 - val_acc: 0.4404\n",
      "Epoch 30/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3735 - acc: 0.5428 - val_loss: 0.3721 - val_acc: 0.5078\n",
      "Epoch 31/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3730 - acc: 0.5455 - val_loss: 0.3748 - val_acc: 0.4663\n",
      "Epoch 32/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3727 - acc: 0.5387 - val_loss: 0.3743 - val_acc: 0.5622\n",
      "Epoch 33/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3737 - acc: 0.5295 - val_loss: 0.3736 - val_acc: 0.5052\n",
      "Epoch 34/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3732 - acc: 0.5378 - val_loss: 0.3731 - val_acc: 0.4974\n",
      "Epoch 35/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3753 - acc: 0.5364 - val_loss: 0.3748 - val_acc: 0.4767\n",
      "Epoch 36/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3732 - acc: 0.5355 - val_loss: 0.3718 - val_acc: 0.5052\n",
      "Epoch 37/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3732 - acc: 0.5387 - val_loss: 0.3741 - val_acc: 0.4845\n",
      "Epoch 38/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3733 - acc: 0.5446 - val_loss: 0.3734 - val_acc: 0.4870\n",
      "Epoch 39/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3736 - acc: 0.5478 - val_loss: 0.3744 - val_acc: 0.4741\n",
      "Epoch 40/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3734 - acc: 0.5368 - val_loss: 0.3730 - val_acc: 0.4948\n",
      "Epoch 41/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3729 - acc: 0.5487 - val_loss: 0.3710 - val_acc: 0.5259\n",
      "Epoch 42/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3737 - acc: 0.5396 - val_loss: 0.3707 - val_acc: 0.5259\n",
      "Epoch 43/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3725 - acc: 0.5437 - val_loss: 0.3718 - val_acc: 0.5155\n",
      "Epoch 44/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3723 - acc: 0.5474 - val_loss: 0.3698 - val_acc: 0.5363\n",
      "Epoch 45/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3726 - acc: 0.5423 - val_loss: 0.3701 - val_acc: 0.5181\n",
      "Epoch 46/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3726 - acc: 0.5478 - val_loss: 0.3747 - val_acc: 0.4896\n",
      "Epoch 47/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3723 - acc: 0.5588 - val_loss: 0.3696 - val_acc: 0.5285\n",
      "Epoch 48/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3723 - acc: 0.5542 - val_loss: 0.3782 - val_acc: 0.4508\n",
      "Epoch 49/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3729 - acc: 0.5483 - val_loss: 0.3695 - val_acc: 0.5337\n",
      "Epoch 50/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3717 - acc: 0.5574 - val_loss: 0.3734 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3721 - acc: 0.5593 - val_loss: 0.3691 - val_acc: 0.5285\n",
      "Epoch 52/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3722 - acc: 0.5478 - val_loss: 0.3743 - val_acc: 0.4922\n",
      "Epoch 53/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3702 - acc: 0.5721 - val_loss: 0.3683 - val_acc: 0.5622\n",
      "Epoch 54/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3717 - acc: 0.5510 - val_loss: 0.3693 - val_acc: 0.5233\n",
      "Epoch 55/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3702 - acc: 0.5561 - val_loss: 0.3729 - val_acc: 0.5078\n",
      "Epoch 56/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3711 - acc: 0.5561 - val_loss: 0.3744 - val_acc: 0.5000\n",
      "Epoch 57/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3714 - acc: 0.5602 - val_loss: 0.3702 - val_acc: 0.5363\n",
      "Epoch 58/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3716 - acc: 0.5478 - val_loss: 0.3725 - val_acc: 0.5104\n",
      "Epoch 59/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3714 - acc: 0.5446 - val_loss: 0.3693 - val_acc: 0.5570\n",
      "Epoch 60/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3719 - acc: 0.5666 - val_loss: 0.3724 - val_acc: 0.5207\n",
      "Epoch 61/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3706 - acc: 0.5698 - val_loss: 0.3717 - val_acc: 0.5207\n",
      "Epoch 62/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3711 - acc: 0.5634 - val_loss: 0.3688 - val_acc: 0.5492\n",
      "Epoch 63/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3714 - acc: 0.5570 - val_loss: 0.3706 - val_acc: 0.5285\n",
      "Epoch 64/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3709 - acc: 0.5606 - val_loss: 0.3796 - val_acc: 0.4611\n",
      "Epoch 65/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3745 - acc: 0.5410 - val_loss: 0.3698 - val_acc: 0.5207\n",
      "Epoch 66/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3723 - acc: 0.5620 - val_loss: 0.3714 - val_acc: 0.5233\n",
      "Epoch 67/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3725 - acc: 0.5455 - val_loss: 0.3726 - val_acc: 0.4845\n",
      "Epoch 68/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3712 - acc: 0.5579 - val_loss: 0.3689 - val_acc: 0.5259\n",
      "Epoch 69/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3710 - acc: 0.5638 - val_loss: 0.3698 - val_acc: 0.5285\n",
      "Epoch 70/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3712 - acc: 0.5551 - val_loss: 0.3688 - val_acc: 0.5389\n",
      "Epoch 71/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3699 - acc: 0.5584 - val_loss: 0.3696 - val_acc: 0.5285\n",
      "Epoch 72/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3710 - acc: 0.5648 - val_loss: 0.3699 - val_acc: 0.5311\n",
      "Epoch 73/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3728 - acc: 0.5625 - val_loss: 0.3830 - val_acc: 0.5181\n",
      "Epoch 74/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3716 - acc: 0.5606 - val_loss: 0.3733 - val_acc: 0.5078\n",
      "Epoch 75/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3712 - acc: 0.5666 - val_loss: 0.3721 - val_acc: 0.5000\n",
      "Epoch 76/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3708 - acc: 0.5721 - val_loss: 0.3757 - val_acc: 0.5259\n",
      "Epoch 77/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3711 - acc: 0.5675 - val_loss: 0.3711 - val_acc: 0.5181\n",
      "Epoch 78/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3712 - acc: 0.5584 - val_loss: 0.3728 - val_acc: 0.5000\n",
      "Epoch 79/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3707 - acc: 0.5693 - val_loss: 0.3660 - val_acc: 0.5570\n",
      "Epoch 80/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3715 - acc: 0.5721 - val_loss: 0.3650 - val_acc: 0.5492\n",
      "Epoch 81/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3712 - acc: 0.5629 - val_loss: 0.3667 - val_acc: 0.5466\n",
      "Epoch 82/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3710 - acc: 0.5684 - val_loss: 0.3737 - val_acc: 0.5259\n",
      "Epoch 83/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3701 - acc: 0.5780 - val_loss: 0.3684 - val_acc: 0.5492\n",
      "Epoch 84/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3704 - acc: 0.5712 - val_loss: 0.3692 - val_acc: 0.5155\n",
      "Epoch 85/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3724 - acc: 0.5721 - val_loss: 0.3728 - val_acc: 0.5440\n",
      "Epoch 86/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3739 - acc: 0.5213 - val_loss: 0.3672 - val_acc: 0.5466\n",
      "Epoch 87/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3759 - acc: 0.5364 - val_loss: 0.3683 - val_acc: 0.5777\n",
      "Epoch 88/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3732 - acc: 0.5391 - val_loss: 0.3690 - val_acc: 0.5518\n",
      "Epoch 89/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3729 - acc: 0.5359 - val_loss: 0.3714 - val_acc: 0.5207\n",
      "Epoch 90/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3727 - acc: 0.5382 - val_loss: 0.3745 - val_acc: 0.4793\n",
      "Epoch 91/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3727 - acc: 0.5378 - val_loss: 0.3705 - val_acc: 0.5544\n",
      "Epoch 92/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3720 - acc: 0.5465 - val_loss: 0.3682 - val_acc: 0.5466\n",
      "Epoch 93/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3716 - acc: 0.5455 - val_loss: 0.3683 - val_acc: 0.5415\n",
      "Epoch 94/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3721 - acc: 0.5419 - val_loss: 0.3699 - val_acc: 0.5544\n",
      "Epoch 95/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3717 - acc: 0.5547 - val_loss: 0.3681 - val_acc: 0.5440\n",
      "Epoch 96/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3701 - acc: 0.5735 - val_loss: 0.3726 - val_acc: 0.5181\n",
      "Epoch 97/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3700 - acc: 0.5657 - val_loss: 0.3677 - val_acc: 0.5648\n",
      "Epoch 98/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3733 - acc: 0.5451 - val_loss: 0.3696 - val_acc: 0.5544\n",
      "Epoch 99/100\n",
      "2185/2185 [==============================] - 8s - loss: 0.3714 - acc: 0.5400 - val_loss: 0.3673 - val_acc: 0.5648\n",
      "Epoch 100/100\n",
      "2185/2185 [==============================] - 7s - loss: 0.3722 - acc: 0.5606 - val_loss: 0.3692 - val_acc: 0.5337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb3ee4ee10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2185 samples, validate on 386 samples\n",
      "Epoch 1/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3650 - acc: 0.6023 - val_loss: 0.3659 - val_acc: 0.5881\n",
      "Epoch 2/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3612 - acc: 0.6252 - val_loss: 0.3680 - val_acc: 0.5570\n",
      "Epoch 3/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3637 - acc: 0.5968 - val_loss: 0.3654 - val_acc: 0.5829\n",
      "Epoch 4/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3646 - acc: 0.5927 - val_loss: 0.3662 - val_acc: 0.5596\n",
      "Epoch 5/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3647 - acc: 0.6005 - val_loss: 0.3659 - val_acc: 0.5803\n",
      "Epoch 6/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3632 - acc: 0.6078 - val_loss: 0.3657 - val_acc: 0.5725\n",
      "Epoch 7/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3644 - acc: 0.5931 - val_loss: 0.3657 - val_acc: 0.5881\n",
      "Epoch 8/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3631 - acc: 0.6110 - val_loss: 0.3654 - val_acc: 0.5881\n",
      "Epoch 9/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3621 - acc: 0.6082 - val_loss: 0.3713 - val_acc: 0.5440\n",
      "Epoch 10/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3649 - acc: 0.5986 - val_loss: 0.3655 - val_acc: 0.5803\n",
      "Epoch 11/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3628 - acc: 0.6073 - val_loss: 0.3673 - val_acc: 0.5725\n",
      "Epoch 12/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3638 - acc: 0.6078 - val_loss: 0.3664 - val_acc: 0.5622\n",
      "Epoch 13/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3645 - acc: 0.5954 - val_loss: 0.3654 - val_acc: 0.5829\n",
      "Epoch 14/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3626 - acc: 0.6119 - val_loss: 0.3667 - val_acc: 0.5699\n",
      "Epoch 15/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3628 - acc: 0.6105 - val_loss: 0.3658 - val_acc: 0.5855\n",
      "Epoch 16/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3624 - acc: 0.6238 - val_loss: 0.3658 - val_acc: 0.5855\n",
      "Epoch 17/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3643 - acc: 0.5982 - val_loss: 0.3654 - val_acc: 0.5674\n",
      "Epoch 18/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3630 - acc: 0.5995 - val_loss: 0.3652 - val_acc: 0.5803\n",
      "Epoch 19/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3641 - acc: 0.5908 - val_loss: 0.3658 - val_acc: 0.5777\n",
      "Epoch 20/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3608 - acc: 0.6137 - val_loss: 0.3656 - val_acc: 0.5959\n",
      "Epoch 21/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3628 - acc: 0.6046 - val_loss: 0.3652 - val_acc: 0.5803\n",
      "Epoch 22/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3631 - acc: 0.6055 - val_loss: 0.3651 - val_acc: 0.5984\n",
      "Epoch 23/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3635 - acc: 0.6092 - val_loss: 0.3659 - val_acc: 0.5648\n",
      "Epoch 24/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3635 - acc: 0.6073 - val_loss: 0.3658 - val_acc: 0.5699\n",
      "Epoch 25/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3616 - acc: 0.6101 - val_loss: 0.3651 - val_acc: 0.5829\n",
      "Epoch 26/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3632 - acc: 0.6105 - val_loss: 0.3650 - val_acc: 0.5751\n",
      "Epoch 27/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3613 - acc: 0.6137 - val_loss: 0.3665 - val_acc: 0.5596\n",
      "Epoch 28/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3595 - acc: 0.6224 - val_loss: 0.3650 - val_acc: 0.6062\n",
      "Epoch 29/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3637 - acc: 0.5945 - val_loss: 0.3675 - val_acc: 0.5622\n",
      "Epoch 30/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3618 - acc: 0.6110 - val_loss: 0.3661 - val_acc: 0.5596\n",
      "Epoch 31/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3641 - acc: 0.5982 - val_loss: 0.3651 - val_acc: 0.5725\n",
      "Epoch 32/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3649 - acc: 0.5959 - val_loss: 0.3665 - val_acc: 0.5648\n",
      "Epoch 33/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3623 - acc: 0.6069 - val_loss: 0.3647 - val_acc: 0.5751\n",
      "Epoch 34/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3616 - acc: 0.6027 - val_loss: 0.3694 - val_acc: 0.5544\n",
      "Epoch 35/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3649 - acc: 0.6050 - val_loss: 0.3665 - val_acc: 0.5570\n",
      "Epoch 36/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3629 - acc: 0.6096 - val_loss: 0.3669 - val_acc: 0.5622\n",
      "Epoch 37/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3623 - acc: 0.5982 - val_loss: 0.3646 - val_acc: 0.5777\n",
      "Epoch 38/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3618 - acc: 0.6023 - val_loss: 0.3650 - val_acc: 0.5933\n",
      "Epoch 39/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3620 - acc: 0.6114 - val_loss: 0.3653 - val_acc: 0.5699\n",
      "Epoch 40/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3617 - acc: 0.6156 - val_loss: 0.3647 - val_acc: 0.5907\n",
      "Epoch 41/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3610 - acc: 0.6142 - val_loss: 0.3644 - val_acc: 0.5907\n",
      "Epoch 42/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3595 - acc: 0.6215 - val_loss: 0.3647 - val_acc: 0.5907\n",
      "Epoch 43/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3615 - acc: 0.6151 - val_loss: 0.3693 - val_acc: 0.5492\n",
      "Epoch 44/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3615 - acc: 0.6055 - val_loss: 0.3650 - val_acc: 0.5803\n",
      "Epoch 45/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3621 - acc: 0.6009 - val_loss: 0.3644 - val_acc: 0.5777\n",
      "Epoch 46/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3624 - acc: 0.6178 - val_loss: 0.3661 - val_acc: 0.5648\n",
      "Epoch 47/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3630 - acc: 0.6032 - val_loss: 0.3647 - val_acc: 0.5855\n",
      "Epoch 48/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3612 - acc: 0.6151 - val_loss: 0.3649 - val_acc: 0.5855\n",
      "Epoch 49/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3600 - acc: 0.6224 - val_loss: 0.3646 - val_acc: 0.5803\n",
      "Epoch 50/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3627 - acc: 0.6183 - val_loss: 0.3644 - val_acc: 0.5777\n",
      "Epoch 51/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3618 - acc: 0.6082 - val_loss: 0.3641 - val_acc: 0.5907\n",
      "Epoch 52/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3630 - acc: 0.6128 - val_loss: 0.3644 - val_acc: 0.5803\n",
      "Epoch 53/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3622 - acc: 0.6069 - val_loss: 0.3641 - val_acc: 0.5855\n",
      "Epoch 54/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3628 - acc: 0.6114 - val_loss: 0.3646 - val_acc: 0.5881\n",
      "Epoch 55/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3623 - acc: 0.6105 - val_loss: 0.3641 - val_acc: 0.5881\n",
      "Epoch 56/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3601 - acc: 0.6183 - val_loss: 0.3676 - val_acc: 0.5570\n",
      "Epoch 57/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3638 - acc: 0.5968 - val_loss: 0.3646 - val_acc: 0.5803\n",
      "Epoch 58/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3630 - acc: 0.6087 - val_loss: 0.3649 - val_acc: 0.5751\n",
      "Epoch 59/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3635 - acc: 0.5950 - val_loss: 0.3644 - val_acc: 0.5829\n",
      "Epoch 60/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3629 - acc: 0.6096 - val_loss: 0.3651 - val_acc: 0.5699\n",
      "Epoch 61/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3607 - acc: 0.6183 - val_loss: 0.3647 - val_acc: 0.5829\n",
      "Epoch 62/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3607 - acc: 0.6197 - val_loss: 0.3647 - val_acc: 0.5829\n",
      "Epoch 63/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3607 - acc: 0.6183 - val_loss: 0.3646 - val_acc: 0.5855\n",
      "Epoch 64/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3611 - acc: 0.6105 - val_loss: 0.3645 - val_acc: 0.5829\n",
      "Epoch 65/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3608 - acc: 0.6224 - val_loss: 0.3678 - val_acc: 0.5518\n",
      "Epoch 66/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3613 - acc: 0.6133 - val_loss: 0.3645 - val_acc: 0.5777\n",
      "Epoch 67/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3632 - acc: 0.6059 - val_loss: 0.3644 - val_acc: 0.5855\n",
      "Epoch 68/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3607 - acc: 0.6174 - val_loss: 0.3672 - val_acc: 0.5596\n",
      "Epoch 69/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3610 - acc: 0.6183 - val_loss: 0.3645 - val_acc: 0.5829\n",
      "Epoch 70/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3593 - acc: 0.6357 - val_loss: 0.3647 - val_acc: 0.5725\n",
      "Epoch 71/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3609 - acc: 0.6192 - val_loss: 0.3649 - val_acc: 0.5777\n",
      "Epoch 72/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3620 - acc: 0.6142 - val_loss: 0.3648 - val_acc: 0.5881\n",
      "Epoch 73/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3636 - acc: 0.6041 - val_loss: 0.3648 - val_acc: 0.5855\n",
      "Epoch 74/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3608 - acc: 0.6192 - val_loss: 0.3648 - val_acc: 0.5881\n",
      "Epoch 75/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3623 - acc: 0.6165 - val_loss: 0.3646 - val_acc: 0.5959\n",
      "Epoch 76/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3614 - acc: 0.6105 - val_loss: 0.3643 - val_acc: 0.5725\n",
      "Epoch 77/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3585 - acc: 0.6252 - val_loss: 0.3656 - val_acc: 0.5725\n",
      "Epoch 78/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3616 - acc: 0.6233 - val_loss: 0.3643 - val_acc: 0.5751\n",
      "Epoch 79/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3639 - acc: 0.6110 - val_loss: 0.3646 - val_acc: 0.5933\n",
      "Epoch 80/80\n",
      "2185/2185 [==============================] - 2s - loss: 0.3616 - acc: 0.6092 - val_loss: 0.3657 - val_acc: 0.5725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21492a9e50>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=80, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1) 缺少两层全连接, 只有输出全连接层,100次,精度0.62  \n",
    "2) 输出全连接层,缺少Att,100次,精度0.62   \n",
    "3) 正常组: 加Att ,精度0.53  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
