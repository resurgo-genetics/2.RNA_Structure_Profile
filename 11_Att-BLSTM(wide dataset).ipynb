{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PESeq = hkl.load('./Gen_data/11_Gen_Pro_Ehr.hkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for index in range(len(PESeq['label'])):\n",
    "    Y.append(PESeq['label'][index])\n",
    "    X.append(np.append(PESeq['Pro-Kmer'][index],PESeq['Ehr-Kmer'][index],axis=1))\n",
    "X = np.array(X)\n",
    "Y  = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23752, 4096, 2)\n",
      "(23752,)\n",
      "['1' '1' '1']\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(Y)\n",
    "print  Y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT =0.1\n",
    "\n",
    "index = range(len(PESeq['label']))\n",
    "np.random.shuffle(index)\n",
    "nb_validation_samples = int(VALIDATION_SPLIT*len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X[index[: - nb_validation_samples]]\n",
    "y_train = Y[index[:-nb_validation_samples]]\n",
    "x_val = X[index[-nb_validation_samples:]]\n",
    "y_val =Y[index[-nb_validation_samples:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) #one-hot 编码labels\n",
    "y_val = to_categorical(np.asarray(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(4096,2), dtype='float32')\n",
    "l_lstm =Bidirectional(LSTM(50,return_sequences=True))(kmer_input)\n",
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_att)\n",
    "preds = Dense(len( y_train[0]), activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_att)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_3 (InputLayer)             (None, 4096, 2)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 4096, 100)     21200       input_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096, 100)     0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 100)           100         dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             202         attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 21,502\n",
      "Trainable params: 21,502\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21377 samples, validate on 2375 samples\n",
      "Epoch 1/20\n",
      "21377/21377 [==============================] - 2768s - loss: 0.4982 - acc: 0.5319 - val_loss: 0.4959 - val_acc: 0.5364\n",
      "Epoch 2/20\n",
      "21377/21377 [==============================] - 3158s - loss: 0.4975 - acc: 0.5387 - val_loss: 0.4963 - val_acc: 0.5103\n",
      "Epoch 3/20\n",
      "21377/21377 [==============================] - 3080s - loss: 0.4973 - acc: 0.5418 - val_loss: 0.4954 - val_acc: 0.5554\n",
      "Epoch 4/20\n",
      "21377/21377 [==============================] - 3162s - loss: 0.4972 - acc: 0.5453 - val_loss: 0.4954 - val_acc: 0.5482\n",
      "Epoch 5/20\n",
      "21377/21377 [==============================] - 3092s - loss: 0.4971 - acc: 0.5450 - val_loss: 0.4951 - val_acc: 0.5436\n",
      "Epoch 6/20\n",
      "21377/21377 [==============================] - 3917s - loss: 0.4969 - acc: 0.5458 - val_loss: 0.4953 - val_acc: 0.5263\n",
      "Epoch 7/20\n",
      "21377/21377 [==============================] - 4591s - loss: 0.4968 - acc: 0.5457 - val_loss: 0.4952 - val_acc: 0.5368\n",
      "Epoch 8/20\n",
      "21377/21377 [==============================] - 4649s - loss: 0.4970 - acc: 0.5436 - val_loss: 0.4955 - val_acc: 0.5385\n",
      "Epoch 9/20\n",
      "21377/21377 [==============================] - 4579s - loss: 0.4969 - acc: 0.5478 - val_loss: 0.4953 - val_acc: 0.5469\n",
      "Epoch 10/20\n",
      "21377/21377 [==============================] - 4654s - loss: 0.4970 - acc: 0.5431 - val_loss: 0.4956 - val_acc: 0.5533\n",
      "Epoch 11/20\n",
      "21377/21377 [==============================] - 4579s - loss: 0.4970 - acc: 0.5443 - val_loss: 0.4957 - val_acc: 0.5196\n",
      "Epoch 12/20\n",
      "21377/21377 [==============================] - 4652s - loss: 0.4971 - acc: 0.5437 - val_loss: 0.4951 - val_acc: 0.5461\n",
      "Epoch 13/20\n",
      "21377/21377 [==============================] - 4583s - loss: 0.4969 - acc: 0.5435 - val_loss: 0.4962 - val_acc: 0.5482\n",
      "Epoch 14/20\n",
      "21377/21377 [==============================] - 4650s - loss: 0.4969 - acc: 0.5443 - val_loss: 0.4951 - val_acc: 0.5516\n",
      "Epoch 15/20\n",
      "21377/21377 [==============================] - 4580s - loss: 0.4968 - acc: 0.5462 - val_loss: 0.4952 - val_acc: 0.5516\n",
      "Epoch 16/20\n",
      "21377/21377 [==============================] - 4651s - loss: 0.4967 - acc: 0.5474 - val_loss: 0.4957 - val_acc: 0.5238\n",
      "Epoch 17/20\n",
      "21377/21377 [==============================] - 4630s - loss: 0.4967 - acc: 0.5445 - val_loss: 0.4953 - val_acc: 0.5554\n",
      "Epoch 18/20\n",
      "21377/21377 [==============================] - 4602s - loss: 0.4968 - acc: 0.5469 - val_loss: 0.4952 - val_acc: 0.5394\n",
      "Epoch 19/20\n",
      "21377/21377 [==============================] - 4652s - loss: 0.4967 - acc: 0.5478 - val_loss: 0.4952 - val_acc: 0.5465\n",
      "Epoch 20/20\n",
      "21377/21377 [==============================] - 4576s - loss: 0.4967 - acc: 0.5473 - val_loss: 0.4955 - val_acc: 0.5318\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa563bad050>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21377 samples, validate on 2375 samples\n",
      "Epoch 1/20\n",
      "21377/21377 [==============================] - 4652s - loss: 0.4967 - acc: 0.5486 - val_loss: 0.4950 - val_acc: 0.5411\n",
      "Epoch 2/20\n",
      "21377/21377 [==============================] - 4579s - loss: 0.4966 - acc: 0.5460 - val_loss: 0.4951 - val_acc: 0.5528\n",
      "Epoch 3/20\n",
      "21377/21377 [==============================] - 4655s - loss: 0.4965 - acc: 0.5524 - val_loss: 0.4949 - val_acc: 0.5385\n",
      "Epoch 4/20\n",
      "21377/21377 [==============================] - 4581s - loss: 0.4965 - acc: 0.5485 - val_loss: 0.4964 - val_acc: 0.5133\n",
      "Epoch 5/20\n",
      "21377/21377 [==============================] - 4657s - loss: 0.4967 - acc: 0.5493 - val_loss: 0.4950 - val_acc: 0.5394\n",
      "Epoch 6/20\n",
      "21377/21377 [==============================] - 4580s - loss: 0.4970 - acc: 0.5447 - val_loss: 0.4956 - val_acc: 0.5512\n",
      "Epoch 7/20\n",
      "21377/21377 [==============================] - 4655s - loss: 0.4969 - acc: 0.5463 - val_loss: 0.4952 - val_acc: 0.5427\n",
      "Epoch 8/20\n",
      "21377/21377 [==============================] - 4580s - loss: 0.4968 - acc: 0.5484 - val_loss: 0.4951 - val_acc: 0.5482\n",
      "Epoch 9/20\n",
      "21377/21377 [==============================] - 4655s - loss: 0.4967 - acc: 0.5492 - val_loss: 0.4948 - val_acc: 0.5512\n",
      "Epoch 10/20\n",
      "21377/21377 [==============================] - 4579s - loss: 0.4969 - acc: 0.5462 - val_loss: 0.4951 - val_acc: 0.5499\n",
      "Epoch 11/20\n",
      "21377/21377 [==============================] - 4654s - loss: 0.4972 - acc: 0.5409 - val_loss: 0.4957 - val_acc: 0.5545\n",
      "Epoch 12/20\n",
      "21377/21377 [==============================] - 4585s - loss: 0.4974 - acc: 0.5428 - val_loss: 0.4955 - val_acc: 0.5389\n",
      "Epoch 13/20\n",
      "21377/21377 [==============================] - 4654s - loss: 0.4973 - acc: 0.5385 - val_loss: 0.4955 - val_acc: 0.5486\n",
      "Epoch 14/20\n",
      "21377/21377 [==============================] - 4584s - loss: 0.4973 - acc: 0.5429 - val_loss: 0.4960 - val_acc: 0.5558\n",
      "Epoch 15/20\n",
      "21377/21377 [==============================] - 4652s - loss: 0.4972 - acc: 0.5422 - val_loss: 0.4952 - val_acc: 0.5503\n",
      "Epoch 16/20\n",
      "21377/21377 [==============================] - 4574s - loss: 0.4971 - acc: 0.5438 - val_loss: 0.4952 - val_acc: 0.5427\n",
      "Epoch 17/20\n",
      "21377/21377 [==============================] - 4615s - loss: 0.4969 - acc: 0.5460 - val_loss: 0.4959 - val_acc: 0.5234\n",
      "Epoch 18/20\n",
      "21377/21377 [==============================] - 4566s - loss: 0.4969 - acc: 0.5457 - val_loss: 0.4955 - val_acc: 0.5297\n",
      "Epoch 19/20\n",
      "21377/21377 [==============================] - 4660s - loss: 0.4968 - acc: 0.5423 - val_loss: 0.4951 - val_acc: 0.5520\n",
      "Epoch 20/20\n",
      "19200/21377 [=========================>....] - ETA: 449s - loss: 0.4970 - acc: 0.5500"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
