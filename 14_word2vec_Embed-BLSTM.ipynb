{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README    \n",
    "1. generate a Keras Layer :AttLayer  \n",
    "2. Use csv module to import data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1)使用Dense层\n",
    "acc在.73\n",
    "roc在0.79\n",
    "说明数据集没有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datafile  =  '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/pairs.csv'\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bin', 'enhancer_chrom', 'enhancer_distance_to_promoter', 'enhancer_end', 'enhancer_name', 'enhancer_start', 'label', 'promoter_chrom', 'promoter_end', 'promoter_name', 'promoter_start', 'window_end', 'window_start', 'window_chrom', 'window_name', 'interactions_in_window', 'active_promoters_in_window']\n"
     ]
    }
   ],
   "source": [
    "with open(datafile) as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for  row in spamreader:\n",
    "        csvkeys = row\n",
    "        break\n",
    "print csvkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "poscsv = {}\n",
    "negcsv = {}\n",
    "with open(datafile) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for line in reader:\n",
    "        if line['bin']  not in poscsv.keys():\n",
    "            poscsv[line['bin']] =[]\n",
    "        if line ['bin']  not in negcsv.keys():\n",
    "            negcsv[line['bin']] =[]\n",
    "\n",
    "        record=[ line['enhancer_chrom']+' '+\n",
    "                line[ 'enhancer_start']+' '+\n",
    "                line['enhancer_end']+' '+\n",
    "                line['promoter_chrom']+' '+\n",
    "                line['promoter_start']+' '+\n",
    "                line['promoter_end']+' '+\n",
    "                line['enhancer_name']+'-'+line['promoter_name']\n",
    "                  ]\n",
    "        if line['label']==str(1):\n",
    "            poscsv[line['bin']].append(record)\n",
    "        else:\n",
    "            negcsv[line['bin']].append(record)\n",
    "\n",
    "    for key in poscsv.keys():\n",
    "        length = len(poscsv[key])\n",
    "        index = range(len(negcsv[key]))\n",
    "        np.random.shuffle(index)\n",
    "        negcsv[key] = np.array(negcsv[key])\n",
    "        negcsv[key] = negcsv[key][index[:length]]\n",
    "        #print len(negcsv[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2113 2113\n"
     ]
    }
   ],
   "source": [
    "posdata =[]\n",
    "negdata = []\n",
    "for key in poscsv.keys():\n",
    "    posdata = np.append(posdata,poscsv[key])\n",
    "    negdata = np.append(negdata,negcsv[key])\n",
    "assert  len(posdata) == len(negdata)\n",
    "print  len(posdata),len(negdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfasta import Fasta\n",
    "genome = Fasta('/home/yinqijin/WorkSpace/DataHub/genome.fa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 5, 6, 7, 8] [0, 1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "print range(4,9),range(0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#获得ｋｍｅｒ的函数\n",
    "embed_dict ={}\n",
    "def seq_to_embed_seq(seq):\n",
    "    kmer=6\n",
    "    embed_seq = []\n",
    "    if len (seq)< kmer:\n",
    "        return embed_seq\n",
    "    for i in range(kmer,len(seq)):\n",
    "        word  = seq[(i-kmer):i] \n",
    "        #if  word not in embed_dict.keys():\n",
    "        #    embed_dict[ word  ] =str( len(embed_dict.keys()) ) #给标号\n",
    "        #embed_seq.append(  str( embed_dict[ word  ]  )  )\n",
    "        embed_seq.append(  word )\n",
    "    return embed_seq\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(8)[3-2:3+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chr17', '41080595', '41080600', 'chr17', '40728821', '40732200', 'GM12878|chr17:41080595-41080600-GM12878|chr17:40728821-40732200']\n",
      "['chr3', '144015595', '144015600', 'chr3', '143689613', '143692895', 'GM12878|chr3:144015595-144015600-GM12878|chr3:143689613-143692895']\n",
      "['chr6', '27857195', '27857200', 'chr6', '27803860', '27807953', 'GM12878|chr6:27857195-27857200-GM12878|chr6:27803860-27807953']\n",
      "pos over\n",
      "2110\n",
      "['chr16', '66958800', '66958805', 'chr16', '68055893', '68057989', 'GM12878|chr16:66958800-66958805-GM12878|chr16:68055893-68057989']\n",
      "['chr6', '2856795', '2856800', 'chr6', '3156975', '3158233', 'GM12878|chr6:2856795-2856800-GM12878|chr6:3156975-3158233']\n",
      "['chr6', '31793795', '31793800', 'chr6', '31938311', '31941111', 'GM12878|chr6:31793795-31793800-GM12878|chr6:31938311-31941111']\n",
      "['chr19', '17829600', '17829605', 'chr19', '18043141', '18044747', 'GM12878|chr19:17829600-17829605-GM12878|chr19:18043141-18044747']\n",
      "['chr1', '160769395', '160769400', 'chr1', '161014145', '161017200', 'GM12878|chr1:160769395-160769400-GM12878|chr1:161014145-161017200']\n",
      "neg over\n",
      "4218\n"
     ]
    }
   ],
   "source": [
    "sentence_len = 806\n",
    "half_len = sentence_len/2\n",
    "\n",
    "PESeq=dict()\n",
    "PESeq['index']=[]\n",
    "PESeq['Pro-Seq']=[]\n",
    "PESeq['Pro-ESeq']=[]\n",
    "PESeq['Ehr-Seq']=[]\n",
    "PESeq['Ehr-ESeq']=[]\n",
    "PESeq['label'] =[]\n",
    "\n",
    "#正样本\n",
    "for index in range(len(posdata)):\n",
    "    line = posdata[index]\n",
    "    #print line\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    #print len(pro_seq)-len(seq_to_embed_seq(pro_seq))\n",
    "    #print seq_to_embed_seq(pro_seq)\n",
    "    \n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    #print seq_to_embed_seq(ehr_seq)\n",
    "    #break\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(1)\n",
    "print 'pos over'\n",
    "print len(PESeq['label'])  \n",
    "\n",
    "#负样本\n",
    "  \n",
    "for  index in range(len(negdata)):\n",
    "    #break\n",
    "    line = negdata[index]\n",
    "    line = line.split()\n",
    "    ehr_seq = genome[line[0]][int(line[1]): int(line[2]) ].upper()\n",
    "    pro_seq = genome [line[3]][int(line[4]) :int(line[5])].upper()\n",
    "    index = line[6]\n",
    "    \n",
    "    #负样本比较多 可以严格限制 不可以 设置为1000 只剩168\n",
    "    if len(pro_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "    if len(ehr_seq)<6:\n",
    "        print line\n",
    "        continue\n",
    "     \n",
    "    middle = ( int(line[1])+int(line[2]))//2\n",
    "    ehr_seq = genome[line[0]][  (middle-half_len): (middle+half_len)   ].upper()\n",
    "    pro_seq = genome [line[3]][  (middle-half_len): (middle+half_len)  ].upper()\n",
    "    \n",
    "    PESeq['index'].append(index )\n",
    "    PESeq['Pro-Seq'].append(pro_seq)\n",
    "    PESeq['Pro-ESeq'].append( seq_to_embed_seq(pro_seq))\n",
    "    PESeq['Ehr-Seq'].append(ehr_seq)\n",
    "    PESeq['Ehr-ESeq'].append(seq_to_embed_seq(ehr_seq))\n",
    "    PESeq['label'].append(0)\n",
    "print 'neg over'\n",
    "\n",
    "print len(PESeq['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "index (4218,)\n",
      "Ehr-Seq (4218,)\n",
      "label (4218,)\n",
      "Ehr-ESeq (4218, 800)\n",
      "Pro-Seq (4218,)\n",
      "Pro-ESeq (4218, 800)\n",
      "pos data 2110\n",
      "neg data 2108\n"
     ]
    }
   ],
   "source": [
    "print '-'*50\n",
    "for item in PESeq.keys():\n",
    "    print item, np.shape(PESeq[item])\n",
    "print 'pos data' , sum(int( 1 ) if item ==1 else int(0) for item in PESeq['label'])\n",
    "print 'neg data',sum(int( 1) if item ==0 else int(0) for item in PESeq['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pro_Sentense = PESeq['Pro-ESeq']\n",
    "with open('./Gen_data/14_Pro_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Pro_Sentense)[0]):\n",
    "        Sentense = Pro_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ehr_Sentense = PESeq['Ehr-ESeq']\n",
    "with open('./Gen_data/14_Ehr_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( Ehr_Sentense)[0]):\n",
    "        Sentense = Ehr_Sentense[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Pro_sentences = word2vec.LineSentence('./Gen_data/14_Pro_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "pro_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4096, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "Ehr_sentences = word2vec.LineSentence('./Gen_data/14_Ehr_sentense.txt')\n",
    "model = Word2Vec(Pro_sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "ehr_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "TTTTTT\n",
      "AAAAAA\n",
      "ATTTTT\n",
      "AAAAAT\n",
      "(100,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method KeyedVectors.word_vec of <gensim.models.keyedvectors.KeyedVectors object at 0x7fb6bb1a90d0>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print  len(ehr_word_vectors.index2word)\n",
    "for item in ehr_word_vectors.index2word[:4]:\n",
    "    print item\n",
    "print  np.shape(ehr_word_vectors['AAAAAA'])\n",
    "ehr_word_vectors.word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'TTGAAC'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehr_word_vectors.index2word[965]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2531"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehr_word_vectors.index2word.index('TAACTA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = pro_word_vectors.index2word[item]\n",
    "    pro_embedding_matrix[item]= pro_word_vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ehr_embedding_matrix = np.zeros((4096,100))  #4096个字,每个字100维\n",
    "for item in range(4096):\n",
    "    index = ehr_word_vectors.index2word[item]\n",
    "    ehr_embedding_matrix[item]= ehr_word_vectors[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.66526341, -2.18758082, -3.17889857, -0.3516255 ,  1.4194845 ,\n",
       "        1.59716058, -2.64628553, -2.03841424,  0.19542363, -3.02977276,\n",
       "        1.95476043,  0.48411638,  0.17369674,  0.90408862, -1.35204351,\n",
       "       -1.50673091,  1.84095097,  0.80530483, -0.37319571, -0.34351718,\n",
       "       -0.30834603,  1.33482087, -1.05478156, -1.2152648 , -0.33839104,\n",
       "        0.10570761,  0.37969404,  1.27467644,  1.61143684, -0.877904  ,\n",
       "        0.55074215, -3.17754579, -1.57869995, -0.79958737,  0.06922833,\n",
       "        0.97938573,  0.33274606, -1.85906291, -4.0350771 ,  1.40179992,\n",
       "       -0.55582768,  2.50550127,  1.86563015,  1.04470849, -3.43924427,\n",
       "       -0.02449347,  0.83693945,  1.26272774,  1.99617875, -1.88724959,\n",
       "       -0.06803232,  0.91416514,  1.01185358,  0.8829487 ,  3.3631084 ,\n",
       "        1.85665107,  0.92707521, -1.04846418, -2.78248119,  2.17803454,\n",
       "        1.62269282,  2.54663444,  1.02478051,  0.24032617, -1.94887912,\n",
       "       -0.07327405, -0.25843757,  1.85820806, -0.88526863,  1.26068902,\n",
       "        1.33506501,  2.99504995,  0.90370393,  0.21233399,  1.42392528,\n",
       "        0.21548778,  1.88624156, -1.05735862,  0.13057207,  1.07411373,\n",
       "       -0.58917505, -0.82920313,  1.44858301, -0.48136446, -0.4975757 ,\n",
       "        1.4506768 , -0.91131395, -0.89712858, -0.24124637, -0.81969261,\n",
       "       -0.14648558, -0.37212929, -1.11608684, -2.44568801,  2.04248643,\n",
       "        1.26510763, -0.67496341, -0.51986557,  0.72442985,  1.78346217])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.shape(pro_embedding_matrix)\n",
    "pro_embedding_matrix[2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle as pkl\n",
    "output = open('./Gen_data/14_embedding_matrix.pkl','w')\n",
    "pkl.dump(pro_embedding_matrix,output)\n",
    "pkl.dump(ehr_embedding_matrix,output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_file = open('./Gen_data/14_embedding_matrix.pkl','r')\n",
    "pro_embedding_matrix = pkl.load(input_file)\n",
    "ehr_embedding_matrix = pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pro_input=np.zeros((len(PESeq['label']),800),dtype = int )\n",
    "ehr_input=np.zeros((len(PESeq['label']),800),dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pro_input=np.zeros((len(PESeq['label']),800),dtype = str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "pro_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_input.append(PESeq['Pro-ESeq'][item])\n",
    "    #print PESeq['Pro-ESeq'][item]\n",
    "    #print  pro_input\n",
    "    #break\n",
    "\n",
    "fun_pro = lambda word : pro_word_vectors.index2word.index(word)\n",
    "pro_input_1 = pd.DataFrame(pro_input)\n",
    "\n",
    "\n",
    "#print pro_input_1[0]\n",
    "\n",
    "pro_input_2 =pro_input_1.applymap(fun_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#print pro_input_2\n",
    "print pro_input_2.as_matrix()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ehr_input = []\n",
    "for item in range(len(PESeq['label'])):\n",
    "    ehr_input.append(PESeq['Ehr-ESeq'][item])\n",
    "fun_ehr = lambda word : ehr_word_vectors.index2word.index(word)\n",
    "ehr_input_1 = pd.DataFrame(ehr_input)\n",
    "pro_input_1.applymap(fun_ehr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pro_fin!\n",
      "ehr_fin!\n"
     ]
    }
   ],
   "source": [
    "pro_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_origin_sentense = PESeq['Pro-ESeq'][item]\n",
    "    pro_index_sentense = []\n",
    "    for word in  pro_origin_sentense:\n",
    "        \n",
    "        pro_index_sentense.append( pro_word_vectors.index2word.index( word))\n",
    "    pro_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'pro_fin!'\n",
    "ehr_input=[]\n",
    "func = lambda word: ehr_word_vectors.index2word.index(word)\n",
    "for item in range(len(PESeq['label'])):\n",
    "    ehr_origin_sentense = PESeq['Ehr-ESeq'][item]\n",
    "    ehr_index_sentense = []\n",
    "    for word in  ehr_origin_sentense:\n",
    "        ehr_index_sentense.append( ehr_word_vectors.index2word.index( word))\n",
    "    ehr_input.append(pro_index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'ehr_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)\n",
    "ehr_input = np.array(ehr_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4218, 800)\n",
      "(800,)\n",
      "(4218, 800)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(pro_input)\n",
    "print np.shape(pro_input[0])\n",
    "print np.shape(ehr_input)\n",
    "#print np,shape(ehr_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_org = PESeq['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle as pkl\n",
    "output = open('./Gen_data/14_Network_input.pkl','w')\n",
    "pkl.dump(pro_input,output)\n",
    "pkl.dump(ehr_input,output)\n",
    "pkl.dump(PESeq['label'],output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "input_file = open('./Gen_data/14_Network_input.pkl','r')\n",
    "pro_input = pkl.load(input_file)\n",
    "ehr_input = pkl.load(input_file)\n",
    "Y_org=pkl.load(input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu is not available  (error: Unable to get the number of gpus available: CUDA driver version is insufficient for CUDA runtime version)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = to_categorical(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SPLIT_point = int(0.85*len(Y_org))\n",
    "seq_index = range(len(Y_org))\n",
    "shuffle(seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train ,y_train =pro_input[:SPLIT_point], Y[:SPLIT_point]\n",
    "x_valid ,y_valid = pro_input[SPLIT_point:], Y[SPLIT_point:]\n",
    "\n",
    "x_train_ehr ,x_valid_ehr =ehr_input[:SPLIT_point], ehr_input[SPLIT_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800,) [ 0.  1.]\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train[0]),y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print len(pro_input[1]),pro_input[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ehr_input=[]\n",
    "for item in range(len(PESeq['label'])):\n",
    "    pro_origin_sentense = PESeq['Pro-ESeq'][item]\n",
    "    pro_index_sentense = []\n",
    "    for word in  pro_origin_sentense:\n",
    "        \n",
    "        pro_index_sentense.append( pro_word_vectors.index2word.index( word))\n",
    "    pro_input.append(pro_index_sentense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,InputLayer\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten,Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "# Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# temp\n",
    "import numpy as np\n",
    "pro_input=np.zeros((800,4096),dtype = int )\n",
    "ehr_input=np.zeros((800,4096),dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pro_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pro_input = np.array(pro_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 4096)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(pro_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "MAX_SEQUENCE_LENGTH = 800  \n",
    "nb_words =4096   #字典的len(keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "ehr_embedded_sequences = ehr_embedding_layer(ehr_sequence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorVariable' object has no attribute 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-9da08d23c8bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mehr_embedded_sequences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorVariable' object has no attribute 'output'"
     ]
    }
   ],
   "source": [
    "ehr_embedded_sequences.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorVariable' object has no attribute 'get_output_shape_at'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-47ffa37e3ad3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_embedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mehr_embedded_sequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, node_indices, tensor_indices, name)\u001b[0m\n\u001b[1;32m   1297\u001b[0m             self._arguments_validation(layers, mode,\n\u001b[1;32m   1298\u001b[0m                                        \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdot_axes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                                        node_indices, tensor_indices)\n\u001b[0m\u001b[1;32m   1300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m_arguments_validation\u001b[0;34m(self, layers, mode, concat_axis, dot_axes, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m             \u001b[0mlayer_output_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_shape_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;31m# Case: the layer has multiple output tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorVariable' object has no attribute 'get_output_shape_at'"
     ]
    }
   ],
   "source": [
    "all_embedded_sequences = Merge([embedded_sequences,ehr_embedded_sequences], mode='concat',concat_axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_lstm =Bidirectional(LSTM(2,return_sequences=False))(embedded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model  = Model (sequence_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 800, 100)      409600      input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 4)             1648        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             10          bidirectional_1[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 411,258\n",
      "Trainable params: 411,258\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The model expects 2 input arrays, but only received one array. Found: array with shape (3585,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-81b28cee6588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1027\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m   1030\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/lib/python2.7/site-packages/Keras-1.2.2-py2.7.egg/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m     89\u001b[0m             raise ValueError('The model expects ' + str(len(names)) +\n\u001b[1;32m     90\u001b[0m                              \u001b[0;34m' input arrays, but only received one array. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                              'Found: array with shape ' + str(data.shape))\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model expects 2 input arrays, but only received one array. Found: array with shape (3585,)"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "ehr_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[pro_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_8 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "input_9 (InputLayer)             (None, 800)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)          (None, 800, 100)      409600                                       \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional)  (None, 800, 200)      240800      merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 200)           200         bidirectional_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             402         attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 1,060,602\n",
      "Trainable params: 1,060,602\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005)))\n",
    "model.compile(loss='mse', optimizer='rmsprop',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3585 samples, validate on 633 samples\n",
      "Epoch 1/20\n",
      "3328/3585 [==========================>...] - ETA: 43s - loss: 0.5719 - acc: 0.5691"
     ]
    }
   ],
   "source": [
    "model.fit([x_train, x_train_ehr], y_train, batch_size=128, nb_epoch=20, validation_data=([x_valid, x_valid_ehr], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (3585,) (3585, 2)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_train_ehr),np.shape(x_train),np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "\n",
    "y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "print acc\n",
    "\n",
    "auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "print auc\n",
    "\n",
    "f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "print f1_score\n",
    "\n",
    "\n",
    "\n",
    "model.save_weights('./Gen_data/14_Att-BLSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
