{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(lasagne.layers.Layer):\n",
    "    '''\n",
    "    A Attention Layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,incoming,W=lasagne.init.Normal(),\n",
    "                b=lasagne.init.Normal(0.),\n",
    "                 nonlinearity = lasagne.nonlinearities.tanh,\n",
    "                 **kwargs):\n",
    "        super(AttentionLayer,self).__init__(incoming,**kwargs)\n",
    "        \n",
    "        self.nonlinearity = (lasagne.nonlinearities.identity \n",
    "                            if nonlinearity is None else nonlinearity)\n",
    "        self.W = self.add_param(W, (self.input_shape[2],), name=\"W\")\n",
    "        if b is None:\n",
    "            self.b = None\n",
    "        else:\n",
    "            self.b = self.add_param(b,(),name='b',regularizable = False)\n",
    "            \n",
    "    def get_output_shape_for (self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    \n",
    "    def get_output_for(self,input,**kwargs):\n",
    "        activation = T.dot(T.tanh(input),W)\n",
    "        \n",
    "        #add bias\n",
    "        if self.b is not None:\n",
    "            activation = activation + self.b\n",
    "            \n",
    "        #apply nonlinearity\n",
    "        activation = T.exp(activation)\n",
    "        activation /= activation.sum(axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = input *activation.dimshuffle(0,1,'x')\n",
    "        \n",
    "        return weighted_input.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections  import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 == 8000\n"
     ]
    }
   ],
   "source": [
    "datafile ='/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/TRAIN_FILE.TXT'\n",
    "f = open(datafile,'r')\n",
    "sentenses = []\n",
    "labels = []\n",
    "\n",
    "i =0\n",
    "for line in f:\n",
    "    if i%4==0:\n",
    "        sen = line.split('\"')[1]\n",
    "        #print sen\n",
    "        sen = sen.replace('<e1>','')\n",
    "        sen = sen.replace('</e1>','')\n",
    "        sen = sen.replace('<e2>','')\n",
    "        sen = sen.replace('</e2>','')\n",
    "        #print sen\n",
    "        sentenses.append(sen)\n",
    "    elif i%4==1:\n",
    "        labels.append(line.splitlines()[0])\n",
    "    elif i%4==2:\n",
    "        #This Commment , it's useless,pass\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    i += 1\n",
    "    ''' \n",
    "    if i>=12:\n",
    "        print sentenses\n",
    "        print labels\n",
    "        break\n",
    "   '''\n",
    "    \n",
    "f.close()\n",
    "labels = np.array(labels)\n",
    "sentenses = np.array(sentenses)\n",
    "\n",
    "print len(sentenses),'==',len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labes kind(include 'Other'):  19\n",
      "[ 3 16 11 ...,  6 16 18]\n"
     ]
    }
   ],
   "source": [
    "label_name = np.unique(labels)\n",
    "print  'labes kind(include \\'Other\\'): ',len(label_name) \n",
    "#print np.array(label_name)\n",
    "\n",
    "label_dict = defaultdict()\n",
    "for item in label_name:\n",
    "    label_dict[item] = len(label_dict)\n",
    "\n",
    "    \n",
    "func = lambda item:label_dict[item]\n",
    "labels_ = pd.Series(labels)\n",
    "labels_ =labels_.apply(func)\n",
    "#labes = labels_.tolist()\n",
    "labels = np.array(labels_)\n",
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "index = range(len(labels))\n",
    "np.random.shuffle(index)\n",
    "#print index\n",
    "#print labels[:5],sentenses[:5]\n",
    "labels = labels[index]\n",
    "sentenses = sentenses[index]\n",
    "#print labels[:5],sentenses[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = '/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(GLOVE_DIR,'r')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19391 unique tokens.\n",
      "19391\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer.fit_on_sequences Tokenizer.fit_on_texts,  any different??\n",
    "tokenizer = Tokenizer(nb_words= MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(sentenses)\n",
    "sequences = tokenizer.texts_to_sequences(sentenses)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "print len(word_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "model_labels = to_categorical(np.asarray(labels))   #one-hot 编码labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0   21   20 1325    1  102   31 1576   50    1 1802\n",
      "    2 1871    4  353  543] 8000\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "  0.]\n"
     ]
    }
   ],
   "source": [
    "print data[1],data.shape[0]\n",
    "print model_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing and validation set number of positive and negative reviews\n",
      "6400\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "nb_validation_samples = int(VALIDATION_SPLIT*data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = model_labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = model_labels[-nb_validation_samples:]\n",
    "\n",
    "print('Traing and validation set number of positive and negative reviews')\n",
    "print y_train.shape[0]\n",
    "print y_val.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lasagne import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MINIBANTH =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gate Initialize\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    # Setting W_cell to None denotes that no cell connection will be used.\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    # By convention, the cell nonlinearity is tanh in an LSTM.\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot initialize parameters: 'spec' is not a numpy array, a Theano expression, or a callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7eaa0d293019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m l_embed  = layers.EmbeddingLayer( l_in ,len(word_index) + 1,\n\u001b[1;32m      3\u001b[0m                             \u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                             W=[embedding_matrix],)\n\u001b[0m\u001b[1;32m      5\u001b[0m l_lstm = layers.LSTMLayer(l_embed ,only_return_final= False,    ingate=gate_parameters, forgetgate=gate_parameters,\n\u001b[1;32m      6\u001b[0m     \u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutgate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgate_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/bin/src/lasagne/lasagne/layers/embedding.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, incoming, input_size, output_size, W, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/bin/src/lasagne/lasagne/layers/base.pyc\u001b[0m in \u001b[0;36madd_param\u001b[0;34m(self, spec, shape, name, **tags)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s.%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# create shared variable, or pass through given variable/expression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;31m# parameters should be trainable and regularizable by default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainable'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainable'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yinqijin/anaconda2/bin/src/lasagne/lasagne/utils.pyc\u001b[0m in \u001b[0;36mcreate_param\u001b[0;34m(spec, shape, name)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         raise RuntimeError(\"cannot initialize parameters: 'spec' is not \"\n\u001b[0m\u001b[1;32m    338\u001b[0m                            \u001b[0;34m\"a numpy array, a Theano expression, or a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                            \"callable\")\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot initialize parameters: 'spec' is not a numpy array, a Theano expression, or a callable"
     ]
    }
   ],
   "source": [
    "l_in = layers.InputLayer(shape =(MINIBANTH,None))\n",
    "l_embed  = layers.EmbeddingLayer( l_in ,len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            W=[embedding_matrix],)\n",
    "l_lstm = layers.LSTMLayer(l_embed ,only_return_final= False,    ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    learn_init=True, grad_clipping=500.)\n",
    "l_lstm_back = layers.LSTMLayer(l_embed, backwards=True,only_return_final=False,    ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    learn_init=True, grad_clipping=500)\n",
    "l_lstm  = layers.ElemwiseSumLayer([l_lstm,l_lstm_back])\n",
    "l_att = AttentionLayer(l_lstm)\n",
    "l_out = layers.DenseLayer(l_att,num_units= 19, nonlinearity=lasagne.nonlinearities.softmax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, None, 100)\n"
     ]
    }
   ],
   "source": [
    "print L_in.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mask Input \n",
    "L_mask = layers.InputLayer(shape = (MINIBANTH,TIMESTEP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gate Initialize\n",
    "gate_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    b=lasagne.init.Constant(0.))\n",
    "\n",
    "cell_parameters = lasagne.layers.recurrent.Gate(\n",
    "    W_in=lasagne.init.Orthogonal(), W_hid=lasagne.init.Orthogonal(),\n",
    "    # Setting W_cell to None denotes that no cell connection will be used.\n",
    "    W_cell=None, b=lasagne.init.Constant(0.),\n",
    "    # By convention, the cell nonlinearity is tanh in an LSTM.\n",
    "    nonlinearity=lasagne.nonlinearities.tanh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LSTMLayer\n",
    "N_HIDDEN = 10\n",
    "\n",
    "L_lstm = lasagne.layers.recurrent.LSTMLayer(\n",
    "    L_in, N_HIDDEN,\n",
    "    # We need to specify a separate input for masks\n",
    "    mask_input=L_mask,\n",
    "    # Here, we supply the gate parameters for each gate\n",
    "    ingate=gate_parameters, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    # We'll learn the initialization and use gradient clipping\n",
    "    #only_return_final = True, # This is to flatten t\n",
    "    learn_init=True, grad_clipping=100.)\n",
    "\n",
    "L_lstm_back = lasagne.layers.recurrent.LSTMLayer(\n",
    "    L_in, N_HIDDEN, ingate=gate_parameters,\n",
    "    mask_input=L_mask, forgetgate=gate_parameters,\n",
    "    cell=cell_parameters, outgate=gate_parameters,\n",
    "    #only_return_final = True,\n",
    "    learn_init=True, grad_clipping=100., backwards=True)\n",
    "# We'll combine the forward and backward layer output by summing.\n",
    "# Merge layers take in lists of layers to merge as input.\n",
    "L_lstm_sum = lasagne.layers.ElemwiseSumLayer([L_lstm, L_lstm_back])\n",
    "#The output of l_sum will be of shape (n_batch, n_time_steps, N_HIDDEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "print L_lstm_sum.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
