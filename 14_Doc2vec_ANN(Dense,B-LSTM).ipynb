{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_root='/home/yinqijin/WorkSpace/2.RNA_Structure_Profile/Orig_data/targetfinder/GM12878/output-epw/'\n",
    "\n",
    "random.seed(666)\n",
    "\n",
    "fin1  = open(base_root+'training.csv','r')\n",
    "\n",
    "pbin   = [ [],[],[],[],[] ]\n",
    "nbin   = [ [],[],[],[],[] ]\n",
    "bins   = []\n",
    "neg    = []\n",
    "pos    = []\n",
    "for line in fin1:\n",
    "\tif line[0] == 'b':\n",
    "\t\tcontinue\n",
    "\telse:\n",
    "\t\tdata  = line.split('\"')\n",
    "\t\tif data[1] not in bins:\n",
    "\t\t\tbins.append(data[1])\n",
    "\t\tindex = bins.index(data[1])\n",
    "\t\tdata  = line.split(',')\n",
    "\t\tif data[7] == '1':\n",
    "\t\t\tpbin[index].append(line)\n",
    "\t\t\tpos.append(line)\n",
    "\t\telse:\n",
    "\t\t\tnbin[index].append(line)\t\n",
    "\n",
    "\n",
    "for i in range(len(bins)):\n",
    "\tnbinlen = len(nbin[i])\n",
    "\ttemp    = random.sample(nbin[i],nbinlen/20)\n",
    "\tneg.extend(temp)\n",
    "\n",
    "nnum   = len(neg)\n",
    "pnum   = len(pos)\n",
    "fnum   = len(neg[0].strip().split(',')) - 18\n",
    "\n",
    "print nnum,pnum,fnum\n",
    "\n",
    "fin2   = open(base_root+'enhancers.bed','r')\n",
    "fin3   = open(base_root+'promoters.bed','r')\n",
    "enhancers = []\n",
    "promoters = []\n",
    "\n",
    "arrays1 = numpy.zeros((nnum+pnum,100,2))\n",
    "labels1 = numpy.zeros(nnum+pnum)\n",
    "\n",
    "for line in fin2:\n",
    "\tdata = line.strip().split()\n",
    "\tenhancers.append(data[3])\n",
    "\n",
    "for line in fin3:\n",
    "\tdata = line.strip().split()\n",
    "\tpromoters.append(data[3])\n",
    "\n",
    "enhancer_model = Doc2Vec.load(base_root+'enhancers_6_1_100.d2v')\n",
    "promoter_model = Doc2Vec.load(base_root+'promoters_6_1_100.d2v')\n",
    "\n",
    "\n",
    "for i in range(len(pos)):\n",
    "        data = pos[i].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))\n",
    "\tarrays1[i] = numpy.append(enhancer_vec,promoter_vec,axis=1)\n",
    "\tlabels1[i] = 1\n",
    "\n",
    "for j in range(len(neg)):\n",
    "        data = neg[j].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))  #.reshape((1,100))\n",
    "\tarrays1[i+j] = numpy.append(enhancer_vec,promoter_vec,axis=1)  #umn_stack\n",
    "\tlabels1[i+j] = 0\n",
    "\n",
    "print numpy.shape(arrays1)\n",
    "\n",
    "arrays1 = numpy.array(arrays1)\n",
    "labels1 = numpy.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "from keras.utils.np_utils import to_categorical\n",
    "index = range(len(labels1))\n",
    "random.shuffle(index)\n",
    "SPLIT_RATIO = 0.85\n",
    "split_point =int(SPLIT_RATIO*len(labels1))\n",
    "\n",
    "print split_point\n",
    "\n",
    "x_train , y_train=  arrays1[index[:split_point]] ,labels1[ index [:split_point]]\n",
    "x_valid ,y_valid= arrays1[index[split_point:]] , labels1[index[split_point:]]\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedata =open('./Gen_data/14_DocVec_all_data.pkl','w')\n",
    "pkl.dump(x_train,savedata)\n",
    "pkl.dump(y_train,savedata)\n",
    "pkl.dump(x_valid,savedata)\n",
    "pkl.dump(y_valid,savedata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaddata = open('./Gen_data/14_DocVec_all_data.pkl','r')\n",
    "x_train=pkl.load(loaddata)\n",
    "y_train=pkl.load(loaddata)\n",
    "x_valid=pkl.load(loaddata)\n",
    "y_valid=pkl.load(loaddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "from collections  import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net1: shallow Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')\n",
    "l_flatten= Flatten()(kmer_input)\n",
    "\n",
    "l_dense_0 = Dense(64)(l_flatten)\n",
    "l_drop_0 = Dropout(0.2)(l_dense_0)\n",
    "\n",
    "l_dense_1 = Dense(32)(l_drop_0)\n",
    "l_drop_1 = Dropout(0.3)(l_dense_1)\n",
    "\n",
    "l_dense_2 = Dense(16)(l_drop_1)\n",
    "l_drop_2 = Dropout(0.5)(l_dense_2)\n",
    "\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_drop_2)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果很差,acc只有0.58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net2: deep Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')\n",
    "l_flatten= Flatten()(kmer_input)\n",
    "\n",
    "l_dense_0 = Dense(200)(l_flatten)\n",
    "l_drop_0 = Dropout(0.2)(l_dense_0)\n",
    "\n",
    "l_dense_1 = Dense(128)(l_drop_0)\n",
    "l_drop_1 = Dropout(0.3)(l_dense_1)\n",
    "\n",
    "l_dense_2 = Dense(64)(l_drop_1)\n",
    "l_drop_2 = Dropout(0.5)(l_dense_2)\n",
    "\n",
    "l_dense_3 = Dense(32)(l_drop_2)\n",
    "l_drop_3 = Dropout(0.5)(l_dense_3)\n",
    "\n",
    "l_dense_4 = Dense(16)(l_drop_3)\n",
    "l_drop_4 = Dropout(0.5)(l_dense_4)\n",
    "\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_drop_4)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果也很查,acc 只有0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net3 : Att-BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')\n",
    "\n",
    "l_lstm =Bidirectional(LSTM(100,return_sequences=True))(kmer_input)\n",
    "\n",
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "#l_flatten = Flatten()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_att)\n",
    "#l_dense = Dense(256)(l_att_drop)\n",
    "l_dense_1 = Dense(16)(l_att_drop)\n",
    "\n",
    "\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_dense_1)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果很差 最好才0.57"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net4: BLSTM+Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')\n",
    "l_lstm =Bidirectional(LSTM(5,return_sequences=True))(kmer_input)\n",
    "\n",
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "l_flatten = Flatten()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_flatten)\n",
    "l_dense = Dense(128)(l_att_drop)\n",
    "l_dense_1 = Dense(32)(l_dense)\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_dense_1)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile&Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 100, 2)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 100, 200)      82400       input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)             (None, 100, 200)      0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_2 (AttLayer)            (None, 200)           200         dropout_20[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)             (None, 200)           0           attlayer_2[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 16)            3216        dropout_21[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 2)             34          dense_25[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 85,850\n",
      "Trainable params: 85,850\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3589 samples, validate on 634 samples\n",
      "Epoch 1/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3713 - acc: 0.5600 - val_loss: 0.3737 - val_acc: 0.5315\n",
      "Epoch 2/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3700 - acc: 0.5748 - val_loss: 0.3691 - val_acc: 0.5505\n",
      "Epoch 3/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3722 - acc: 0.5573 - val_loss: 0.3725 - val_acc: 0.5095\n",
      "Epoch 4/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3748 - acc: 0.5138 - val_loss: 0.3735 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3746 - acc: 0.5141 - val_loss: 0.3713 - val_acc: 0.5773\n",
      "Epoch 6/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3742 - acc: 0.5216 - val_loss: 0.3710 - val_acc: 0.5552\n",
      "Epoch 7/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3742 - acc: 0.5364 - val_loss: 0.3721 - val_acc: 0.5284\n",
      "Epoch 8/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3738 - acc: 0.5378 - val_loss: 0.3713 - val_acc: 0.5426\n",
      "Epoch 9/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3736 - acc: 0.5394 - val_loss: 0.3719 - val_acc: 0.5237\n",
      "Epoch 10/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3736 - acc: 0.5467 - val_loss: 0.3699 - val_acc: 0.5757\n",
      "Epoch 11/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3733 - acc: 0.5442 - val_loss: 0.3706 - val_acc: 0.5584\n",
      "Epoch 12/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3733 - acc: 0.5456 - val_loss: 0.3710 - val_acc: 0.5552\n",
      "Epoch 13/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3732 - acc: 0.5383 - val_loss: 0.3720 - val_acc: 0.5237\n",
      "Epoch 14/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3737 - acc: 0.5358 - val_loss: 0.3730 - val_acc: 0.4637\n",
      "Epoch 15/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3745 - acc: 0.5130 - val_loss: 0.3732 - val_acc: 0.4890\n",
      "Epoch 16/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3753 - acc: 0.4957 - val_loss: 0.3730 - val_acc: 0.4890\n",
      "Epoch 17/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3746 - acc: 0.5185 - val_loss: 0.3738 - val_acc: 0.4968\n",
      "Epoch 18/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3752 - acc: 0.4921 - val_loss: 0.3727 - val_acc: 0.5000\n",
      "Epoch 19/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3748 - acc: 0.5079 - val_loss: 0.3729 - val_acc: 0.4937\n",
      "Epoch 20/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3745 - acc: 0.5040 - val_loss: 0.3719 - val_acc: 0.5252\n",
      "Epoch 21/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3738 - acc: 0.5272 - val_loss: 0.3729 - val_acc: 0.5205\n",
      "Epoch 22/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3743 - acc: 0.5263 - val_loss: 0.3715 - val_acc: 0.5552\n",
      "Epoch 23/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3733 - acc: 0.5475 - val_loss: 0.3708 - val_acc: 0.5552\n",
      "Epoch 24/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3729 - acc: 0.5469 - val_loss: 0.3715 - val_acc: 0.5489\n",
      "Epoch 25/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3745 - acc: 0.5166 - val_loss: 0.3723 - val_acc: 0.5410\n",
      "Epoch 26/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3728 - acc: 0.5528 - val_loss: 0.3703 - val_acc: 0.5678\n",
      "Epoch 27/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3739 - acc: 0.5252 - val_loss: 0.3726 - val_acc: 0.5347\n",
      "Epoch 28/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3747 - acc: 0.5149 - val_loss: 0.3723 - val_acc: 0.5095\n",
      "Epoch 29/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3747 - acc: 0.5180 - val_loss: 0.3733 - val_acc: 0.5032\n",
      "Epoch 30/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3743 - acc: 0.5266 - val_loss: 0.3722 - val_acc: 0.5095\n",
      "Epoch 31/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3740 - acc: 0.5263 - val_loss: 0.3730 - val_acc: 0.4937\n",
      "Epoch 32/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3746 - acc: 0.5188 - val_loss: 0.3727 - val_acc: 0.5174\n",
      "Epoch 33/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3743 - acc: 0.5258 - val_loss: 0.3710 - val_acc: 0.5584\n",
      "Epoch 34/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3739 - acc: 0.5350 - val_loss: 0.3732 - val_acc: 0.5063\n",
      "Epoch 35/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3730 - acc: 0.5531 - val_loss: 0.3709 - val_acc: 0.5647\n",
      "Epoch 36/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3740 - acc: 0.5308 - val_loss: 0.3710 - val_acc: 0.5741\n",
      "Epoch 37/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3732 - acc: 0.5461 - val_loss: 0.3717 - val_acc: 0.5457\n",
      "Epoch 38/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3734 - acc: 0.5355 - val_loss: 0.3702 - val_acc: 0.5694\n",
      "Epoch 39/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3732 - acc: 0.5472 - val_loss: 0.3711 - val_acc: 0.5426\n",
      "Epoch 40/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3740 - acc: 0.5364 - val_loss: 0.3727 - val_acc: 0.5174\n",
      "Epoch 41/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3732 - acc: 0.5495 - val_loss: 0.3740 - val_acc: 0.4968\n",
      "Epoch 42/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3751 - acc: 0.5130 - val_loss: 0.3718 - val_acc: 0.5379\n",
      "Epoch 43/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3738 - acc: 0.5361 - val_loss: 0.3702 - val_acc: 0.5678\n",
      "Epoch 44/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3740 - acc: 0.5325 - val_loss: 0.3739 - val_acc: 0.4937\n",
      "Epoch 45/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3734 - acc: 0.5378 - val_loss: 0.3722 - val_acc: 0.5268\n",
      "Epoch 46/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3733 - acc: 0.5366 - val_loss: 0.3711 - val_acc: 0.5631\n",
      "Epoch 47/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3736 - acc: 0.5347 - val_loss: 0.3715 - val_acc: 0.5410\n",
      "Epoch 48/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3732 - acc: 0.5447 - val_loss: 0.3706 - val_acc: 0.5599\n",
      "Epoch 49/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3738 - acc: 0.5378 - val_loss: 0.3719 - val_acc: 0.5347\n",
      "Epoch 50/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3732 - acc: 0.5489 - val_loss: 0.3709 - val_acc: 0.5410\n",
      "Epoch 51/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3729 - acc: 0.5517 - val_loss: 0.3719 - val_acc: 0.5363\n",
      "Epoch 52/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3735 - acc: 0.5400 - val_loss: 0.3735 - val_acc: 0.5158\n",
      "Epoch 53/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3747 - acc: 0.5166 - val_loss: 0.3722 - val_acc: 0.5331\n",
      "Epoch 54/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3736 - acc: 0.5247 - val_loss: 0.3703 - val_acc: 0.5726\n",
      "Epoch 55/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3730 - acc: 0.5511 - val_loss: 0.3698 - val_acc: 0.5789\n",
      "Epoch 56/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3728 - acc: 0.5525 - val_loss: 0.3731 - val_acc: 0.5268\n",
      "Epoch 57/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3731 - acc: 0.5433 - val_loss: 0.3702 - val_acc: 0.5726\n",
      "Epoch 58/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3724 - acc: 0.5400 - val_loss: 0.3711 - val_acc: 0.5536\n",
      "Epoch 59/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3718 - acc: 0.5570 - val_loss: 0.3706 - val_acc: 0.5631\n",
      "Epoch 60/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3730 - acc: 0.5430 - val_loss: 0.3756 - val_acc: 0.4921\n",
      "Epoch 61/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3749 - acc: 0.5118 - val_loss: 0.3732 - val_acc: 0.4984\n",
      "Epoch 62/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3747 - acc: 0.5116 - val_loss: 0.3727 - val_acc: 0.5410\n",
      "Epoch 63/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3742 - acc: 0.5163 - val_loss: 0.3716 - val_acc: 0.5536\n",
      "Epoch 64/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3737 - acc: 0.5300 - val_loss: 0.3737 - val_acc: 0.4953\n",
      "Epoch 65/100\n",
      "3589/3589 [==============================] - 35s - loss: 0.3748 - acc: 0.5029 - val_loss: 0.3727 - val_acc: 0.5347\n",
      "Epoch 66/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3738 - acc: 0.5339 - val_loss: 0.3708 - val_acc: 0.5568\n",
      "Epoch 67/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3729 - acc: 0.5539 - val_loss: 0.3700 - val_acc: 0.5552\n",
      "Epoch 68/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3728 - acc: 0.5419 - val_loss: 0.3708 - val_acc: 0.5489\n",
      "Epoch 69/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3724 - acc: 0.5606 - val_loss: 0.3706 - val_acc: 0.5552\n",
      "Epoch 70/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3728 - acc: 0.5481 - val_loss: 0.3726 - val_acc: 0.5300\n",
      "Epoch 71/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3728 - acc: 0.5506 - val_loss: 0.3702 - val_acc: 0.5741\n",
      "Epoch 72/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3721 - acc: 0.5595 - val_loss: 0.3735 - val_acc: 0.5142\n",
      "Epoch 73/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3730 - acc: 0.5355 - val_loss: 0.3715 - val_acc: 0.5568\n",
      "Epoch 74/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3725 - acc: 0.5542 - val_loss: 0.3727 - val_acc: 0.5284\n",
      "Epoch 75/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3728 - acc: 0.5489 - val_loss: 0.3731 - val_acc: 0.5142\n",
      "Epoch 76/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3730 - acc: 0.5430 - val_loss: 0.3715 - val_acc: 0.5347\n",
      "Epoch 77/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3723 - acc: 0.5550 - val_loss: 0.3703 - val_acc: 0.5647\n",
      "Epoch 78/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3715 - acc: 0.5489 - val_loss: 0.3705 - val_acc: 0.5662\n",
      "Epoch 79/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3728 - acc: 0.5514 - val_loss: 0.3705 - val_acc: 0.5662\n",
      "Epoch 80/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3721 - acc: 0.5587 - val_loss: 0.3737 - val_acc: 0.5252\n",
      "Epoch 81/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3740 - acc: 0.5325 - val_loss: 0.3714 - val_acc: 0.5410\n",
      "Epoch 82/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3733 - acc: 0.5486 - val_loss: 0.3712 - val_acc: 0.5489\n",
      "Epoch 83/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3733 - acc: 0.5478 - val_loss: 0.3702 - val_acc: 0.5568\n",
      "Epoch 84/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3734 - acc: 0.5391 - val_loss: 0.3731 - val_acc: 0.4968\n",
      "Epoch 85/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3740 - acc: 0.5311 - val_loss: 0.3721 - val_acc: 0.4953\n",
      "Epoch 86/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3748 - acc: 0.5102 - val_loss: 0.3715 - val_acc: 0.5584\n",
      "Epoch 87/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3739 - acc: 0.5249 - val_loss: 0.3723 - val_acc: 0.5252\n",
      "Epoch 88/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3740 - acc: 0.5372 - val_loss: 0.3727 - val_acc: 0.5063\n",
      "Epoch 89/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3741 - acc: 0.5205 - val_loss: 0.3720 - val_acc: 0.5379\n",
      "Epoch 90/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3732 - acc: 0.5422 - val_loss: 0.3717 - val_acc: 0.5568\n",
      "Epoch 91/100\n",
      "3589/3589 [==============================] - 32s - loss: 0.3732 - acc: 0.5358 - val_loss: 0.3730 - val_acc: 0.5079\n",
      "Epoch 92/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3735 - acc: 0.5333 - val_loss: 0.3719 - val_acc: 0.5331\n",
      "Epoch 93/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3737 - acc: 0.5288 - val_loss: 0.3719 - val_acc: 0.5174\n",
      "Epoch 94/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3732 - acc: 0.5330 - val_loss: 0.3716 - val_acc: 0.5536\n",
      "Epoch 95/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3725 - acc: 0.5570 - val_loss: 0.3729 - val_acc: 0.5032\n",
      "Epoch 96/100\n",
      "3589/3589 [==============================] - 34s - loss: 0.3734 - acc: 0.5380 - val_loss: 0.3703 - val_acc: 0.5662\n",
      "Epoch 97/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3724 - acc: 0.5559 - val_loss: 0.3796 - val_acc: 0.5174\n",
      "Epoch 98/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3729 - acc: 0.5508 - val_loss: 0.3707 - val_acc: 0.5521\n",
      "Epoch 99/100\n",
      "3589/3589 [==============================] - 33s - loss: 0.3728 - acc: 0.5481 - val_loss: 0.3721 - val_acc: 0.5032\n",
      "Epoch 100/100\n",
      "3589/3589 [==============================] - 31s - loss: 0.3733 - acc: 0.5464 - val_loss: 0.3706 - val_acc: 0.5662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6a8dccd90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3589 samples, validate on 634 samples\n",
      "Epoch 1/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3569 - acc: 0.6456 - val_loss: 0.3716 - val_acc: 0.5883\n",
      "Epoch 2/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3551 - acc: 0.6481 - val_loss: 0.3676 - val_acc: 0.5883\n",
      "Epoch 3/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3545 - acc: 0.6498 - val_loss: 0.3671 - val_acc: 0.5694\n",
      "Epoch 4/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3560 - acc: 0.6570 - val_loss: 0.3680 - val_acc: 0.5694\n",
      "Epoch 5/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3554 - acc: 0.6431 - val_loss: 0.3760 - val_acc: 0.5757\n",
      "Epoch 6/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3568 - acc: 0.6514 - val_loss: 0.3724 - val_acc: 0.5647\n",
      "Epoch 7/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3568 - acc: 0.6537 - val_loss: 0.3722 - val_acc: 0.5852\n",
      "Epoch 8/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6459 - val_loss: 0.3765 - val_acc: 0.5678\n",
      "Epoch 9/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6475 - val_loss: 0.3665 - val_acc: 0.5868\n",
      "Epoch 10/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3564 - acc: 0.6531 - val_loss: 0.3677 - val_acc: 0.5883\n",
      "Epoch 11/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6520 - val_loss: 0.3690 - val_acc: 0.5694\n",
      "Epoch 12/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3558 - acc: 0.6492 - val_loss: 0.3702 - val_acc: 0.5883\n",
      "Epoch 13/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6517 - val_loss: 0.3678 - val_acc: 0.5804\n",
      "Epoch 14/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3573 - acc: 0.6456 - val_loss: 0.3691 - val_acc: 0.5915\n",
      "Epoch 15/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3566 - acc: 0.6434 - val_loss: 0.3694 - val_acc: 0.5631\n",
      "Epoch 16/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6473 - val_loss: 0.3707 - val_acc: 0.5868\n",
      "Epoch 17/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6467 - val_loss: 0.3671 - val_acc: 0.5726\n",
      "Epoch 18/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6481 - val_loss: 0.3679 - val_acc: 0.5852\n",
      "Epoch 19/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3570 - acc: 0.6528 - val_loss: 0.3679 - val_acc: 0.5836\n",
      "Epoch 20/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3563 - acc: 0.6484 - val_loss: 0.3664 - val_acc: 0.5836\n",
      "Epoch 21/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3567 - acc: 0.6517 - val_loss: 0.3689 - val_acc: 0.5931\n",
      "Epoch 22/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3565 - acc: 0.6545 - val_loss: 0.3679 - val_acc: 0.5804\n",
      "Epoch 23/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6498 - val_loss: 0.3675 - val_acc: 0.5978\n",
      "Epoch 24/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3559 - acc: 0.6567 - val_loss: 0.3695 - val_acc: 0.5804\n",
      "Epoch 25/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3558 - acc: 0.6520 - val_loss: 0.3767 - val_acc: 0.5647\n",
      "Epoch 26/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3561 - acc: 0.6525 - val_loss: 0.3681 - val_acc: 0.5757\n",
      "Epoch 27/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3559 - acc: 0.6562 - val_loss: 0.3666 - val_acc: 0.5741\n",
      "Epoch 28/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6475 - val_loss: 0.3688 - val_acc: 0.5804\n",
      "Epoch 29/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6434 - val_loss: 0.3676 - val_acc: 0.5836\n",
      "Epoch 30/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3550 - acc: 0.6551 - val_loss: 0.3660 - val_acc: 0.5852\n",
      "Epoch 31/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3551 - acc: 0.6512 - val_loss: 0.3656 - val_acc: 0.5773\n",
      "Epoch 32/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3568 - acc: 0.6498 - val_loss: 0.3686 - val_acc: 0.5789\n",
      "Epoch 33/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3547 - acc: 0.6553 - val_loss: 0.3687 - val_acc: 0.5931\n",
      "Epoch 34/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3558 - acc: 0.6484 - val_loss: 0.3659 - val_acc: 0.5789\n",
      "Epoch 35/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6484 - val_loss: 0.3735 - val_acc: 0.5820\n",
      "Epoch 36/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3543 - acc: 0.6498 - val_loss: 0.3704 - val_acc: 0.5726\n",
      "Epoch 37/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3548 - acc: 0.6537 - val_loss: 0.3681 - val_acc: 0.5836\n",
      "Epoch 38/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3559 - acc: 0.6503 - val_loss: 0.3721 - val_acc: 0.5962\n",
      "Epoch 39/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3560 - acc: 0.6525 - val_loss: 0.3657 - val_acc: 0.5899\n",
      "Epoch 40/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3551 - acc: 0.6425 - val_loss: 0.3668 - val_acc: 0.5694\n",
      "Epoch 41/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3549 - acc: 0.6503 - val_loss: 0.3768 - val_acc: 0.5883\n",
      "Epoch 42/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3560 - acc: 0.6450 - val_loss: 0.3680 - val_acc: 0.5868\n",
      "Epoch 43/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6478 - val_loss: 0.3692 - val_acc: 0.5820\n",
      "Epoch 44/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3550 - acc: 0.6523 - val_loss: 0.3676 - val_acc: 0.5915\n",
      "Epoch 45/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6439 - val_loss: 0.3682 - val_acc: 0.5789\n",
      "Epoch 46/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3547 - acc: 0.6464 - val_loss: 0.3695 - val_acc: 0.5931\n",
      "Epoch 47/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3558 - acc: 0.6525 - val_loss: 0.3683 - val_acc: 0.5836\n",
      "Epoch 48/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3570 - acc: 0.6467 - val_loss: 0.3686 - val_acc: 0.5962\n",
      "Epoch 49/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3554 - acc: 0.6475 - val_loss: 0.3673 - val_acc: 0.5757\n",
      "Epoch 50/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3563 - acc: 0.6428 - val_loss: 0.3696 - val_acc: 0.5915\n",
      "Epoch 51/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3564 - acc: 0.6517 - val_loss: 0.3720 - val_acc: 0.5694\n",
      "Epoch 52/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3552 - acc: 0.6498 - val_loss: 0.3684 - val_acc: 0.5694\n",
      "Epoch 53/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3551 - acc: 0.6581 - val_loss: 0.3660 - val_acc: 0.5741\n",
      "Epoch 54/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3556 - acc: 0.6442 - val_loss: 0.3669 - val_acc: 0.5694\n",
      "Epoch 55/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3552 - acc: 0.6517 - val_loss: 0.3659 - val_acc: 0.5868\n",
      "Epoch 56/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3556 - acc: 0.6520 - val_loss: 0.3724 - val_acc: 0.5852\n",
      "Epoch 57/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3536 - acc: 0.6531 - val_loss: 0.3690 - val_acc: 0.6025\n",
      "Epoch 58/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3552 - acc: 0.6489 - val_loss: 0.3670 - val_acc: 0.5868\n",
      "Epoch 59/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3554 - acc: 0.6542 - val_loss: 0.3706 - val_acc: 0.5931\n",
      "Epoch 60/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3565 - acc: 0.6473 - val_loss: 0.3679 - val_acc: 0.5931\n",
      "Epoch 61/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6467 - val_loss: 0.3667 - val_acc: 0.5757\n",
      "Epoch 62/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6567 - val_loss: 0.3684 - val_acc: 0.5899\n",
      "Epoch 63/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3549 - acc: 0.6503 - val_loss: 0.3726 - val_acc: 0.5694\n",
      "Epoch 64/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3549 - acc: 0.6553 - val_loss: 0.3687 - val_acc: 0.5789\n",
      "Epoch 65/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3554 - acc: 0.6475 - val_loss: 0.3686 - val_acc: 0.5631\n",
      "Epoch 66/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3556 - acc: 0.6545 - val_loss: 0.3665 - val_acc: 0.5726\n",
      "Epoch 67/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6498 - val_loss: 0.3700 - val_acc: 0.5804\n",
      "Epoch 68/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3542 - acc: 0.6615 - val_loss: 0.3704 - val_acc: 0.5726\n",
      "Epoch 69/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3542 - acc: 0.6506 - val_loss: 0.3685 - val_acc: 0.5883\n",
      "Epoch 70/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6489 - val_loss: 0.3688 - val_acc: 0.5710\n",
      "Epoch 71/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3555 - acc: 0.6576 - val_loss: 0.3705 - val_acc: 0.5836\n",
      "Epoch 72/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3563 - acc: 0.6520 - val_loss: 0.3668 - val_acc: 0.5804\n",
      "Epoch 73/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3543 - acc: 0.6587 - val_loss: 0.3708 - val_acc: 0.5726\n",
      "Epoch 74/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6484 - val_loss: 0.3687 - val_acc: 0.5931\n",
      "Epoch 75/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3557 - acc: 0.6484 - val_loss: 0.3765 - val_acc: 0.5899\n",
      "Epoch 76/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3541 - acc: 0.6514 - val_loss: 0.3724 - val_acc: 0.5615\n",
      "Epoch 77/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3548 - acc: 0.6498 - val_loss: 0.3675 - val_acc: 0.5789\n",
      "Epoch 78/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3547 - acc: 0.6450 - val_loss: 0.3680 - val_acc: 0.5962\n",
      "Epoch 79/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6456 - val_loss: 0.3692 - val_acc: 0.5899\n",
      "Epoch 80/80\n",
      "3589/3589 [==============================] - 0s - loss: 0.3553 - acc: 0.6503 - val_loss: 0.3674 - val_acc: 0.5852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff6d075af50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=80, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result  \n",
    "1) 缺少两层全连接, 只有输出全连接层,100次,精度0.62  \n",
    "2) 输出全连接层,缺少Att,100次,精度0.62   \n",
    "3) 正常组: 加Att ,精度0.53  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
