{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PESeq = hkl.load('./Gen_data/11_Gen_Pro_Ehr.hkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-process data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=[]\n",
    "Y=[]\n",
    "for index in range(len(PESeq['label'])):\n",
    "    Y.append(PESeq['label'][index])\n",
    "    X.append(np.append(PESeq['Pro-Kmer'][index],PESeq['Ehr-Kmer'][index],axis=1))\n",
    "X = np.array(X)\n",
    "Y  = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT =0.1\n",
    "\n",
    "index = range(len(PESeq['label']))\n",
    "np.random.shuffle(index)\n",
    "nb_validation_samples = int(VALIDATION_SPLIT*len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = X[index[: - nb_validation_samples]]\n",
    "y_train = Y[index[:-nb_validation_samples]]\n",
    "x_val = X[index[-nb_validation_samples:]]\n",
    "y_val =Y[index[-nb_validation_samples:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) #one-hot 编码labels\n",
    "y_val = to_categorical(np.asarray(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(4096,2), dtype='float32')\n",
    "l_lstm =Bidirectional(LSTM(2,return_sequences=True))(kmer_input)\n",
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_att)\n",
    "preds = Dense(len( y_train[0]), activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_att)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 4096, 2)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 4096, 4)       80          input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096, 4)       0           bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 4)             4           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             10          attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 94\n",
      "Trainable params: 94\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21377 samples, validate on 2375 samples\n",
      "Epoch 1/20\n",
      "21377/21377 [==============================] - 4650s - loss: 0.4995 - acc: 0.5037 - val_loss: 0.4972 - val_acc: 0.5154\n",
      "Epoch 2/20\n",
      "21377/21377 [==============================] - 4583s - loss: 0.4979 - acc: 0.5365 - val_loss: 0.4965 - val_acc: 0.5171\n",
      "Epoch 3/20\n",
      "21377/21377 [==============================] - 4646s - loss: 0.4973 - acc: 0.5401 - val_loss: 0.4961 - val_acc: 0.5339\n",
      "Epoch 4/20\n",
      "21377/21377 [==============================] - 4574s - loss: 0.4972 - acc: 0.5418 - val_loss: 0.4961 - val_acc: 0.5373\n",
      "Epoch 5/20\n",
      "21377/21377 [==============================] - 4644s - loss: 0.4972 - acc: 0.5400 - val_loss: 0.4962 - val_acc: 0.5267\n",
      "Epoch 6/20\n",
      "21377/21377 [==============================] - 4572s - loss: 0.4972 - acc: 0.5420 - val_loss: 0.4964 - val_acc: 0.5196\n",
      "Epoch 7/20\n",
      "21377/21377 [==============================] - 4646s - loss: 0.4972 - acc: 0.5417 - val_loss: 0.4959 - val_acc: 0.5364\n",
      "Epoch 8/20\n",
      "21377/21377 [==============================] - 4570s - loss: 0.4972 - acc: 0.5402 - val_loss: 0.4959 - val_acc: 0.5381\n",
      "Epoch 9/20\n",
      "21377/21377 [==============================] - 4645s - loss: 0.4971 - acc: 0.5423 - val_loss: 0.4959 - val_acc: 0.5356\n",
      "Epoch 10/20\n",
      "21377/21377 [==============================] - 4571s - loss: 0.4971 - acc: 0.5427 - val_loss: 0.4959 - val_acc: 0.5356\n",
      "Epoch 11/20\n",
      "21377/21377 [==============================] - 4646s - loss: 0.4971 - acc: 0.5423 - val_loss: 0.4959 - val_acc: 0.5352\n",
      "Epoch 12/20\n",
      "21377/21377 [==============================] - 4570s - loss: 0.4971 - acc: 0.5402 - val_loss: 0.4959 - val_acc: 0.5352\n",
      "Epoch 13/20\n",
      "21377/21377 [==============================] - 4646s - loss: 0.4971 - acc: 0.5419 - val_loss: 0.4960 - val_acc: 0.5288\n",
      "Epoch 14/20\n",
      "21377/21377 [==============================] - 4568s - loss: 0.4971 - acc: 0.5424 - val_loss: 0.4958 - val_acc: 0.5356\n",
      "Epoch 15/20\n",
      "21377/21377 [==============================] - 4645s - loss: 0.4970 - acc: 0.5442 - val_loss: 0.4958 - val_acc: 0.5381\n",
      "Epoch 16/20\n",
      "21377/21377 [==============================] - 4568s - loss: 0.4971 - acc: 0.5436 - val_loss: 0.4958 - val_acc: 0.5347\n",
      "Epoch 17/20\n",
      "21377/21377 [==============================] - 4644s - loss: 0.4970 - acc: 0.5432 - val_loss: 0.4957 - val_acc: 0.5360\n",
      "Epoch 18/20\n",
      "21377/21377 [==============================] - 4575s - loss: 0.4970 - acc: 0.5420 - val_loss: 0.4957 - val_acc: 0.5368\n",
      "Epoch 19/20\n",
      "21377/21377 [==============================] - 4643s - loss: 0.4970 - acc: 0.5434 - val_loss: 0.4959 - val_acc: 0.5309\n",
      "Epoch 20/20\n",
      "21377/21377 [==============================] - 4573s - loss: 0.4970 - acc: 0.5424 - val_loss: 0.4958 - val_acc: 0.5322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f16bb6ef450>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21377 samples, validate on 2375 samples\n",
      "Epoch 1/20\n",
      "21377/21377 [==============================] - 4646s - loss: 0.4970 - acc: 0.5424 - val_loss: 0.4959 - val_acc: 0.5293\n",
      "Epoch 2/20\n",
      "21377/21377 [==============================] - 4627s - loss: 0.4970 - acc: 0.5440 - val_loss: 0.4957 - val_acc: 0.5343\n",
      "Epoch 3/20\n",
      "21377/21377 [==============================] - 4592s - loss: 0.4969 - acc: 0.5457 - val_loss: 0.4956 - val_acc: 0.5427\n",
      "Epoch 4/20\n",
      "21377/21377 [==============================] - 4641s - loss: 0.4970 - acc: 0.5445 - val_loss: 0.4956 - val_acc: 0.5343\n",
      "Epoch 5/20\n",
      "21377/21377 [==============================] - 4574s - loss: 0.4969 - acc: 0.5435 - val_loss: 0.4957 - val_acc: 0.5297\n",
      "Epoch 6/20\n",
      "21377/21377 [==============================] - 4644s - loss: 0.4969 - acc: 0.5458 - val_loss: 0.4956 - val_acc: 0.5343\n",
      "Epoch 7/20\n",
      "21377/21377 [==============================] - 4576s - loss: 0.4969 - acc: 0.5449 - val_loss: 0.4955 - val_acc: 0.5406\n",
      "Epoch 8/20\n",
      "21377/21377 [==============================] - 4648s - loss: 0.4969 - acc: 0.5456 - val_loss: 0.4957 - val_acc: 0.5309\n",
      "Epoch 9/20\n",
      "21377/21377 [==============================] - 4568s - loss: 0.4969 - acc: 0.5455 - val_loss: 0.4955 - val_acc: 0.5503\n",
      "Epoch 10/20\n",
      "21377/21377 [==============================] - 4643s - loss: 0.4969 - acc: 0.5449 - val_loss: 0.4955 - val_acc: 0.5347\n",
      "Epoch 11/20\n",
      "21377/21377 [==============================] - 4600s - loss: 0.4969 - acc: 0.5458 - val_loss: 0.4954 - val_acc: 0.5398\n",
      "Epoch 12/20\n",
      "21377/21377 [==============================] - 4697s - loss: 0.4969 - acc: 0.5442 - val_loss: 0.4954 - val_acc: 0.5432\n",
      "Epoch 13/20\n",
      "21377/21377 [==============================] - 4604s - loss: 0.4969 - acc: 0.5446 - val_loss: 0.4954 - val_acc: 0.5436\n",
      "Epoch 14/20\n",
      "21377/21377 [==============================] - 4597s - loss: 0.4969 - acc: 0.5455 - val_loss: 0.4955 - val_acc: 0.5347\n",
      "Epoch 15/20\n",
      " 8600/21377 [===========>..................] - ETA: 2638s - loss: 0.4967 - acc: 0.5474"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
