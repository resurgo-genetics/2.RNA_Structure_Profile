{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_label=hkl.load('./Gen_data/15_label.hkl')\n",
    "seqa_org =hkl.load('./Gen_data/15_A0.hkl')\n",
    "seqb_org= hkl.load('./Gen_data/15_B0.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAAACATTGCAAGTCGCCTTCATGGTGACCATATTCCCCATTCTCTTCCGCTTTTTATGTTTCCTGCTTCCCAACCTCACCTCAATCTCTTTTTGGGTGGTTTTTCCCTCTTTCGTTTAGTTTTGTTTTACCCATTTTTGTTTGCTTTTCACAATTTTTGTTTTGTTTTACCGCTTTTTGTGTTTTTACACATTTTTGTTTTGTTTTACCCATTTTTGGTTTGCTTTTCCCAATTTGGTTTTGTTTTACCCATTTTTTGTTTGTTTTACCCATTTTTGTTTTGTTTTACCCATTTTTGGTTTGCTTTTCCCAATTGGGTTTTGTTTTACCCATTTTTTTGTTTTCTGTTCCCAATTTTTGTTTTGTTGGACCCATTTTTGTTTTATTTTGTTTTCCCCGTTTTTGCTTTGTTGTACATCTTTTGGTTTTGTTTTACCTCGTTTCTGTACTTTCCCTCTTGGGTTTTTAGACCATTCTCCTTACCTGTCGGCCCACCTCCTCATCACCATCTCCCTTTACCTCTGCCTCCTGTTGTTCTCTCTTCTATAAGCCCCCATCTCTACCCAATCGTTTGTCCCTTCCCTTACCCTTCTTCATTGGGCGTGCTTGTGTCTATGTTTGCTCTTCATTTCCTTCTTTACATCCATTTTACTCTTTTAATTTCTGATTTGTATCTTGCTTTTTACTTTTGTATGTATTTTCCTCTTTGTTCCCGGGTGTGGGTCGATATTTTCTTTGTACCTCCGCCTCTTCTCTTTTTGCCCATGGCGACGTGGGTGGTTCTTTTCCTTTTCCTCTTACCTTTTCTACTGTTTAAACAAAATGATAATGTTTCCTCCTTTCTCTCACACTTCTTTGTCTTGCCGCCGATTAGTCTCTCTCCTTCGCCCGCCCCTCTTCCCCTCCCCTCTCAACTCTTTGCCCAAAGCGACGTTGACAACAATTACTCTAGCGTGTCCCGTTTGCATATCCACCCATGATAGGTTTTTCCAGGTTGTCTCTCTTCCTGCTGGCCTCGGCTTCCCACATCCCCTCCCCTCGTCGTCCCCGTGTTTCCCTCATTCTATAACTCGTCCTCGTTGGACTCGGTTCTTCCGGGACCTCCACCTCCACCTCCACTCGGGCCCCCTCCTTCCCCTCCTGGTCCTGGTAGTCCACAACCTCCTCTCCTGCTATTACCGGGCCTTCTCCCTCTCTCCCCTTACTCTTTGTTACATTTATCCACTCTTTTCACTCTTGTCTCACTTCTTCTTAAAGCTCTTTTACCGTTCTCCTTCCTCTTACGCTGCCCATTGTCCTCCTTCCGCGTCCACTCTCCTCCCACACATTTGCTTGGCCCGCTCCCATGCCCATCCTGTCCATTTACACCCTGTTTCCCCTGGTCTTGTGTCCATCCCGTACCCTGTGCTCCGTGTACCATCTGTTTTTCGCACGTGTCCTCCTTTTCCCCGTGCCCTCCTCGTCTGCTTTTATACCTTCCCTCTCGCCCGGGGGTTCCCCTGTGCTTGTGCACTTATTTGTCCACAAAACATACTGTGCGCTCGTTTCTCCTCTTTCTACTGGGTCCTAAATAGCGGGAAACCACTGTTTTCAACCAATGCCCCATCGTTCGTTCTGTAGTAAACCTGGTAAGTTGCGTCCGCGTTTTCCCTTTGTGTTGACTGCCCTTACTCTCTACCCGCCCCCCCTCTGCTACCGCGTCTCCTTTTTTTTAATTGTTCTACATTTGGTTAATTTTGTACATTTTTTAAAATTTTTACAGTTTTTAATTTTTTAGAGATTTTTAATTTTGTACATTTTTTAATTTTTTACATTTTTTAATTGTTTACCTTTTTACATTTTTTACTGTTTGGCTGTTTCCCACTGTTTACCTTTCGTTGCTAGCTCTTGGCCCCGGCGGTTGTTGGCAGCCATCTTCTTGTTCCCGGTTAGTTTAACGGGAAAGCTTACACAGGGTAAGCCAAAGAAACTTTTTAGTGGAATACTCGCACCAGCGGGGC', 'AAAACATTGCAAGTCGCCTTCATGGTGACCATATTCCCCATTCTCTTCCGCTTTTTATGTTTCCTGCTTCCCAACCTCACCTCAATCTCTTTTTGGGTGGTTTTTCCCTCTTTCGTTTAGTTTTGTTTTACCCATTTTTGTTTGCTTTTCACAATTTTTGTTTTGTTTTACCGCTTTTTGTGTTTTTACACATTTTTGTTTTGTTTTACCCATTTTTGGTTTGCTTTTCCCAATTTGGTTTTGTTTTACCCATTTTTTGTTTGTTTTACCCATTTTTGTTTTGTTTTACCCATTTTTGGTTTGCTTTTCCCAATTGGGTTTTGTTTTACCCATTTTTTTGTTTTCTGTTCCCAATTTTTGTTTTGTTGGACCCATTTTTGTTTTATTTTGTTTTCCCCGTTTTTGCTTTGTTGTACATCTTTTGGTTTTGTTTTACCTCGTTTCTGTACTTTCCCTCTTGGGTTTTTAGACCATTCTCCTTACCTGTCGGCCCACCTCCTCATCACCATCTCCCTTTACCTCTGCCTCCTGTTGTTCTCTCTTCTATAAGCCCCCATCTCTACCCAATCGTTTGTCCCTTCCCTTACCCTTCTTCATTGGGCGTGCTTGTGTCTATGTTTGCTCTTCATTTCCTTCTTTACATCCATTTTACTCTTTTAATTTCTGATTTGTATCTTGCTTTTTACTTTTGTATGTATTTTCCTCTTTGTTCCCGGGTGTGGGTCGATATTTTCTTTGTACCTCCGCCTCTTCTCTTTTTGCCCATGGCGACGTGGGTGGTTCTTTTCCTTTTCCTCTTACCTTTTCTACTGTTTAAACAAAATGATAATGTTTCCTCCTTTCTCTCACACTTCTTTGTCTTGCCGCCGATTAGTCTCTCTCCTTCGCCCGCCCCTCTTCCCCTCCCCTCTCAACTCTTTGCCCAAAGCGACGTTGACAACAATTACTCTAGCGTGTCCCGTTTGCATATCCACCCATGATAGGTTTTTCCAGGTTGTCTCTCTTCCTGCTGGCCTCGGCTTCCCACATCCCCTCCCCTCGTCGTCCCCGTGTTTCCCTCATTCTATAACTCGTCCTCGTTGGACTCGGTTCTTCCGGGACCTCCACCTCCACCTCCACTCGGGCCCCCTCCTTCCCCTCCTGGTCCTGGTAGTCCACAACCTCCTCTCCTGCTATTACCGGGCCTTCTCCCTCTCTCCCCTTACTCTTTGTTACATTTATCCACTCTTTTCACTCTTGTCTCACTTCTTCTTAAAGCTCTTTTACCGTTCTCCTTCCTCTTACGCTGCCCATTGTCCTCCTTCCGCGTCCACTCTCCTCCCACACATTTGCTTGGCCCGCTCCCATGCCCATCCTGTCCATTTACACCCTGTTTCCCCTGGTCTTGTGTCCATCCCGTACCCTGTGCTCCGTGTACCATCTGTTTTTCGCACGTGTCCTCCTTTTCCCCGTGCCCTCCTCGTCTGCTTTTATACCTTCCCTCTCGCCCGGGGGTTCCCCTGTGCTTGTGCACTTATTTGTCCACAAAACATACTGTGCGCTCGTTTCTCCTCTTTCTACTGGGTCCTAAATAGCGGGAAACCACTGTTTTCAACCAATGCCCCATCGTTCGTTCTGTAGTAAACCTGGTAAGTTGCGTCCGCGTTTTCCCTTTGTGTTGACTGCCCTTACTCTCTACCCGCCCCCCCTCTGCTACCGCGTCTCCTTTTTTTTAATTGTTCTACATTTGGTTAATTTTGTACATTTTTTAAAATTTTTACAGTTTTTAATTTTTTAGAGATTTTTAATTTTGTACATTTTTTAATTTTTTACATTTTTTAATTGTTTACCTTTTTACATTTTTTACTGTTTGGCTGTTTCCCACTGTTTACCTTTCGTTGCTAGCTCTTGGCCCCGGCGGTTGTTGGCAGCCATCTTCTTGTTCCCGGTTAGTTTAACGGGAAAGCTTACACAGGGTAAGCCAAAGAAACTTTTTAGTGGAATACTCGCACCAGCGGGGC']\n"
     ]
    }
   ],
   "source": [
    "print seqa_org[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获得ｋｍｅｒ的函数\n",
    "embed_dict =[]\n",
    "def seq_to_embed_seq(seq):\n",
    "    kmer=6\n",
    "    embed_seq = []\n",
    "    if len (seq)< kmer:\n",
    "        return embed_seq\n",
    "    for i in range(kmer,len(seq)):\n",
    "        word  = seq[(i-kmer):i] \n",
    "        #if  word not in embed_dict.keys():\n",
    "        #    embed_dict[ word  ] =str( len(embed_dict.keys()) ) #给标号\n",
    "        #embed_seq.append(  str( embed_dict[ word  ]  )  )\n",
    "        embed_seq.append(  word )\n",
    "        if word not in embed_dict:\n",
    "            embed_dict.append(word)\n",
    "    return embed_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seqa_embed=[]\n",
    "for seq in seqa_org:\n",
    "    seq = seq.upper()\n",
    "    seqa_embed.append( seq_to_embed_seq(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqb_embed=[]\n",
    "for seq in seqb_org:\n",
    "    seq = seq.upper()\n",
    "    seqb_embed.append( seq_to_embed_seq(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切分训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import shuffle\n",
    "import numpy as np\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_index = range(len(seq_label))\n",
    "shuffle(seq_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_point=int(0.8*len(seq_label))\n",
    "train_index = seq_index[: split_point]\n",
    "valid_index = seq_index[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenses_to_train_embeding_matrix =  np.array(seqa_embed)[train_index]\n",
    "with open('./Gen_data/hic/15_seqa_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( sentenses_to_train_embeding_matrix)[0]):\n",
    "        Sentense = sentenses_to_train_embeding_matrix[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentenses_to_train_embeding_matrix =  np.array(seqb_embed)[train_index]\n",
    "with open('./Gen_data/hic/15_seqb_sentense.txt','w') as f:\n",
    "    for row in range(np.shape( sentenses_to_train_embeding_matrix)[0]):\n",
    "        Sentense = sentenses_to_train_embeding_matrix[row]\n",
    "        f.write( ' '.join(Sentense)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence('./Gen_data/hic/15_seqa_sentense.txt')\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "seqa_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence('./Gen_data/hic/15_seqb_sentense.txt')\n",
    "model = Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "print(model)\n",
    "#print(model.wv['AAAAAA'])\n",
    "seqb_word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_dict_len = len(embed_dict)\n",
    "seqa_embedding_matrix = np.random.random((embed_dict_len,100))  #4096个字,每个字100维\n",
    "for item in range(embed_dict_len):\n",
    "    word  = embed_dict[item] \n",
    "    if word in seqa_word_vectors.index2word :   \n",
    "        seqa_embedding_matrix[item]= seqa_word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqb_embedding_matrix = np.random.random((embed_dict_len,100))  #4096个字,每个字100维\n",
    "for item in range(embed_dict_len):\n",
    "    word  = embed_dict[item] \n",
    "    if word in seqb_word_vectors.index2word :   \n",
    "        seqb_embedding_matrix[item]= seqb_word_vectors[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqa_input=[]\n",
    "for item in range(len(seq_label)):\n",
    "    origin_sentense = seqa_embed[item]\n",
    "    index_sentense = []\n",
    "    for word in  origin_sentense:      \n",
    "        index_sentense.append( embed_dict.index(word))\n",
    "    seqa_input.append(index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'seqa_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqb_input=[]\n",
    "for item in range(len(seq_label)):\n",
    "    origin_sentense = seqb_embed[item]\n",
    "    index_sentense = []\n",
    "    for word in  origin_sentense:      \n",
    "        index_sentense.append( embed_dict.index(word))\n",
    "    seqb_input.append(index_sentense)\n",
    "    #print pro_index_sentense\n",
    "print 'seqb_fin!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = to_categorical(seq_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train ,y_train =seqa_input[train_index], Y[train_index]\n",
    "x_valid ,y_valid = seqa_input[valid_index], Y[valid_index]\n",
    "\n",
    "x_train_seqb, x_valid_seqb = seqb_input[train_index],seqb_input[valid_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "input_file = open('./Gen_data/15_data.pkl','w')\n",
    "pkl.dump(x_train,input_file)\n",
    "pkl.dump(x_valid,input_file)\n",
    "pkl.dump(x_train_seqb,input_file)\n",
    "pkl.dump(x_valid_seqb,input_file)\n",
    "pkl.dump(y_train,input_file)\n",
    "pkl.dump(y_valid,input_file)\n",
    "pkl.dump(train_index,input_file)\n",
    "pkl.dump(valid_index,input_file)\n",
    "pkl.dump(embedding_matrix,input_file)\n",
    "input_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "output_file = open('./Gen_data/15_data.pkl','r')\n",
    "x_train = pkl.load(output_file)\n",
    "x_valid = pkl.load(output_file)\n",
    "x_train_seqb = pkl.load(output_file)\n",
    "y_valid_seqb = pkl.load(output_file)\n",
    "y_trian = pkl.load(output_file)\n",
    "y_valid = pkl.load(output_file)\n",
    "train_index = pkl.load(output_file)\n",
    "valid_index = pkl.load(output_file)\n",
    "embedding_matrix  = pkl.load(output_file)\n",
    "output_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,InputLayer\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten,Merge\n",
    "from keras import backend as K\n",
    "from keras import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM =100\n",
    "MAX_SEQUENCE_LENGTH = 1000  \n",
    "nb_words =embed_dict_len   #字典的len(keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializations\n",
    "# Attention GRU network\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seqa_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[seqa_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "seqb_embedding_layer = Embedding(nb_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[seqb_embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = Sequential()\n",
    "left.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "left.add(embedding_layer)\n",
    "\n",
    "right = Sequential()\n",
    "right.add( InputLayer(input_shape=(MAX_SEQUENCE_LENGTH,),input_dtype='int32'))\n",
    "right.add(ehr_embedding_layer)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Merge([left, right], mode='concat'))\n",
    "model.add(Bidirectional(LSTM(100,return_sequences=True)))\n",
    "model.add(AttLayer())\n",
    "model.add(Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005,l1=0.005)))\n",
    "rmsprop = opt.rmsprop(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=rmsprop,metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "cell_type = 'GM12878'\n",
    "with open('./Gen_data/'+'hic'+'/15_newwork_result.txt','w') as f:\n",
    "    f.write( ','.join(  ['acc','auc','f1_score']))\n",
    "    for iter_index in range(60):\n",
    "        model.fit([x_train, x_train_seqb], y_train, batch_size=128, nb_epoch=10, validation_data=([x_valid, x_valid_seqb], y_valid))\n",
    "        pro = model.predict_on_batch([x_valid,x_valid_ehr])\n",
    "        y_pred = [ 1 if item1>item0 else 0 for item0,item1 in pro]\n",
    "        acc = metrics.accuracy_score(y_valid[:,1],y_pred)\n",
    "        auc = metrics.roc_auc_score(y_valid[:,1],pro[:,1])\n",
    "        f1_score = metrics.f1_score(y_valid[:,1],y_pred)\n",
    "        f.write('{:2f},{:2f},{:2f}'.format(acc,auc,f1_score))\n",
    "        print '{:2f},{:2f},{:2f}'.format(acc,auc,f1_score)\n",
    "        if  iter_index%19==0 :\n",
    "            model.save_weights('./Gen_data/'+'hic'+'/15_Att-BLSTM_model_iter{:03d}.h5'.format (iter_index*10))\n",
    "    model.save_weights('./Gen_data/'+'hic'+'/15_Att-BLSTM_model_iter{:s}.h5'.format ('fin'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
