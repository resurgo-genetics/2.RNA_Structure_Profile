{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这一个是负样本均衡过的\n",
    "import hickle as hkl\n",
    "x_val = hkl.load('./Gen_data/13_x_val.hkl')\n",
    "y_val = hkl.load('./Gen_data/13_y_val.hkl')\n",
    "x_train = hkl.load('./Gen_data/13_x_train.hkl')\n",
    "y_train = hkl.load('./Gen_data/13_y_train.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(843, 4096, 2)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import random\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_root='/home/yinqijin/tempdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280 1291 60\n"
     ]
    }
   ],
   "source": [
    "random.seed(666)\n",
    "\n",
    "fin1  = open(base_root+'/training.csv','r')\n",
    "\n",
    "pbin   = [ [],[],[],[],[] ]\n",
    "nbin   = [ [],[],[],[],[] ]\n",
    "bins   = []\n",
    "neg    = []\n",
    "pos    = []\n",
    "for line in fin1:\n",
    "\tif line[0] == 'b':\n",
    "\t\tcontinue\n",
    "\telse:\n",
    "\t\tdata  = line.split('\"')\n",
    "\t\tif data[1] not in bins:\n",
    "\t\t\tbins.append(data[1])\n",
    "\t\tindex = bins.index(data[1])\n",
    "\t\tdata  = line.split(',')\n",
    "\t\tif data[7] == '1':\n",
    "\t\t\tpbin[index].append(line)\n",
    "\t\t\tpos.append(line)\n",
    "\t\telse:\n",
    "\t\t\tnbin[index].append(line)\t\n",
    "\n",
    "\n",
    "for i in range(len(bins)):\n",
    "\tnbinlen = len(nbin[i])\n",
    "\ttemp    = random.sample(nbin[i],nbinlen/20)\n",
    "\tneg.extend(temp)\n",
    "\n",
    "nnum   = len(neg)\n",
    "pnum   = len(pos)\n",
    "fnum   = len(neg[0].strip().split(',')) - 18\n",
    "\n",
    "print nnum,pnum,fnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fin2   = open(base_root+'enhancers.bed','r')\n",
    "fin3   = open(base_root+'promoters.bed','r')\n",
    "enhancers = []\n",
    "promoters = []\n",
    "\n",
    "arrays1 = numpy.zeros((nnum+pnum,100,2))\n",
    "labels1 = numpy.zeros(nnum+pnum)\n",
    "\n",
    "for line in fin2:\n",
    "\tdata = line.strip().split()\n",
    "\tenhancers.append(data[3])\n",
    "\n",
    "for line in fin3:\n",
    "\tdata = line.strip().split()\n",
    "\tpromoters.append(data[3])\n",
    "\n",
    "enhancer_model = Doc2Vec.load(base_root+'enhancers_6_1_100.d2v')\n",
    "promoter_model = Doc2Vec.load(base_root+'promoters_6_1_100.d2v')\n",
    "\n",
    "\n",
    "for i in range(len(pos)):\n",
    "        data = pos[i].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))\n",
    "\tarrays1[i] = numpy.append(enhancer_vec,promoter_vec,axis=1)\n",
    "\tlabels1[i] = 1\n",
    "\n",
    "for j in range(len(neg)):\n",
    "        data = neg[j].strip().split(',')\n",
    "        e_index = enhancers.index(data[5])\n",
    "        p_index = promoters.index(data[10])\n",
    "        prefix_enhancer = 'ENHANCERS_' + str(e_index)\n",
    "        prefix_promoter = 'PROMOTERS_' + str(p_index)\n",
    "        enhancer_vec = enhancer_model.docvecs[prefix_enhancer]\n",
    "        promoter_vec = promoter_model.docvecs[prefix_promoter]\n",
    "        enhancer_vec = enhancer_vec.reshape((100,1))\n",
    "        promoter_vec = promoter_vec.reshape((100,1))  #.reshape((1,100))\n",
    "\tarrays1[i+j] = numpy.append(enhancer_vec,promoter_vec,axis=1)  #umn_stack\n",
    "\tlabels1[i+j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2571, 100, 2)\n"
     ]
    }
   ],
   "source": [
    "print numpy.shape(arrays1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = range(len(labels1))\n",
    "random.shuffle(index)\n",
    "SPLIT_RATIO = 0.85\n",
    "split_point =int(SPLIT_RATIO*len(labels1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2185\n"
     ]
    }
   ],
   "source": [
    "print split_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrays1 = numpy.array(arrays1)\n",
    "labels1 = numpy.array(labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train , y_train=  arrays1[index[:split_point]] ,labels1[ index [:split_point]]\n",
    "x_valid ,y_valid= arrays1[index[split_point:]] , labels1[index[split_point:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "savedata =open('./Gen_data/14_data.pkl','w')\n",
    "pkl.dump(x_train,savedata)\n",
    "pkl.dump(y_train,savedata)\n",
    "pkl.dump(x_valid,savedata)\n",
    "pkl.dump(y_valid,savedata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loaddata = open('./Gen_data/14_data.pkl','r')\n",
    "x_train_1=pkl.load(loaddata)\n",
    "y_train_1=pkl.load(loaddata)\n",
    "x_valid_1=pkl.load(loaddata)\n",
    "x_valid_1=pkl.load(loaddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert  all( y_train == y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2185, 100, 2)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.shape(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import backend as K\n",
    "\n",
    "from collections  import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "\n",
    "        M = K.tanh(x)\n",
    "        alpha = K.dot(M,self.W)#.dimshuffle(0,2,1)\n",
    "\n",
    "        ai = K.exp(alpha)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return K.tanh(weighted_input.sum(axis=1))\n",
    "        '''\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "        '''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense,Input,Activation\n",
    "from keras.layers import Embedding, LSTM, Bidirectional,GRU,InputLayer\n",
    "from keras.models import Model,Sequential\n",
    "from  keras.regularizers import ActivityRegularizer\n",
    "from keras.layers.core import Dropout,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmer_input = Input(shape=(100,2), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_lstm =Bidirectional(LSTM(5,return_sequences=True))(kmer_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_lstm_drop = Dropout(0.3)(l_lstm)\n",
    "l_att = AttLayer()(l_lstm_drop)\n",
    "l_flatten = Flatten()(l_lstm_drop)\n",
    "l_att_drop = Dropout(0.5)(l_flatten)\n",
    "l_dense = Dense(128)(l_att_drop)\n",
    "l_dense_1 = Dense(32)(l_dense)\n",
    "preds = Dense(2, activation='softmax',activity_regularizer= ActivityRegularizer(l2=0.005))(l_dense_1)\n",
    "model  = Model (kmer_input,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model fitting - attention LSTM network\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 100, 2)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 100, 10)       320         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 100, 10)       0           bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 1000)          0           dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 2)             2002        flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,322\n",
      "Trainable params: 2,322\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mse',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "print(\"model fitting - attention LSTM network\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Waiting for existing lock by process '73460' (I am process '125580')\n",
      "INFO (theano.gof.compilelock): To manually release the lock, delete /home/yinqijin/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.2.1511-Core-x86_64-2.7.12-64/lock_dir\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid), nb_epoch=20, batch_size=100)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
